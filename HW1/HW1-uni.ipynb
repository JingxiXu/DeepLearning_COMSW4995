{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# COMS 4995_002 Deep Learning Assignment 1\n",
    "Due on Monday, Oct 9, 11:59pm\n",
    "\n",
    "This assignment can be done in groups of at most 3 students. Everyone must submit on Courseworks individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the UNIs of your group (if applicable)\n",
    "\n",
    "Member 1: Kaho Chan, kc3137\n",
    "\n",
    "Member 2: Yu Wang, yw3025\n",
    "\n",
    "Member 3: Jingxi Xu, jx2324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "# you shouldn't need to make any more imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    Abstraction of neural network.\n",
    "    Stores parameters, activations, cached values. \n",
    "    Provides necessary functions for training and prediction. \n",
    "    \"\"\"\n",
    "    def __init__(self, layer_dimensions, drop_prob=0.0, reg_lambda=0.0):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases for each layer\n",
    "        :param layer_dimensions: (list) number of nodes in each layer\n",
    "        :param drop_prob: drop probability for dropout layers. Only required in part 2 of the assignment\n",
    "        :param reg_lambda: regularization parameter. Only required in part 2 of the assignment\n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        self.parameters = {}\n",
    "        self.num_layers = len(layer_dimensions)\n",
    "        self.drop_prob = drop_prob\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.status = 'train'\n",
    "        self.X_val = None\n",
    "        self.y_val = None\n",
    "        \n",
    "        # init parameters\n",
    "        for i in range(1, self.num_layers):\n",
    "#             self.parameters[('W', i)] = np.random.normal(0, 1, (layer_dimensions[i], layer_dimensions[i - 1]))\n",
    "#             self.parameters[('W', i)] /= np.sqrt(layer_dimensions[i - 1])\n",
    "            self.parameters[('W', i)] = np.random.randn(layer_dimensions[i], layer_dimensions[i - 1]) / np.sqrt(layer_dimensions[i - 1])\n",
    "            self.parameters[('b', i)] = np.zeros([layer_dimensions[i], 1])\n",
    "\n",
    "    def affineForward(self, A, W, b):\n",
    "        \"\"\"\n",
    "        Forward pass for the affine layer.\n",
    "        :param A: input matrix, shape (L, S), where L is the number of hidden units in the previous layer and S is\n",
    "        the number of samples\n",
    "        :returns: the affine product WA + b, along with the cache required for the backward pass\n",
    "        \"\"\"\n",
    "#         M = None\n",
    "#         if self.status == 'train' and self.drop_prob > 0:\n",
    "#             # dropout only when training\n",
    "#             A, M = self.dropout(A, self.drop_prob)\n",
    "        Z = np.dot(W, A) + b\n",
    "        cache = (A, W, b, Z)\n",
    "        return Z, cache\n",
    "\n",
    "    def activationForward(self, A, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Common interface to access all activation functions.\n",
    "        :param A: input to the activation function\n",
    "        :param prob: activation funciton to apply to A. Just \"relu\" for this assignment.\n",
    "        :returns: activation(A)\n",
    "        \"\"\" \n",
    "        return self.relu(A)\n",
    "\n",
    "    def relu(self, X):\n",
    "        A = np.maximum(0, X)\n",
    "        assert (X.shape == A.shape)\n",
    "        return A\n",
    "            \n",
    "    def dropout(self, A, prob):\n",
    "        \"\"\"\n",
    "        :param A: \n",
    "        :param prob: drop prob\n",
    "        :returns: tuple (A, M) \n",
    "            WHERE\n",
    "            A is matrix after applying dropout\n",
    "            M is dropout mask, used in the backward pass\n",
    "        \"\"\"\n",
    "        M = np.random.uniform(size = A.shape)\n",
    "        M = (M > prob) / (1 - prob)\n",
    "        A = A * M\n",
    "        return A, M\n",
    "\n",
    "    def forwardPropagation(self, X):\n",
    "        \"\"\"\n",
    "        Runs an input X through the neural network to compute activations\n",
    "        for all layers. Returns the output computed at the last layer along\n",
    "        with the cache required for backpropagation.\n",
    "        :returns: (tuple) AL, cache\n",
    "            WHERE \n",
    "            AL is activation of last layer\n",
    "            cache is cached values for each layer that\n",
    "                     are needed in further steps\n",
    "        \"\"\"\n",
    "        \n",
    "        cache = {}\n",
    "        A = X\n",
    "        for l in range(1, self.num_layers):\n",
    "            A, cache_l = self.affineForward(A,\n",
    "                                            self.parameters[('W', l)], \n",
    "                                            self.parameters[('b', l)])\n",
    "            \n",
    "            if (l != self.num_layers - 1):\n",
    "                # don't do relu on output layer\n",
    "                A = self.activationForward(A)\n",
    "            M = None\n",
    "            if self.status == 'train' and self.drop_prob > 0 and l != self.num_layers - 1:\n",
    "            # dropout only when training\n",
    "                A, M = self.dropout(A, self.drop_prob)\n",
    "            cache[l] = cache_l + (M,)\n",
    "#             print (len(cache[l]))\n",
    "        # return AL, cache\n",
    "        return A, cache\n",
    "    \n",
    "    def costFunction(self, AL, y):\n",
    "        \"\"\"\n",
    "        :param AL: Activation of last layer, shape (num_classes, S)\n",
    "        :param y: labels, shape (S)\n",
    "        :param alpha: regularization parameter\n",
    "        :returns cost, dAL: A scalar denoting cost and the gradient of cost\n",
    "        \"\"\"\n",
    "        assert (self.status == 'train')\n",
    "        # compute loss\n",
    "        S = AL.shape[1]\n",
    "        probs = np.exp(AL - np.max(AL, axis=0, keepdims=True))\n",
    "        probs /= np.sum(probs, axis=0, keepdims=True)\n",
    "        cost = -np.sum(np.log(probs[y, np.arange(S)])) / S\n",
    "        \n",
    "        if self.reg_lambda > 0:\n",
    "            # add regularization\n",
    "            for l in range(1, self.num_layers):\n",
    "                cost += 0.5 * self.reg_lambda * np.sum(np.power(self.parameters[('W', l)], 2))\n",
    "        dAL = probs.copy()\n",
    "        dAL[y, np.arange(S)] -= 1\n",
    "        dAL /= S\n",
    "        return cost, dAL\n",
    "\n",
    "    def affineBackward(self, dA_prev, cache):\n",
    "        \"\"\"\n",
    "        Backward pass for the affine layer.\n",
    "        :param dA_prev: gradient from the next layer.\n",
    "        :param cache: cache returned in affineForward\n",
    "        :returns dA: gradient on the input to this layer\n",
    "                 dW: gradient on the weights\n",
    "                 db: gradient on the bias\n",
    "        \"\"\"\n",
    "#        A = cache[0]\n",
    "#        W = cache[1]\n",
    "#        b = cache[2]\n",
    "        A, W, b, Z, M = cache\n",
    "        S = A.shape[1]\n",
    "        \n",
    "#         dZ = self.activationBackward(dA_prev, cache)\n",
    "        dA = np.dot(W.T, dA_prev)\n",
    "        dW = np.dot(dA_prev, A.T) / S\n",
    "        db = np.sum(dA_prev, axis = 1, keepdims=True) / S # or np.mean() ?\n",
    "        # dA = W.T.dot(dA_prev) \n",
    "        # dW = dA_prev.dot(A.T) / S\n",
    "        # db = np.sum(dA_pre, axis = 1) / S\n",
    "        return dA, dW, db\n",
    "    \n",
    "    def activationBackward(self, dA, cache, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Interface to call backward on activation functions.\n",
    "        In this case, it's just relu. \n",
    "        \"\"\"\n",
    "        return self.relu_derivative(dA, cache[3]) # cache[3] == Z[l]\n",
    "        \n",
    "    def relu_derivative(self, dx, cached_x):\n",
    "        out = np.maximum(0, cached_x)\n",
    "        out[out > 0] = 1\n",
    "        dx = out * dx\n",
    "        return dx\n",
    "\n",
    "    def dropout_backward(self, dA, cache):\n",
    "        A, W, b, Z, M = cache\n",
    "        return dA * M # cache[4] = Mdropout_backward\n",
    "\n",
    "    def backPropagation(self, dAL, Y, cache):\n",
    "        \"\"\"\n",
    "        Run backpropagation to compute gradients on all paramters in the model\n",
    "        :param dAL: gradient on the last layer of the network. Returned by the cost function.\n",
    "        :param Y: labels\n",
    "        :param cache: cached values during forwardprop\n",
    "        :returns gradients: dW and db for each weight/bias\n",
    "        \"\"\"\n",
    "        assert (self.status == 'train')\n",
    "        gradients = {}\n",
    "        dA = dAL\n",
    "        for l in range(self.num_layers-1, 0, -1):\n",
    "            if self.drop_prob > 0 and l != self.num_layers - 1:\n",
    "                dA = self.dropout_backward(dA, cache[l])\n",
    "            if l != self.num_layers - 1:\n",
    "                dA = self.activationBackward(dA, cache[l])\n",
    "            dA, dW, db = self.affineBackward(dA, cache[l])\n",
    "            # assert (dW.shape == self.parameters[('W', l)].shape), '{} - {}'.format(dW.shape, self.parameters[('W', l)].shape)\n",
    "            # assert (db.shape == self.parameters[('b', l)].shape), '{} - {}'.format(db.shape, self.parameters[('b', l)].shape)\n",
    "            gradients[('W', l)] = dW\n",
    "            # gradients[('b', l)] = db # should b be regularized ?\n",
    "            if self.reg_lambda > 0:\n",
    "                # add gradients from L2 regularization to each dW\n",
    "                gradients[('W', l)] += self.reg_lambda * self.parameters[('W', l)]\n",
    "               #  gradients[('b', l)] += reg_lambda * self.parameters[('b', l)]\n",
    "        return gradients\n",
    "\n",
    "    def updateParameters(self, gradients, alpha):\n",
    "        \"\"\"\n",
    "        :param gradients: gradients for each weight/bias\n",
    "        :param alpha: step size for gradient descent \n",
    "        \"\"\"\n",
    "        for key in gradients.keys():\n",
    "            self.parameters[key] -= alpha * gradients[key]\n",
    "            \n",
    "    def train(self, X, y, iters=1000, alpha=0.0001, batch_size=100, print_every=100):\n",
    "        \"\"\"\n",
    "        :param X: input samples, each column is a sample\n",
    "        :param y: labels for input samples, y.shape[0] must equal X.shape[1]\n",
    "        :param iters: number of training iterations\n",
    "        :param alpha: step size for gradient descent\n",
    "        :param batch_size: number of samples in a minibatch\n",
    "        :param print_every: no. of iterations to print debug info after\n",
    "        \"\"\"\n",
    "        assert (alpha * self.reg_lambda < 1)\n",
    "        self.status = 'train'\n",
    "        self.parameters['mean'] = np.mean(X, axis = 1, keepdims = True)\n",
    "        self.parameters['var'] = np.var(X, axis = 1, keepdims = True)\n",
    "        X = (X - self.parameters['mean']) / np.sqrt(self.parameters['var'])\n",
    "        for i in range(0, iters):\n",
    "            # get minibatch\n",
    "            X_batch, y_batch = self.get_batch(X, y, batch_size)\n",
    "            # forward prop\n",
    "            AL, cache = self.forwardPropagation(X_batch)\n",
    "            # compute loss\n",
    "            cost, dAL = self.costFunction(AL, y_batch)\n",
    "            # compute gradients\n",
    "            gradients = self.backPropagation(dAL, y_batch, cache)\n",
    "            # update weights and biases based on gradient\n",
    "            self.updateParameters(gradients, alpha)\n",
    "            if i % print_every == 0:\n",
    "                # print cost, train and validation set accuracies\n",
    "                trn_acc = self.score(self.predict(X), y)\n",
    "                if self.X_val is not None:\n",
    "                    val_acc = self.score(self.predict(self.X_val), self.y_val)\n",
    "                else:\n",
    "                    val_acc = np.nan\n",
    "                print('iter={:5}, cost={:.4f}, trn_acc={:.4f}, val_acc={:.4f}'.format(i, cost, trn_acc, val_acc))\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for each sample\n",
    "        \"\"\"\n",
    "        self.status = 'predict'\n",
    "        X = (X - self.parameters['mean']) / np.sqrt(self.parameters['var'])\n",
    "        AL, _ = self.forwardPropagation(X)\n",
    "        y_pred = np.argmax(AL, axis = 0)\n",
    "        self.status = 'train'\n",
    "        return y_pred\n",
    "\n",
    "    def get_batch(self, X, y, batch_size):\n",
    "        \"\"\"\n",
    "        Return minibatch of samples and labels\n",
    "        \n",
    "        :param X, y: samples and corresponding labels\n",
    "        :parma batch_size: minibatch size\n",
    "        :returns: (tuple) X_batch, y_batch\n",
    "        \"\"\"\n",
    "        batch_idx = np.random.randint(X.shape[1], size = batch_size)\n",
    "        X_batch = X[:, batch_idx]\n",
    "        y_batch = y[batch_idx]\n",
    "#         print y_batch.shape\n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def score(self, y_pred, y_gold):\n",
    "        return np.mean(y_pred == y_gold)\n",
    "    \n",
    "    def load_validation_set(self, X_val, y_val):\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions, DO NOT modify this\n",
    "def get_img_array(path):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imread(path)\n",
    "\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to load data, DO NOT change these\n",
    "\n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    y_one_hot[y] = 1\n",
    "    return y_one_hot.T\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = get_img_array(f)\n",
    "        img_arr = img_arr.flatten() / 255.0\n",
    "        images.append(img_arr)\n",
    "    X = np.column_stack(images)\n",
    "\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n",
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "Loaded 10000/10000\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_root_path = 'cifar10-hw1/'\n",
    "X_train, y_train = get_train_data(data_root_path) # this may take a few minutes\n",
    "X_test = get_images(data_root_path + 'test')\n",
    "print('Data loading done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 50000)\n",
      "(50000,)\n",
      "[6 1 6 6 8]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group's helper function\n",
    "def split(X, y, test_size):\n",
    "    '''\n",
    "    split the data into training and validation set\n",
    "    '''\n",
    "    indices = np.random.permutation(X.shape[1])\n",
    "    test_num = int(test_size * X.shape[1])\n",
    "    return X[:, indices[test_num:]], X[:, indices[:test_num]], y[indices[test_num:]], y[indices[:test_num]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment on manually splitted train/validation (trn/val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_trn, X_val, y_trn, y_val = split(X_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=    0, cost=2.2772, trn_acc=0.1072, val_acc=0.1078\n",
      "iter=  100, cost=2.0054, trn_acc=0.2676, val_acc=0.2930\n",
      "iter=  200, cost=1.9131, trn_acc=0.3277, val_acc=0.3570\n",
      "iter=  300, cost=1.8278, trn_acc=0.3623, val_acc=0.3826\n",
      "iter=  400, cost=1.7765, trn_acc=0.3831, val_acc=0.4042\n",
      "iter=  500, cost=1.6021, trn_acc=0.4001, val_acc=0.4094\n",
      "iter=  600, cost=1.6736, trn_acc=0.4052, val_acc=0.4276\n",
      "iter=  700, cost=1.6786, trn_acc=0.4252, val_acc=0.4376\n",
      "iter=  800, cost=1.5087, trn_acc=0.4409, val_acc=0.4500\n",
      "iter=  900, cost=1.3721, trn_acc=0.4405, val_acc=0.4526\n",
      "iter= 1000, cost=1.2821, trn_acc=0.4468, val_acc=0.4560\n",
      "iter= 1100, cost=1.5660, trn_acc=0.4603, val_acc=0.4604\n",
      "iter= 1200, cost=1.3536, trn_acc=0.4684, val_acc=0.4716\n",
      "iter= 1300, cost=1.4831, trn_acc=0.4729, val_acc=0.4722\n",
      "iter= 1400, cost=1.4379, trn_acc=0.4877, val_acc=0.4792\n",
      "iter= 1500, cost=1.2795, trn_acc=0.4767, val_acc=0.4762\n",
      "iter= 1600, cost=1.4163, trn_acc=0.4880, val_acc=0.4754\n",
      "iter= 1700, cost=1.2399, trn_acc=0.5050, val_acc=0.4828\n",
      "iter= 1800, cost=1.3005, trn_acc=0.5062, val_acc=0.4860\n",
      "iter= 1900, cost=1.1625, trn_acc=0.5045, val_acc=0.4820\n",
      "iter= 2000, cost=1.4014, trn_acc=0.5054, val_acc=0.4836\n",
      "iter= 2100, cost=1.4571, trn_acc=0.5159, val_acc=0.4880\n",
      "iter= 2200, cost=1.1188, trn_acc=0.5190, val_acc=0.4872\n",
      "iter= 2300, cost=1.2920, trn_acc=0.5018, val_acc=0.4804\n",
      "iter= 2400, cost=1.1686, trn_acc=0.5344, val_acc=0.4986\n",
      "iter= 2500, cost=1.2466, trn_acc=0.5504, val_acc=0.4950\n",
      "iter= 2600, cost=1.1868, trn_acc=0.5441, val_acc=0.4992\n",
      "iter= 2700, cost=1.4138, trn_acc=0.5366, val_acc=0.4954\n",
      "iter= 2800, cost=1.1815, trn_acc=0.5621, val_acc=0.4976\n",
      "iter= 2900, cost=1.1713, trn_acc=0.5558, val_acc=0.4972\n",
      "iter= 3000, cost=1.0356, trn_acc=0.5449, val_acc=0.4932\n",
      "iter= 3100, cost=1.0922, trn_acc=0.5645, val_acc=0.5054\n",
      "iter= 3200, cost=1.0938, trn_acc=0.5671, val_acc=0.5092\n",
      "iter= 3300, cost=1.1996, trn_acc=0.5752, val_acc=0.5102\n",
      "iter= 3400, cost=1.3191, trn_acc=0.5644, val_acc=0.5000\n",
      "iter= 3500, cost=1.2199, trn_acc=0.5593, val_acc=0.5034\n",
      "iter= 3600, cost=1.0340, trn_acc=0.5894, val_acc=0.5016\n",
      "iter= 3700, cost=1.0703, trn_acc=0.6003, val_acc=0.5050\n",
      "iter= 3800, cost=1.0080, trn_acc=0.5865, val_acc=0.5016\n",
      "iter= 3900, cost=1.1667, trn_acc=0.5736, val_acc=0.5062\n",
      "iter= 4000, cost=0.9640, trn_acc=0.5956, val_acc=0.5162\n",
      "iter= 4100, cost=0.9821, trn_acc=0.5998, val_acc=0.5104\n",
      "iter= 4200, cost=0.9547, trn_acc=0.5941, val_acc=0.5120\n",
      "iter= 4300, cost=0.9516, trn_acc=0.6047, val_acc=0.4998\n",
      "iter= 4400, cost=1.1014, trn_acc=0.6092, val_acc=0.5056\n",
      "iter= 4500, cost=0.9926, trn_acc=0.6222, val_acc=0.5092\n",
      "iter= 4600, cost=1.1260, trn_acc=0.5958, val_acc=0.5020\n",
      "iter= 4700, cost=0.8432, trn_acc=0.6053, val_acc=0.5128\n",
      "iter= 4800, cost=0.9544, trn_acc=0.6274, val_acc=0.5130\n",
      "iter= 4900, cost=0.8384, trn_acc=0.6439, val_acc=0.5084\n"
     ]
    }
   ],
   "source": [
    "layer_dimensions = [X_train.shape[0], 1000, 200, 50, 10]\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "NN.load_validation_set(X_val, y_val)\n",
    "NN.train(X_trn, y_trn, iters=5000, alpha=1, batch_size=100, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=    0, cost=2.2772, trn_acc=0.1072, val_acc=0.1078\n",
      "iter=  100, cost=2.0054, trn_acc=0.2676, val_acc=0.2930\n",
      "iter=  200, cost=1.9131, trn_acc=0.3277, val_acc=0.3570\n",
      "iter=  300, cost=1.8278, trn_acc=0.3623, val_acc=0.3826\n",
      "iter=  400, cost=1.7765, trn_acc=0.3831, val_acc=0.4042\n",
      "iter=  500, cost=1.6021, trn_acc=0.4001, val_acc=0.4094\n",
      "iter=  600, cost=1.6736, trn_acc=0.4052, val_acc=0.4276\n",
      "iter=  700, cost=1.6786, trn_acc=0.4252, val_acc=0.4376\n",
      "iter=  800, cost=1.5087, trn_acc=0.4409, val_acc=0.4500\n",
      "iter=  900, cost=1.3721, trn_acc=0.4405, val_acc=0.4526\n",
      "iter= 1000, cost=1.2821, trn_acc=0.4468, val_acc=0.4560\n",
      "iter= 1100, cost=1.5660, trn_acc=0.4603, val_acc=0.4604\n",
      "iter= 1200, cost=1.3536, trn_acc=0.4684, val_acc=0.4716\n",
      "iter= 1300, cost=1.4831, trn_acc=0.4729, val_acc=0.4722\n",
      "iter= 1400, cost=1.4379, trn_acc=0.4877, val_acc=0.4792\n",
      "iter= 1500, cost=1.2795, trn_acc=0.4767, val_acc=0.4762\n",
      "iter= 1600, cost=1.4163, trn_acc=0.4880, val_acc=0.4754\n",
      "iter= 1700, cost=1.2399, trn_acc=0.5050, val_acc=0.4828\n",
      "iter= 1800, cost=1.3005, trn_acc=0.5062, val_acc=0.4860\n",
      "iter= 1900, cost=1.1625, trn_acc=0.5045, val_acc=0.4820\n",
      "iter= 2000, cost=1.4014, trn_acc=0.5054, val_acc=0.4836\n",
      "iter= 2100, cost=1.4571, trn_acc=0.5159, val_acc=0.4880\n",
      "iter= 2200, cost=1.1188, trn_acc=0.5190, val_acc=0.4872\n",
      "iter= 2300, cost=1.2920, trn_acc=0.5018, val_acc=0.4804\n",
      "iter= 2400, cost=1.1686, trn_acc=0.5344, val_acc=0.4986\n",
      "iter= 2500, cost=1.2466, trn_acc=0.5504, val_acc=0.4950\n",
      "iter= 2600, cost=1.1868, trn_acc=0.5441, val_acc=0.4992\n",
      "iter= 2700, cost=1.4138, trn_acc=0.5366, val_acc=0.4954\n",
      "iter= 2800, cost=1.1815, trn_acc=0.5621, val_acc=0.4976\n",
      "iter= 2900, cost=1.1713, trn_acc=0.5558, val_acc=0.4972\n",
      "iter= 3000, cost=1.0356, trn_acc=0.5449, val_acc=0.4932\n",
      "iter= 3100, cost=1.0922, trn_acc=0.5645, val_acc=0.5054\n",
      "iter= 3200, cost=1.0938, trn_acc=0.5671, val_acc=0.5092\n",
      "iter= 3300, cost=1.1996, trn_acc=0.5752, val_acc=0.5102\n",
      "iter= 3400, cost=1.3191, trn_acc=0.5644, val_acc=0.5000\n",
      "iter= 3500, cost=1.2199, trn_acc=0.5593, val_acc=0.5034\n",
      "iter= 3600, cost=1.0340, trn_acc=0.5894, val_acc=0.5016\n",
      "iter= 3700, cost=1.0703, trn_acc=0.6003, val_acc=0.5050\n",
      "iter= 3800, cost=1.0080, trn_acc=0.5865, val_acc=0.5016\n",
      "iter= 3900, cost=1.1667, trn_acc=0.5736, val_acc=0.5062\n",
      "iter= 4000, cost=0.9640, trn_acc=0.5956, val_acc=0.5162\n",
      "iter= 4100, cost=0.9821, trn_acc=0.5998, val_acc=0.5104\n",
      "iter= 4200, cost=0.9547, trn_acc=0.5941, val_acc=0.5120\n",
      "iter= 4300, cost=0.9516, trn_acc=0.6047, val_acc=0.4998\n",
      "iter= 4400, cost=1.1014, trn_acc=0.6092, val_acc=0.5056\n",
      "iter= 4500, cost=0.9926, trn_acc=0.6222, val_acc=0.5092\n",
      "iter= 4600, cost=1.1260, trn_acc=0.5958, val_acc=0.5020\n",
      "iter= 4700, cost=0.8432, trn_acc=0.6053, val_acc=0.5128\n",
      "iter= 4800, cost=0.9544, trn_acc=0.6274, val_acc=0.5130\n",
      "iter= 4900, cost=0.8384, trn_acc=0.6439, val_acc=0.5084\n",
      "iter= 5000, cost=1.1800, trn_acc=0.6219, val_acc=0.5116\n",
      "iter= 5100, cost=0.9790, trn_acc=0.6255, val_acc=0.5102\n",
      "iter= 5200, cost=1.0586, trn_acc=0.6188, val_acc=0.5122\n",
      "iter= 5300, cost=0.7912, trn_acc=0.6269, val_acc=0.5082\n",
      "iter= 5400, cost=0.9795, trn_acc=0.6438, val_acc=0.5128\n",
      "iter= 5500, cost=1.0355, trn_acc=0.6212, val_acc=0.5146\n",
      "iter= 5600, cost=0.8354, trn_acc=0.6420, val_acc=0.5062\n",
      "iter= 5700, cost=0.8359, trn_acc=0.6471, val_acc=0.5120\n",
      "iter= 5800, cost=0.8632, trn_acc=0.6380, val_acc=0.5032\n",
      "iter= 5900, cost=0.7737, trn_acc=0.6402, val_acc=0.5186\n",
      "iter= 6000, cost=0.8128, trn_acc=0.6454, val_acc=0.5038\n",
      "iter= 6100, cost=0.8649, trn_acc=0.6696, val_acc=0.5090\n",
      "iter= 6200, cost=0.9437, trn_acc=0.6745, val_acc=0.5182\n",
      "iter= 6300, cost=0.5826, trn_acc=0.6799, val_acc=0.5218\n",
      "iter= 6400, cost=0.5850, trn_acc=0.6470, val_acc=0.5098\n",
      "iter= 6500, cost=0.7554, trn_acc=0.6824, val_acc=0.5180\n",
      "iter= 6600, cost=0.5916, trn_acc=0.6632, val_acc=0.5046\n",
      "iter= 6700, cost=0.8197, trn_acc=0.6832, val_acc=0.5178\n",
      "iter= 6800, cost=0.8233, trn_acc=0.6881, val_acc=0.5070\n",
      "iter= 6900, cost=0.6084, trn_acc=0.6760, val_acc=0.5144\n",
      "iter= 7000, cost=0.7349, trn_acc=0.6899, val_acc=0.4926\n",
      "iter= 7100, cost=0.5115, trn_acc=0.7037, val_acc=0.5160\n",
      "iter= 7200, cost=0.7915, trn_acc=0.7210, val_acc=0.5250\n",
      "iter= 7300, cost=0.6179, trn_acc=0.7173, val_acc=0.5240\n",
      "iter= 7400, cost=0.6222, trn_acc=0.7006, val_acc=0.5152\n",
      "iter= 7500, cost=0.5885, trn_acc=0.7265, val_acc=0.5204\n",
      "iter= 7600, cost=0.5489, trn_acc=0.7174, val_acc=0.5192\n",
      "iter= 7700, cost=0.6005, trn_acc=0.7220, val_acc=0.5194\n",
      "iter= 7800, cost=0.7167, trn_acc=0.7006, val_acc=0.5122\n",
      "iter= 7900, cost=0.5299, trn_acc=0.7146, val_acc=0.5106\n",
      "iter= 8000, cost=0.6654, trn_acc=0.7234, val_acc=0.5246\n",
      "iter= 8100, cost=0.5218, trn_acc=0.7325, val_acc=0.5132\n",
      "iter= 8200, cost=0.6294, trn_acc=0.7306, val_acc=0.5146\n",
      "iter= 8300, cost=0.7267, trn_acc=0.6516, val_acc=0.4950\n",
      "iter= 8400, cost=0.5399, trn_acc=0.7031, val_acc=0.4998\n",
      "iter= 8500, cost=0.6035, trn_acc=0.7099, val_acc=0.5086\n",
      "iter= 8600, cost=0.6399, trn_acc=0.7056, val_acc=0.5068\n",
      "iter= 8700, cost=0.5360, trn_acc=0.7204, val_acc=0.4944\n",
      "iter= 8800, cost=0.6716, trn_acc=0.7104, val_acc=0.5082\n",
      "iter= 8900, cost=0.4858, trn_acc=0.7298, val_acc=0.5078\n",
      "iter= 9000, cost=0.5447, trn_acc=0.7533, val_acc=0.5074\n",
      "iter= 9100, cost=0.5149, trn_acc=0.7286, val_acc=0.5176\n",
      "iter= 9200, cost=0.4522, trn_acc=0.7453, val_acc=0.5084\n",
      "iter= 9300, cost=0.6063, trn_acc=0.7522, val_acc=0.5118\n",
      "iter= 9400, cost=0.5672, trn_acc=0.7082, val_acc=0.5054\n",
      "iter= 9500, cost=0.5430, trn_acc=0.7241, val_acc=0.5032\n",
      "iter= 9600, cost=0.4095, trn_acc=0.7344, val_acc=0.5146\n",
      "iter= 9700, cost=0.4912, trn_acc=0.7092, val_acc=0.4976\n",
      "iter= 9800, cost=0.6344, trn_acc=0.7260, val_acc=0.4938\n",
      "iter= 9900, cost=0.4562, trn_acc=0.7671, val_acc=0.5158\n"
     ]
    }
   ],
   "source": [
    "# check if iterations are not enough: the answer is that 5000 is enough\n",
    "layer_dimensions = [X_train.shape[0], 1000, 200, 50, 10]\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "NN.load_validation_set(X_val, y_val)\n",
    "NN.train(X_trn, y_trn, iters=10000, alpha=1, batch_size=100, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "#### Simple fully-connected deep neural network\n",
    "#### Final output for the given test set (train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=    0, cost=2.3859, trn_acc=0.1160, val_acc=nan\n",
      "iter=  100, cost=2.0567, trn_acc=0.2779, val_acc=nan\n",
      "iter=  200, cost=1.9116, trn_acc=0.3363, val_acc=nan\n",
      "iter=  300, cost=1.7787, trn_acc=0.3665, val_acc=nan\n",
      "iter=  400, cost=1.6508, trn_acc=0.3790, val_acc=nan\n",
      "iter=  500, cost=1.7339, trn_acc=0.3990, val_acc=nan\n",
      "iter=  600, cost=1.4921, trn_acc=0.4213, val_acc=nan\n",
      "iter=  700, cost=1.6661, trn_acc=0.4277, val_acc=nan\n",
      "iter=  800, cost=1.6047, trn_acc=0.4303, val_acc=nan\n",
      "iter=  900, cost=1.5008, trn_acc=0.4422, val_acc=nan\n",
      "iter= 1000, cost=1.4384, trn_acc=0.4428, val_acc=nan\n",
      "iter= 1100, cost=1.4229, trn_acc=0.4596, val_acc=nan\n",
      "iter= 1200, cost=1.4369, trn_acc=0.4653, val_acc=nan\n",
      "iter= 1300, cost=1.4385, trn_acc=0.4718, val_acc=nan\n",
      "iter= 1400, cost=1.5543, trn_acc=0.4750, val_acc=nan\n",
      "iter= 1500, cost=1.1944, trn_acc=0.4910, val_acc=nan\n",
      "iter= 1600, cost=1.3153, trn_acc=0.4953, val_acc=nan\n",
      "iter= 1700, cost=1.4025, trn_acc=0.4898, val_acc=nan\n",
      "iter= 1800, cost=1.4529, trn_acc=0.5029, val_acc=nan\n",
      "iter= 1900, cost=1.2753, trn_acc=0.4900, val_acc=nan\n",
      "iter= 2000, cost=1.4271, trn_acc=0.5149, val_acc=nan\n",
      "iter= 2100, cost=1.3318, trn_acc=0.4973, val_acc=nan\n",
      "iter= 2200, cost=1.3199, trn_acc=0.5213, val_acc=nan\n",
      "iter= 2300, cost=1.2102, trn_acc=0.5313, val_acc=nan\n",
      "iter= 2400, cost=1.2707, trn_acc=0.5248, val_acc=nan\n",
      "iter= 2500, cost=1.2794, trn_acc=0.5258, val_acc=nan\n",
      "iter= 2600, cost=1.2518, trn_acc=0.5405, val_acc=nan\n",
      "iter= 2700, cost=1.2180, trn_acc=0.5457, val_acc=nan\n",
      "iter= 2800, cost=0.9707, trn_acc=0.5261, val_acc=nan\n",
      "iter= 2900, cost=1.1582, trn_acc=0.5382, val_acc=nan\n",
      "iter= 3000, cost=1.3152, trn_acc=0.5527, val_acc=nan\n",
      "iter= 3100, cost=1.2605, trn_acc=0.5349, val_acc=nan\n",
      "iter= 3200, cost=1.0938, trn_acc=0.5709, val_acc=nan\n",
      "iter= 3300, cost=1.1165, trn_acc=0.5607, val_acc=nan\n",
      "iter= 3400, cost=1.1191, trn_acc=0.5691, val_acc=nan\n",
      "iter= 3500, cost=1.2765, trn_acc=0.5694, val_acc=nan\n",
      "iter= 3600, cost=1.2029, trn_acc=0.5863, val_acc=nan\n",
      "iter= 3700, cost=1.1081, trn_acc=0.5772, val_acc=nan\n",
      "iter= 3800, cost=1.2046, trn_acc=0.5675, val_acc=nan\n",
      "iter= 3900, cost=1.0493, trn_acc=0.5925, val_acc=nan\n",
      "iter= 4000, cost=0.9161, trn_acc=0.5878, val_acc=nan\n",
      "iter= 4100, cost=0.9733, trn_acc=0.5951, val_acc=nan\n",
      "iter= 4200, cost=1.0001, trn_acc=0.5665, val_acc=nan\n",
      "iter= 4300, cost=1.0843, trn_acc=0.5948, val_acc=nan\n",
      "iter= 4400, cost=1.1302, trn_acc=0.6038, val_acc=nan\n",
      "iter= 4500, cost=0.9383, trn_acc=0.6069, val_acc=nan\n",
      "iter= 4600, cost=0.9355, trn_acc=0.6152, val_acc=nan\n",
      "iter= 4700, cost=1.0614, trn_acc=0.6198, val_acc=nan\n",
      "iter= 4800, cost=0.9498, trn_acc=0.6233, val_acc=nan\n",
      "iter= 4900, cost=1.0060, trn_acc=0.6093, val_acc=nan\n"
     ]
    }
   ],
   "source": [
    "layer_dimensions = [X_train.shape[0], 1000, 200, 50, 10]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "NN.train(X_train, y_train, iters=5000, alpha=1, batch_size=100, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted = NN.predict(X_test)\n",
    "save_predictions('ans1-uni', y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 8, 0, 4, 5, 3, 8, 4, 8, 1], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if your numpy file has been saved correctly\n",
    "loaded_y = np.load('ans1-uni.npy')\n",
    "print(loaded_y.shape)\n",
    "loaded_y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments for dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_trn, X_val, y_trn, y_val = split(X_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=    0, cost=2.3254, trn_acc=0.1193, val_acc=0.1128\n",
      "iter=  100, cost=1.9503, trn_acc=0.2760, val_acc=0.2888\n",
      "iter=  200, cost=1.8306, trn_acc=0.3285, val_acc=0.3382\n",
      "iter=  300, cost=1.7726, trn_acc=0.3604, val_acc=0.3738\n",
      "iter=  400, cost=1.8955, trn_acc=0.3877, val_acc=0.3980\n",
      "iter=  500, cost=1.5365, trn_acc=0.3962, val_acc=0.4088\n",
      "iter=  600, cost=1.5045, trn_acc=0.4075, val_acc=0.4202\n",
      "iter=  700, cost=1.6285, trn_acc=0.4246, val_acc=0.4270\n",
      "iter=  800, cost=1.6281, trn_acc=0.4379, val_acc=0.4342\n",
      "iter=  900, cost=1.5433, trn_acc=0.4473, val_acc=0.4438\n",
      "iter= 1000, cost=1.5082, trn_acc=0.4448, val_acc=0.4472\n",
      "iter= 1100, cost=1.5041, trn_acc=0.4607, val_acc=0.4446\n",
      "iter= 1200, cost=1.4156, trn_acc=0.4610, val_acc=0.4536\n",
      "iter= 1300, cost=1.5625, trn_acc=0.4747, val_acc=0.4592\n",
      "iter= 1400, cost=1.4482, trn_acc=0.4744, val_acc=0.4562\n",
      "iter= 1500, cost=1.5898, trn_acc=0.4816, val_acc=0.4624\n",
      "iter= 1600, cost=1.3320, trn_acc=0.4895, val_acc=0.4696\n",
      "iter= 1700, cost=1.5738, trn_acc=0.4896, val_acc=0.4700\n",
      "iter= 1800, cost=1.4721, trn_acc=0.4971, val_acc=0.4748\n",
      "iter= 1900, cost=1.5328, trn_acc=0.5041, val_acc=0.4754\n",
      "iter= 2000, cost=1.3006, trn_acc=0.5050, val_acc=0.4830\n",
      "iter= 2100, cost=1.5381, trn_acc=0.5135, val_acc=0.4826\n",
      "iter= 2200, cost=1.4098, trn_acc=0.5211, val_acc=0.4860\n",
      "iter= 2300, cost=1.3282, trn_acc=0.5098, val_acc=0.4890\n",
      "iter= 2400, cost=1.3110, trn_acc=0.5211, val_acc=0.4884\n",
      "iter= 2500, cost=1.2228, trn_acc=0.5350, val_acc=0.4914\n",
      "iter= 2600, cost=1.2240, trn_acc=0.5212, val_acc=0.4912\n",
      "iter= 2700, cost=1.2891, trn_acc=0.5385, val_acc=0.4928\n",
      "iter= 2800, cost=1.4793, trn_acc=0.5444, val_acc=0.5026\n",
      "iter= 2900, cost=1.5299, trn_acc=0.5391, val_acc=0.4938\n",
      "iter= 3000, cost=1.4805, trn_acc=0.5508, val_acc=0.5012\n",
      "iter= 3100, cost=1.2315, trn_acc=0.5528, val_acc=0.4976\n",
      "iter= 3200, cost=1.1493, trn_acc=0.5628, val_acc=0.5050\n",
      "iter= 3300, cost=1.2373, trn_acc=0.5667, val_acc=0.5064\n",
      "iter= 3400, cost=1.2145, trn_acc=0.5548, val_acc=0.5002\n",
      "iter= 3500, cost=1.1988, trn_acc=0.5634, val_acc=0.5074\n",
      "iter= 3600, cost=1.3188, trn_acc=0.5644, val_acc=0.5120\n",
      "iter= 3700, cost=1.2598, trn_acc=0.5672, val_acc=0.5072\n",
      "iter= 3800, cost=1.0888, trn_acc=0.5715, val_acc=0.5076\n",
      "iter= 3900, cost=1.2636, trn_acc=0.5763, val_acc=0.5008\n",
      "iter= 4000, cost=1.2186, trn_acc=0.5860, val_acc=0.5110\n",
      "iter= 4100, cost=1.1401, trn_acc=0.5826, val_acc=0.5130\n",
      "iter= 4200, cost=1.0317, trn_acc=0.5868, val_acc=0.5188\n",
      "iter= 4300, cost=1.1356, trn_acc=0.5831, val_acc=0.5162\n",
      "iter= 4400, cost=1.2691, trn_acc=0.5945, val_acc=0.5110\n",
      "iter= 4500, cost=1.2025, trn_acc=0.5959, val_acc=0.5254\n",
      "iter= 4600, cost=1.0758, trn_acc=0.5998, val_acc=0.5164\n",
      "iter= 4700, cost=0.8758, trn_acc=0.5987, val_acc=0.5180\n",
      "iter= 4800, cost=1.1118, trn_acc=0.6121, val_acc=0.5290\n",
      "iter= 4900, cost=1.2468, trn_acc=0.6171, val_acc=0.5236\n",
      "iter= 5000, cost=1.0341, trn_acc=0.6015, val_acc=0.5196\n",
      "iter= 5100, cost=1.1422, trn_acc=0.6218, val_acc=0.5242\n",
      "iter= 5200, cost=1.0937, trn_acc=0.6124, val_acc=0.5192\n",
      "iter= 5300, cost=1.2345, trn_acc=0.6045, val_acc=0.5156\n",
      "iter= 5400, cost=0.9933, trn_acc=0.6180, val_acc=0.5234\n",
      "iter= 5500, cost=1.0574, trn_acc=0.6248, val_acc=0.5152\n",
      "iter= 5600, cost=1.2325, trn_acc=0.6237, val_acc=0.5252\n",
      "iter= 5700, cost=0.9414, trn_acc=0.6182, val_acc=0.5288\n",
      "iter= 5800, cost=0.9904, trn_acc=0.6295, val_acc=0.5238\n",
      "iter= 5900, cost=1.1475, trn_acc=0.6450, val_acc=0.5346\n",
      "iter= 6000, cost=1.1419, trn_acc=0.6368, val_acc=0.5254\n",
      "iter= 6100, cost=1.0416, trn_acc=0.6184, val_acc=0.5192\n",
      "iter= 6200, cost=0.9532, trn_acc=0.6536, val_acc=0.5326\n",
      "iter= 6300, cost=0.9578, trn_acc=0.6512, val_acc=0.5276\n",
      "iter= 6400, cost=1.0895, trn_acc=0.6576, val_acc=0.5298\n",
      "iter= 6500, cost=1.1041, trn_acc=0.6543, val_acc=0.5218\n",
      "iter= 6600, cost=1.0100, trn_acc=0.6601, val_acc=0.5308\n",
      "iter= 6700, cost=0.9848, trn_acc=0.6542, val_acc=0.5318\n",
      "iter= 6800, cost=1.1064, trn_acc=0.6492, val_acc=0.5238\n",
      "iter= 6900, cost=1.0191, trn_acc=0.6676, val_acc=0.5412\n",
      "iter= 7000, cost=0.9814, trn_acc=0.6732, val_acc=0.5346\n",
      "iter= 7100, cost=0.9760, trn_acc=0.6745, val_acc=0.5262\n",
      "iter= 7200, cost=0.9754, trn_acc=0.6704, val_acc=0.5316\n",
      "iter= 7300, cost=1.0378, trn_acc=0.6720, val_acc=0.5314\n",
      "iter= 7400, cost=1.0192, trn_acc=0.6675, val_acc=0.5298\n",
      "iter= 7500, cost=1.0578, trn_acc=0.6752, val_acc=0.5276\n",
      "iter= 7600, cost=1.0355, trn_acc=0.6649, val_acc=0.5316\n",
      "iter= 7700, cost=1.0022, trn_acc=0.6653, val_acc=0.5320\n",
      "iter= 7800, cost=0.8764, trn_acc=0.6806, val_acc=0.5326\n",
      "iter= 7900, cost=0.8042, trn_acc=0.6744, val_acc=0.5318\n",
      "iter= 8000, cost=1.0206, trn_acc=0.6677, val_acc=0.5358\n",
      "iter= 8100, cost=0.7916, trn_acc=0.7011, val_acc=0.5328\n",
      "iter= 8200, cost=0.7750, trn_acc=0.6941, val_acc=0.5368\n",
      "iter= 8300, cost=1.0444, trn_acc=0.6988, val_acc=0.5270\n",
      "iter= 8400, cost=0.8562, trn_acc=0.7021, val_acc=0.5298\n",
      "iter= 8500, cost=0.9918, trn_acc=0.6982, val_acc=0.5396\n",
      "iter= 8600, cost=0.8604, trn_acc=0.7070, val_acc=0.5306\n",
      "iter= 8700, cost=0.6842, trn_acc=0.7085, val_acc=0.5348\n",
      "iter= 8800, cost=0.9353, trn_acc=0.7106, val_acc=0.5396\n",
      "iter= 8900, cost=0.7400, trn_acc=0.7124, val_acc=0.5378\n",
      "iter= 9000, cost=0.8276, trn_acc=0.7094, val_acc=0.5290\n",
      "iter= 9100, cost=0.7969, trn_acc=0.7117, val_acc=0.5396\n",
      "iter= 9200, cost=0.7565, trn_acc=0.7094, val_acc=0.5364\n",
      "iter= 9300, cost=0.7352, trn_acc=0.7236, val_acc=0.5366\n",
      "iter= 9400, cost=0.7257, trn_acc=0.7273, val_acc=0.5374\n",
      "iter= 9500, cost=0.8950, trn_acc=0.7220, val_acc=0.5326\n",
      "iter= 9600, cost=0.7748, trn_acc=0.7173, val_acc=0.5350\n",
      "iter= 9700, cost=0.9061, trn_acc=0.7319, val_acc=0.5388\n",
      "iter= 9800, cost=0.7805, trn_acc=0.7303, val_acc=0.5364\n",
      "iter= 9900, cost=0.7558, trn_acc=0.7299, val_acc=0.5414\n"
     ]
    }
   ],
   "source": [
    "layer_dimensions = [X_train.shape[0], 1000, 200, 50, 10]\n",
    "NN = NeuralNetwork(layer_dimensions, drop_prob=0.065, reg_lambda=0.0)\n",
    "NN.load_validation_set(X_val, y_val)\n",
    "# 4900 iter to 0.5194\n",
    "NN.train(X_trn, y_trn, iters=10000, alpha=1, batch_size=100, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=    0, cost=2.7273, trn_acc=0.1178, val_acc=0.1128\n",
      "iter=  100, cost=2.2517, trn_acc=0.2331, val_acc=0.2386\n",
      "iter=  200, cost=2.2968, trn_acc=0.2484, val_acc=0.2542\n",
      "iter=  300, cost=2.1033, trn_acc=0.2768, val_acc=0.2814\n",
      "iter=  400, cost=2.1803, trn_acc=0.2820, val_acc=0.2996\n",
      "iter=  500, cost=2.0181, trn_acc=0.3000, val_acc=0.3136\n",
      "iter=  600, cost=2.0213, trn_acc=0.3160, val_acc=0.3292\n",
      "iter=  700, cost=2.0101, trn_acc=0.3249, val_acc=0.3348\n",
      "iter=  800, cost=2.1024, trn_acc=0.3404, val_acc=0.3432\n",
      "iter=  900, cost=2.0388, trn_acc=0.3476, val_acc=0.3508\n",
      "iter= 1000, cost=1.9798, trn_acc=0.3542, val_acc=0.3602\n",
      "iter= 1100, cost=1.9242, trn_acc=0.3546, val_acc=0.3582\n",
      "iter= 1200, cost=1.9046, trn_acc=0.3685, val_acc=0.3710\n",
      "iter= 1300, cost=1.9298, trn_acc=0.3683, val_acc=0.3746\n",
      "iter= 1400, cost=1.9494, trn_acc=0.3704, val_acc=0.3736\n",
      "iter= 1500, cost=2.0656, trn_acc=0.3769, val_acc=0.3800\n",
      "iter= 1600, cost=1.8559, trn_acc=0.3782, val_acc=0.3822\n",
      "iter= 1700, cost=1.8782, trn_acc=0.3869, val_acc=0.3772\n",
      "iter= 1800, cost=1.8551, trn_acc=0.3850, val_acc=0.3864\n",
      "iter= 1900, cost=1.8325, trn_acc=0.3962, val_acc=0.3944\n",
      "iter= 2000, cost=1.8586, trn_acc=0.3968, val_acc=0.3916\n",
      "iter= 2100, cost=1.9367, trn_acc=0.3961, val_acc=0.3934\n",
      "iter= 2200, cost=1.8381, trn_acc=0.3970, val_acc=0.3952\n",
      "iter= 2300, cost=1.8108, trn_acc=0.3984, val_acc=0.4064\n",
      "iter= 2400, cost=1.7563, trn_acc=0.4044, val_acc=0.4062\n",
      "iter= 2500, cost=1.6648, trn_acc=0.4138, val_acc=0.4104\n",
      "iter= 2600, cost=1.7451, trn_acc=0.4002, val_acc=0.4042\n",
      "iter= 2700, cost=1.8136, trn_acc=0.4097, val_acc=0.4126\n",
      "iter= 2800, cost=1.9171, trn_acc=0.4152, val_acc=0.4126\n",
      "iter= 2900, cost=2.0119, trn_acc=0.4178, val_acc=0.4160\n",
      "iter= 3000, cost=1.8733, trn_acc=0.4144, val_acc=0.4148\n",
      "iter= 3100, cost=1.8489, trn_acc=0.4182, val_acc=0.4148\n",
      "iter= 3200, cost=1.7505, trn_acc=0.4210, val_acc=0.4134\n",
      "iter= 3300, cost=1.8697, trn_acc=0.4287, val_acc=0.4214\n",
      "iter= 3400, cost=1.9606, trn_acc=0.4284, val_acc=0.4200\n",
      "iter= 3500, cost=1.7485, trn_acc=0.4250, val_acc=0.4228\n",
      "iter= 3600, cost=1.8251, trn_acc=0.4236, val_acc=0.4204\n",
      "iter= 3700, cost=1.7467, trn_acc=0.4290, val_acc=0.4282\n",
      "iter= 3800, cost=1.6673, trn_acc=0.4248, val_acc=0.4230\n",
      "iter= 3900, cost=2.0063, trn_acc=0.4284, val_acc=0.4336\n",
      "iter= 4000, cost=1.7635, trn_acc=0.4353, val_acc=0.4314\n",
      "iter= 4100, cost=1.6857, trn_acc=0.4329, val_acc=0.4260\n",
      "iter= 4200, cost=1.6106, trn_acc=0.4340, val_acc=0.4372\n",
      "iter= 4300, cost=1.6197, trn_acc=0.4361, val_acc=0.4364\n",
      "iter= 4400, cost=1.7939, trn_acc=0.4337, val_acc=0.4380\n",
      "iter= 4500, cost=1.7624, trn_acc=0.4337, val_acc=0.4382\n",
      "iter= 4600, cost=1.8165, trn_acc=0.4426, val_acc=0.4376\n",
      "iter= 4700, cost=1.5043, trn_acc=0.4401, val_acc=0.4368\n",
      "iter= 4800, cost=1.6441, trn_acc=0.4428, val_acc=0.4392\n",
      "iter= 4900, cost=1.7399, trn_acc=0.4420, val_acc=0.4392\n",
      "iter= 5000, cost=1.7465, trn_acc=0.4437, val_acc=0.4440\n",
      "iter= 5100, cost=1.8261, trn_acc=0.4475, val_acc=0.4426\n",
      "iter= 5200, cost=1.7074, trn_acc=0.4486, val_acc=0.4462\n",
      "iter= 5300, cost=1.6681, trn_acc=0.4485, val_acc=0.4480\n",
      "iter= 5400, cost=1.6081, trn_acc=0.4510, val_acc=0.4502\n",
      "iter= 5500, cost=1.7571, trn_acc=0.4565, val_acc=0.4544\n",
      "iter= 5600, cost=1.9539, trn_acc=0.4558, val_acc=0.4490\n",
      "iter= 5700, cost=1.5703, trn_acc=0.4530, val_acc=0.4540\n",
      "iter= 5800, cost=1.7543, trn_acc=0.4577, val_acc=0.4516\n",
      "iter= 5900, cost=1.7276, trn_acc=0.4551, val_acc=0.4500\n"
     ]
    }
   ],
   "source": [
    "layer_dimensions = [X_train.shape[0], 1000, 200, 50, 10]\n",
    "NN = NeuralNetwork(layer_dimensions, drop_prob=0.5, reg_lambda=0.0)\n",
    "NN.load_validation_set(X_val, y_val)\n",
    "NN.train(X_trn, y_trn, iters=6000, alpha=1, batch_size=100, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=    0, cost=2.3416, trn_acc=0.1182, val_acc=0.1112\n",
      "iter=  100, cost=1.9510, trn_acc=0.2706, val_acc=0.2850\n",
      "iter=  200, cost=1.9061, trn_acc=0.3251, val_acc=0.3370\n",
      "iter=  300, cost=1.8005, trn_acc=0.3552, val_acc=0.3670\n",
      "iter=  400, cost=1.9061, trn_acc=0.3828, val_acc=0.3928\n",
      "iter=  500, cost=1.5840, trn_acc=0.3941, val_acc=0.4026\n",
      "iter=  600, cost=1.5878, trn_acc=0.3991, val_acc=0.4138\n",
      "iter=  700, cost=1.6535, trn_acc=0.4186, val_acc=0.4228\n",
      "iter=  800, cost=1.6627, trn_acc=0.4275, val_acc=0.4340\n",
      "iter=  900, cost=1.5304, trn_acc=0.4399, val_acc=0.4390\n",
      "iter= 1000, cost=1.5391, trn_acc=0.4386, val_acc=0.4432\n",
      "iter= 1100, cost=1.5205, trn_acc=0.4537, val_acc=0.4476\n",
      "iter= 1200, cost=1.4340, trn_acc=0.4537, val_acc=0.4508\n",
      "iter= 1300, cost=1.5837, trn_acc=0.4672, val_acc=0.4554\n",
      "iter= 1400, cost=1.4943, trn_acc=0.4695, val_acc=0.4560\n",
      "iter= 1500, cost=1.6310, trn_acc=0.4756, val_acc=0.4550\n",
      "iter= 1600, cost=1.4170, trn_acc=0.4814, val_acc=0.4638\n",
      "iter= 1700, cost=1.5872, trn_acc=0.4829, val_acc=0.4678\n",
      "iter= 1800, cost=1.5566, trn_acc=0.4916, val_acc=0.4656\n",
      "iter= 1900, cost=1.5628, trn_acc=0.4949, val_acc=0.4700\n",
      "iter= 2000, cost=1.3371, trn_acc=0.4969, val_acc=0.4764\n",
      "iter= 2100, cost=1.6010, trn_acc=0.5068, val_acc=0.4778\n",
      "iter= 2200, cost=1.4721, trn_acc=0.5112, val_acc=0.4806\n",
      "iter= 2300, cost=1.3790, trn_acc=0.5020, val_acc=0.4834\n",
      "iter= 2400, cost=1.3673, trn_acc=0.5106, val_acc=0.4774\n",
      "iter= 2500, cost=1.2689, trn_acc=0.5220, val_acc=0.4858\n",
      "iter= 2600, cost=1.2897, trn_acc=0.5161, val_acc=0.4876\n",
      "iter= 2700, cost=1.3430, trn_acc=0.5281, val_acc=0.4932\n",
      "iter= 2800, cost=1.5530, trn_acc=0.5378, val_acc=0.4964\n",
      "iter= 2900, cost=1.6410, trn_acc=0.5321, val_acc=0.4940\n",
      "iter= 3000, cost=1.5327, trn_acc=0.5408, val_acc=0.4978\n",
      "iter= 3100, cost=1.2960, trn_acc=0.5471, val_acc=0.4960\n",
      "iter= 3200, cost=1.2650, trn_acc=0.5517, val_acc=0.4988\n",
      "iter= 3300, cost=1.2934, trn_acc=0.5550, val_acc=0.5018\n",
      "iter= 3400, cost=1.2967, trn_acc=0.5468, val_acc=0.4964\n",
      "iter= 3500, cost=1.2843, trn_acc=0.5490, val_acc=0.5006\n",
      "iter= 3600, cost=1.3507, trn_acc=0.5521, val_acc=0.5052\n",
      "iter= 3700, cost=1.3558, trn_acc=0.5550, val_acc=0.4962\n",
      "iter= 3800, cost=1.1556, trn_acc=0.5549, val_acc=0.5040\n",
      "iter= 3900, cost=1.2455, trn_acc=0.5634, val_acc=0.5000\n",
      "iter= 4000, cost=1.2726, trn_acc=0.5722, val_acc=0.5054\n",
      "iter= 4100, cost=1.1946, trn_acc=0.5695, val_acc=0.5064\n",
      "iter= 4200, cost=1.0525, trn_acc=0.5736, val_acc=0.5118\n",
      "iter= 4300, cost=1.2201, trn_acc=0.5687, val_acc=0.5126\n",
      "iter= 4400, cost=1.3726, trn_acc=0.5787, val_acc=0.5144\n",
      "iter= 4500, cost=1.2151, trn_acc=0.5858, val_acc=0.5148\n",
      "iter= 4600, cost=1.1447, trn_acc=0.5873, val_acc=0.5194\n",
      "iter= 4700, cost=0.9403, trn_acc=0.5808, val_acc=0.5114\n",
      "iter= 4800, cost=1.1917, trn_acc=0.5927, val_acc=0.5266\n",
      "iter= 4900, cost=1.3438, trn_acc=0.5962, val_acc=0.5144\n",
      "iter= 5000, cost=1.1580, trn_acc=0.5872, val_acc=0.5168\n",
      "iter= 5100, cost=1.1742, trn_acc=0.6043, val_acc=0.5222\n",
      "iter= 5200, cost=1.1489, trn_acc=0.5952, val_acc=0.5146\n",
      "iter= 5300, cost=1.3172, trn_acc=0.5909, val_acc=0.5152\n",
      "iter= 5400, cost=1.1223, trn_acc=0.6044, val_acc=0.5196\n",
      "iter= 5500, cost=1.1222, trn_acc=0.6046, val_acc=0.5126\n",
      "iter= 5600, cost=1.3350, trn_acc=0.6064, val_acc=0.5276\n",
      "iter= 5700, cost=0.9152, trn_acc=0.6023, val_acc=0.5236\n",
      "iter= 5800, cost=1.1070, trn_acc=0.6153, val_acc=0.5236\n",
      "iter= 5900, cost=1.1758, trn_acc=0.6221, val_acc=0.5286\n",
      "iter= 6000, cost=1.1714, trn_acc=0.6220, val_acc=0.5246\n",
      "iter= 6100, cost=1.0916, trn_acc=0.6103, val_acc=0.5218\n",
      "iter= 6200, cost=1.0158, trn_acc=0.6328, val_acc=0.5266\n",
      "iter= 6300, cost=1.0609, trn_acc=0.6242, val_acc=0.5258\n",
      "iter= 6400, cost=1.1646, trn_acc=0.6378, val_acc=0.5272\n",
      "iter= 6500, cost=1.2650, trn_acc=0.6335, val_acc=0.5274\n",
      "iter= 6600, cost=1.0384, trn_acc=0.6391, val_acc=0.5284\n",
      "iter= 6700, cost=1.0369, trn_acc=0.6312, val_acc=0.5306\n",
      "iter= 6800, cost=1.2856, trn_acc=0.6326, val_acc=0.5224\n",
      "iter= 6900, cost=1.1246, trn_acc=0.6456, val_acc=0.5346\n",
      "iter= 7000, cost=1.0656, trn_acc=0.6563, val_acc=0.5366\n",
      "iter= 7100, cost=1.0843, trn_acc=0.6568, val_acc=0.5384\n",
      "iter= 7200, cost=1.0849, trn_acc=0.6506, val_acc=0.5310\n",
      "iter= 7300, cost=1.0469, trn_acc=0.6533, val_acc=0.5354\n",
      "iter= 7400, cost=1.0862, trn_acc=0.6529, val_acc=0.5354\n",
      "iter= 7500, cost=1.1979, trn_acc=0.6598, val_acc=0.5364\n",
      "iter= 7600, cost=1.1099, trn_acc=0.6477, val_acc=0.5430\n",
      "iter= 7700, cost=1.0871, trn_acc=0.6440, val_acc=0.5410\n",
      "iter= 7800, cost=0.9755, trn_acc=0.6540, val_acc=0.5316\n",
      "iter= 7900, cost=0.8960, trn_acc=0.6523, val_acc=0.5406\n",
      "iter= 8000, cost=1.0897, trn_acc=0.6501, val_acc=0.5388\n",
      "iter= 8100, cost=0.8928, trn_acc=0.6732, val_acc=0.5410\n",
      "iter= 8200, cost=0.9544, trn_acc=0.6739, val_acc=0.5342\n",
      "iter= 8300, cost=1.2138, trn_acc=0.6837, val_acc=0.5346\n",
      "iter= 8400, cost=0.9574, trn_acc=0.6756, val_acc=0.5308\n",
      "iter= 8500, cost=1.0934, trn_acc=0.6743, val_acc=0.5402\n",
      "iter= 8600, cost=0.9683, trn_acc=0.6852, val_acc=0.5294\n",
      "iter= 8700, cost=0.8477, trn_acc=0.6741, val_acc=0.5380\n",
      "iter= 8800, cost=1.0216, trn_acc=0.6882, val_acc=0.5450\n",
      "iter= 8900, cost=0.8800, trn_acc=0.6899, val_acc=0.5400\n",
      "iter= 9000, cost=0.8956, trn_acc=0.6912, val_acc=0.5328\n",
      "iter= 9100, cost=0.8374, trn_acc=0.6930, val_acc=0.5388\n",
      "iter= 9200, cost=0.8696, trn_acc=0.6836, val_acc=0.5316\n",
      "iter= 9300, cost=0.8183, trn_acc=0.6952, val_acc=0.5390\n",
      "iter= 9400, cost=0.9026, trn_acc=0.6985, val_acc=0.5422\n",
      "iter= 9500, cost=0.9356, trn_acc=0.6860, val_acc=0.5382\n",
      "iter= 9600, cost=0.7709, trn_acc=0.7053, val_acc=0.5394\n",
      "iter= 9700, cost=1.0304, trn_acc=0.6945, val_acc=0.5414\n",
      "iter= 9800, cost=0.9582, trn_acc=0.7036, val_acc=0.5406\n",
      "iter= 9900, cost=0.8953, trn_acc=0.7009, val_acc=0.5420\n"
     ]
    }
   ],
   "source": [
    "layer_dimensions = [X_train.shape[0], 1000, 200, 50, 10]\n",
    "NN = NeuralNetwork(layer_dimensions, drop_prob=0.1, reg_lambda=0.0)\n",
    "NN.load_validation_set(X_val, y_val)\n",
    "NN.train(X_trn, y_trn, iters=10000, alpha=1, batch_size=100, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for L2 regularization\n",
    "0.05, 0.005, 0.001 all too large, 10\\*\\*(-4) seems acceptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=    0, cost=33.8131, trn_acc=0.1192, val_acc=0.1136\n",
      "iter=  100, cost=2.3037, trn_acc=0.1536, val_acc=0.1702\n",
      "iter=  200, cost=2.3026, trn_acc=0.1536, val_acc=0.1702\n",
      "iter=  300, cost=2.3026, trn_acc=0.1536, val_acc=0.1702\n",
      "iter=  400, cost=2.3026, trn_acc=0.1536, val_acc=0.1702\n",
      "iter=  500, cost=2.3026, trn_acc=0.1536, val_acc=0.1702\n",
      "iter=  600, cost=2.3026, trn_acc=0.1536, val_acc=0.1702\n",
      "iter=  700, cost=2.3026, trn_acc=0.1536, val_acc=0.1702\n",
      "iter=  800, cost=2.3026, trn_acc=0.1536, val_acc=0.1702\n",
      "iter=  900, cost=2.3026, trn_acc=0.1536, val_acc=0.1702\n"
     ]
    }
   ],
   "source": [
    "# Test the L2 Regularization: 0.05 is too large\n",
    "layer_dimensions = [X_train.shape[0], 1000, 200, 50, 10]\n",
    "NN = NeuralNetwork(layer_dimensions, drop_prob=0.0, reg_lambda=0.05)\n",
    "NN.load_validation_set(X_val, y_val)\n",
    "NN.train(X_trn, y_trn, iters=1000, alpha=1, batch_size=100, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=    0, cost=5.4393, trn_acc=0.1190, val_acc=0.1136\n",
      "iter=  100, cost=3.3789, trn_acc=0.2213, val_acc=0.2378\n",
      "iter=  200, cost=2.6995, trn_acc=0.2178, val_acc=0.2162\n",
      "iter=  300, cost=2.4548, trn_acc=0.2141, val_acc=0.2016\n",
      "iter=  400, cost=2.3591, trn_acc=0.2107, val_acc=0.1942\n",
      "iter=  500, cost=2.3235, trn_acc=0.2093, val_acc=0.1908\n",
      "iter=  600, cost=2.3103, trn_acc=0.2088, val_acc=0.1898\n",
      "iter=  700, cost=2.3054, trn_acc=0.2087, val_acc=0.1894\n",
      "iter=  800, cost=2.3036, trn_acc=0.2087, val_acc=0.1894\n",
      "iter=  900, cost=2.3030, trn_acc=0.2086, val_acc=0.1896\n",
      "iter= 1000, cost=2.3027, trn_acc=0.2086, val_acc=0.1896\n",
      "iter= 1100, cost=2.3026, trn_acc=0.2086, val_acc=0.1896\n",
      "iter= 1200, cost=2.3026, trn_acc=0.2086, val_acc=0.1896\n",
      "iter= 1300, cost=2.3026, trn_acc=0.2086, val_acc=0.1896\n",
      "iter= 1400, cost=2.3026, trn_acc=0.2086, val_acc=0.1896\n",
      "iter= 1500, cost=2.3026, trn_acc=0.2086, val_acc=0.1896\n",
      "iter= 1600, cost=2.3026, trn_acc=0.2086, val_acc=0.1896\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-b5d9f9b58b46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mNN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_dimensions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_lambda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.005\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_validation_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_trn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_trn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-eda6deef46ba>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y, iters, alpha, batch_size, print_every)\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforwardPropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[1;31m# compute loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m             \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdAL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcostFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m             \u001b[1;31m# compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackPropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-eda6deef46ba>\u001b[0m in \u001b[0;36mcostFunction\u001b[1;34m(self, AL, y)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# add regularization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m                 \u001b[0mcost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg_lambda\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'W'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[0mdAL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mdAL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer_dimensions = [X_train.shape[0], 1000, 200, 50, 10]\n",
    "NN = NeuralNetwork(layer_dimensions, drop_prob=0.0, reg_lambda=0.005)\n",
    "NN.load_validation_set(X_val, y_val)\n",
    "NN.train(X_trn, y_trn, iters=5000, alpha=1, batch_size=100, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=    0, cost=2.9172, trn_acc=0.1189, val_acc=0.1134\n",
      "iter=  100, cost=2.5606, trn_acc=0.2551, val_acc=0.2758\n",
      "iter=  200, cost=2.4062, trn_acc=0.2928, val_acc=0.3066\n",
      "iter=  300, cost=2.3148, trn_acc=0.3274, val_acc=0.3412\n",
      "iter=  400, cost=2.2519, trn_acc=0.3356, val_acc=0.3496\n",
      "iter=  500, cost=2.0762, trn_acc=0.3445, val_acc=0.3616\n",
      "iter=  600, cost=1.9950, trn_acc=0.3412, val_acc=0.3622\n",
      "iter=  700, cost=1.9583, trn_acc=0.3463, val_acc=0.3540\n",
      "iter=  800, cost=1.9567, trn_acc=0.3316, val_acc=0.3538\n",
      "iter=  900, cost=1.9653, trn_acc=0.3269, val_acc=0.3472\n",
      "iter= 1000, cost=1.9317, trn_acc=0.3341, val_acc=0.3476\n",
      "iter= 1100, cost=2.0324, trn_acc=0.3305, val_acc=0.3454\n",
      "iter= 1200, cost=1.9130, trn_acc=0.3188, val_acc=0.3436\n",
      "iter= 1300, cost=1.9648, trn_acc=0.3272, val_acc=0.3428\n",
      "iter= 1400, cost=1.8840, trn_acc=0.3215, val_acc=0.3382\n",
      "iter= 1500, cost=1.8023, trn_acc=0.3297, val_acc=0.3370\n",
      "iter= 1600, cost=1.8596, trn_acc=0.3266, val_acc=0.3370\n",
      "iter= 1700, cost=1.8643, trn_acc=0.3107, val_acc=0.3252\n",
      "iter= 1800, cost=1.9814, trn_acc=0.3221, val_acc=0.3230\n",
      "iter= 1900, cost=1.9156, trn_acc=0.3152, val_acc=0.3170\n",
      "iter= 2000, cost=1.9006, trn_acc=0.3190, val_acc=0.3254\n",
      "iter= 2100, cost=1.9257, trn_acc=0.3108, val_acc=0.3204\n",
      "iter= 2200, cost=1.9306, trn_acc=0.3059, val_acc=0.3180\n",
      "iter= 2300, cost=1.8458, trn_acc=0.3047, val_acc=0.3178\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-5ad6aee72817>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mNN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_dimensions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_lambda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_validation_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_trn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_trn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-eda6deef46ba>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y, iters, alpha, batch_size, print_every)\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforwardPropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[1;31m# compute loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m             \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdAL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcostFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m             \u001b[1;31m# compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackPropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-eda6deef46ba>\u001b[0m in \u001b[0;36mcostFunction\u001b[1;34m(self, AL, y)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# add regularization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m                 \u001b[0mcost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg_lambda\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'W'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[0mdAL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mdAL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer_dimensions = [X_train.shape[0], 1000, 200, 50, 10]\n",
    "NN = NeuralNetwork(layer_dimensions, drop_prob=0.0, reg_lambda=0.001)\n",
    "NN.load_validation_set(X_val, y_val)\n",
    "NN.train(X_trn, y_trn, iters=5000, alpha=1, batch_size=100, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=    0, cost=2.3497, trn_acc=0.1189, val_acc=0.1134\n",
      "iter=  100, cost=2.0610, trn_acc=0.2643, val_acc=0.2826\n",
      "iter=  200, cost=1.9572, trn_acc=0.3190, val_acc=0.3388\n",
      "iter=  300, cost=1.8653, trn_acc=0.3663, val_acc=0.3794\n",
      "iter=  400, cost=1.8422, trn_acc=0.3743, val_acc=0.3964\n",
      "iter=  500, cost=1.6808, trn_acc=0.3976, val_acc=0.4142\n",
      "iter=  600, cost=1.6373, trn_acc=0.4057, val_acc=0.4222\n",
      "iter=  700, cost=1.5681, trn_acc=0.4219, val_acc=0.4256\n",
      "iter=  800, cost=1.5769, trn_acc=0.4282, val_acc=0.4390\n",
      "iter=  900, cost=1.5736, trn_acc=0.4295, val_acc=0.4396\n",
      "iter= 1000, cost=1.4976, trn_acc=0.4476, val_acc=0.4416\n",
      "iter= 1100, cost=1.5790, trn_acc=0.4341, val_acc=0.4518\n",
      "iter= 1200, cost=1.5367, trn_acc=0.4570, val_acc=0.4542\n",
      "iter= 1300, cost=1.5746, trn_acc=0.4614, val_acc=0.4640\n",
      "iter= 1400, cost=1.4248, trn_acc=0.4668, val_acc=0.4652\n",
      "iter= 1500, cost=1.2521, trn_acc=0.4848, val_acc=0.4744\n",
      "iter= 1600, cost=1.2945, trn_acc=0.4826, val_acc=0.4696\n",
      "iter= 1700, cost=1.4142, trn_acc=0.4809, val_acc=0.4790\n",
      "iter= 1800, cost=1.5205, trn_acc=0.4927, val_acc=0.4748\n",
      "iter= 1900, cost=1.4992, trn_acc=0.4916, val_acc=0.4688\n",
      "iter= 2000, cost=1.4246, trn_acc=0.5024, val_acc=0.4826\n",
      "iter= 2100, cost=1.4616, trn_acc=0.5050, val_acc=0.4776\n",
      "iter= 2200, cost=1.3183, trn_acc=0.4973, val_acc=0.4792\n",
      "iter= 2300, cost=1.3277, trn_acc=0.4902, val_acc=0.4826\n",
      "iter= 2400, cost=1.3347, trn_acc=0.5102, val_acc=0.4842\n",
      "iter= 2500, cost=1.2539, trn_acc=0.5132, val_acc=0.4896\n",
      "iter= 2600, cost=1.4395, trn_acc=0.5243, val_acc=0.4974\n",
      "iter= 2700, cost=1.3737, trn_acc=0.5193, val_acc=0.4984\n",
      "iter= 2800, cost=1.5662, trn_acc=0.5028, val_acc=0.4868\n",
      "iter= 2900, cost=1.5551, trn_acc=0.5245, val_acc=0.5020\n",
      "iter= 3000, cost=1.2925, trn_acc=0.5122, val_acc=0.4928\n",
      "iter= 3100, cost=1.3287, trn_acc=0.5318, val_acc=0.5048\n",
      "iter= 3200, cost=1.3155, trn_acc=0.5404, val_acc=0.5062\n",
      "iter= 3300, cost=1.3174, trn_acc=0.5378, val_acc=0.5032\n",
      "iter= 3400, cost=1.2090, trn_acc=0.5392, val_acc=0.5050\n",
      "iter= 3500, cost=1.1012, trn_acc=0.5465, val_acc=0.5008\n",
      "iter= 3600, cost=1.3086, trn_acc=0.5402, val_acc=0.5034\n",
      "iter= 3700, cost=1.2595, trn_acc=0.5507, val_acc=0.5050\n",
      "iter= 3800, cost=1.1576, trn_acc=0.5592, val_acc=0.5030\n",
      "iter= 3900, cost=1.1020, trn_acc=0.5471, val_acc=0.5116\n",
      "iter= 4000, cost=1.3440, trn_acc=0.5612, val_acc=0.5050\n",
      "iter= 4100, cost=1.1775, trn_acc=0.5507, val_acc=0.5134\n",
      "iter= 4200, cost=1.2116, trn_acc=0.5692, val_acc=0.5170\n",
      "iter= 4300, cost=1.0543, trn_acc=0.5584, val_acc=0.5050\n",
      "iter= 4400, cost=1.1544, trn_acc=0.5703, val_acc=0.5132\n",
      "iter= 4500, cost=1.2002, trn_acc=0.5659, val_acc=0.5206\n",
      "iter= 4600, cost=1.1727, trn_acc=0.5600, val_acc=0.5168\n",
      "iter= 4700, cost=1.1284, trn_acc=0.5593, val_acc=0.5050\n",
      "iter= 4800, cost=1.3097, trn_acc=0.5788, val_acc=0.5160\n",
      "iter= 4900, cost=0.9063, trn_acc=0.5734, val_acc=0.5092\n",
      "iter= 5000, cost=1.2458, trn_acc=0.5757, val_acc=0.5154\n",
      "iter= 5100, cost=1.2529, trn_acc=0.5693, val_acc=0.5194\n",
      "iter= 5200, cost=1.0251, trn_acc=0.5701, val_acc=0.5158\n",
      "iter= 5300, cost=1.1999, trn_acc=0.5842, val_acc=0.5200\n",
      "iter= 5400, cost=1.2014, trn_acc=0.5790, val_acc=0.5188\n",
      "iter= 5500, cost=1.0741, trn_acc=0.5870, val_acc=0.5264\n",
      "iter= 5600, cost=1.2541, trn_acc=0.5930, val_acc=0.5190\n",
      "iter= 5700, cost=1.0881, trn_acc=0.5869, val_acc=0.5112\n",
      "iter= 5800, cost=1.1967, trn_acc=0.5936, val_acc=0.5254\n",
      "iter= 5900, cost=0.9811, trn_acc=0.6005, val_acc=0.5306\n",
      "iter= 6000, cost=1.0370, trn_acc=0.5949, val_acc=0.5256\n",
      "iter= 6100, cost=1.1177, trn_acc=0.5750, val_acc=0.5218\n",
      "iter= 6200, cost=1.0359, trn_acc=0.6041, val_acc=0.5232\n",
      "iter= 6300, cost=1.0679, trn_acc=0.6015, val_acc=0.5290\n",
      "iter= 6400, cost=1.1519, trn_acc=0.5868, val_acc=0.5168\n",
      "iter= 6500, cost=1.2221, trn_acc=0.6130, val_acc=0.5228\n",
      "iter= 6600, cost=1.0543, trn_acc=0.6193, val_acc=0.5346\n",
      "iter= 6700, cost=1.1980, trn_acc=0.6032, val_acc=0.5292\n",
      "iter= 6800, cost=1.0021, trn_acc=0.6136, val_acc=0.5302\n",
      "iter= 6900, cost=1.0393, trn_acc=0.6160, val_acc=0.5202\n",
      "iter= 7000, cost=1.0060, trn_acc=0.6080, val_acc=0.5268\n",
      "iter= 7100, cost=0.9540, trn_acc=0.6329, val_acc=0.5348\n",
      "iter= 7200, cost=0.9313, trn_acc=0.6100, val_acc=0.5328\n",
      "iter= 7300, cost=1.0758, trn_acc=0.5906, val_acc=0.5206\n",
      "iter= 7400, cost=0.9423, trn_acc=0.6077, val_acc=0.5214\n",
      "iter= 7500, cost=0.8880, trn_acc=0.6092, val_acc=0.5138\n",
      "iter= 7600, cost=0.9713, trn_acc=0.6251, val_acc=0.5274\n",
      "iter= 7700, cost=0.9345, trn_acc=0.6301, val_acc=0.5324\n",
      "iter= 7800, cost=1.0563, trn_acc=0.6305, val_acc=0.5280\n",
      "iter= 7900, cost=0.8941, trn_acc=0.6287, val_acc=0.5254\n",
      "iter= 8000, cost=0.8722, trn_acc=0.6336, val_acc=0.5332\n",
      "iter= 8100, cost=1.1413, trn_acc=0.6341, val_acc=0.5292\n",
      "iter= 8200, cost=1.0594, trn_acc=0.6353, val_acc=0.5226\n",
      "iter= 8300, cost=1.1306, trn_acc=0.5489, val_acc=0.4986\n",
      "iter= 8400, cost=0.9135, trn_acc=0.6359, val_acc=0.5400\n",
      "iter= 8500, cost=0.8240, trn_acc=0.6070, val_acc=0.5166\n",
      "iter= 8600, cost=0.9625, trn_acc=0.6274, val_acc=0.5232\n",
      "iter= 8700, cost=0.8871, trn_acc=0.6034, val_acc=0.5232\n",
      "iter= 8800, cost=0.9167, trn_acc=0.6455, val_acc=0.5446\n",
      "iter= 8900, cost=0.8407, trn_acc=0.6318, val_acc=0.5308\n",
      "iter= 9000, cost=0.7604, trn_acc=0.6228, val_acc=0.5216\n",
      "iter= 9100, cost=0.9420, trn_acc=0.6406, val_acc=0.5342\n",
      "iter= 9200, cost=1.1162, trn_acc=0.6480, val_acc=0.5390\n",
      "iter= 9300, cost=0.9491, trn_acc=0.6418, val_acc=0.5270\n",
      "iter= 9400, cost=0.9564, trn_acc=0.6549, val_acc=0.5358\n",
      "iter= 9500, cost=0.9536, trn_acc=0.6191, val_acc=0.5260\n",
      "iter= 9600, cost=0.9923, trn_acc=0.6436, val_acc=0.5332\n",
      "iter= 9700, cost=0.9629, trn_acc=0.6336, val_acc=0.5310\n",
      "iter= 9800, cost=0.9739, trn_acc=0.6410, val_acc=0.5356\n",
      "iter= 9900, cost=0.7569, trn_acc=0.6316, val_acc=0.5242\n",
      "iter=10000, cost=0.9576, trn_acc=0.6364, val_acc=0.5176\n"
     ]
    }
   ],
   "source": [
    "layer_dimensions = [X_train.shape[0], 1000, 200, 50, 10]\n",
    "NN = NeuralNetwork(layer_dimensions, drop_prob=0.0, reg_lambda=10**(-4))\n",
    "NN.load_validation_set(X_val, y_val)\n",
    "NN.train(X_trn, y_trn, iters=10001, alpha=1, batch_size=100, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for combination of dropout and regularization\n",
    "I couldn't find a good combination of these two, though they are both effective on themselves.\n",
    "\n",
    "It takes time to run this (about 30 seconds for 100 iterations on my laptop, so 10000 iterations take around 50 mins), it seems that more iterations could be useful, but the time is limited.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=    0, cost=2.3516, trn_acc=0.1185, val_acc=0.1126\n",
      "iter=  100, cost=2.0053, trn_acc=0.2785, val_acc=0.2894\n",
      "iter=  200, cost=1.9014, trn_acc=0.3328, val_acc=0.3410\n",
      "iter=  300, cost=1.7160, trn_acc=0.3624, val_acc=0.3758\n",
      "iter=  400, cost=1.8775, trn_acc=0.3804, val_acc=0.3968\n",
      "iter=  500, cost=1.5603, trn_acc=0.3871, val_acc=0.4084\n",
      "iter=  600, cost=1.5516, trn_acc=0.4028, val_acc=0.4212\n",
      "iter=  700, cost=1.6742, trn_acc=0.4220, val_acc=0.4288\n",
      "iter=  800, cost=1.6435, trn_acc=0.4313, val_acc=0.4328\n",
      "iter=  900, cost=1.5985, trn_acc=0.4400, val_acc=0.4432\n",
      "iter= 1000, cost=1.5041, trn_acc=0.4338, val_acc=0.4462\n",
      "iter= 1100, cost=1.5347, trn_acc=0.4528, val_acc=0.4474\n",
      "iter= 1200, cost=1.4142, trn_acc=0.4574, val_acc=0.4530\n",
      "iter= 1300, cost=1.5990, trn_acc=0.4677, val_acc=0.4574\n",
      "iter= 1400, cost=1.4235, trn_acc=0.4662, val_acc=0.4616\n",
      "iter= 1500, cost=1.6213, trn_acc=0.4735, val_acc=0.4628\n",
      "iter= 1600, cost=1.3699, trn_acc=0.4819, val_acc=0.4714\n",
      "iter= 1700, cost=1.5902, trn_acc=0.4826, val_acc=0.4742\n",
      "iter= 1800, cost=1.5302, trn_acc=0.4874, val_acc=0.4754\n",
      "iter= 1900, cost=1.5025, trn_acc=0.4941, val_acc=0.4712\n",
      "iter= 2000, cost=1.3472, trn_acc=0.4888, val_acc=0.4768\n",
      "iter= 2100, cost=1.5450, trn_acc=0.4992, val_acc=0.4798\n",
      "iter= 2200, cost=1.3383, trn_acc=0.5102, val_acc=0.4854\n",
      "iter= 2300, cost=1.3755, trn_acc=0.5004, val_acc=0.4858\n",
      "iter= 2400, cost=1.3071, trn_acc=0.5094, val_acc=0.4816\n",
      "iter= 2500, cost=1.2326, trn_acc=0.5232, val_acc=0.4848\n",
      "iter= 2600, cost=1.2909, trn_acc=0.5040, val_acc=0.4928\n",
      "iter= 2700, cost=1.3282, trn_acc=0.5245, val_acc=0.4940\n",
      "iter= 2800, cost=1.4621, trn_acc=0.5245, val_acc=0.5004\n",
      "iter= 2900, cost=1.5826, trn_acc=0.5202, val_acc=0.4968\n",
      "iter= 3000, cost=1.4790, trn_acc=0.5340, val_acc=0.5000\n",
      "iter= 3100, cost=1.2481, trn_acc=0.5394, val_acc=0.4954\n",
      "iter= 3200, cost=1.2911, trn_acc=0.5409, val_acc=0.4976\n",
      "iter= 3300, cost=1.3127, trn_acc=0.5470, val_acc=0.5002\n",
      "iter= 3400, cost=1.3235, trn_acc=0.5283, val_acc=0.5014\n",
      "iter= 3500, cost=1.2308, trn_acc=0.5446, val_acc=0.5040\n",
      "iter= 3600, cost=1.3571, trn_acc=0.5416, val_acc=0.5068\n",
      "iter= 3700, cost=1.2776, trn_acc=0.5415, val_acc=0.4918\n",
      "iter= 3800, cost=1.1586, trn_acc=0.5488, val_acc=0.5040\n",
      "iter= 3900, cost=1.3310, trn_acc=0.5515, val_acc=0.4938\n",
      "iter= 4000, cost=1.2534, trn_acc=0.5496, val_acc=0.5066\n",
      "iter= 4100, cost=1.2228, trn_acc=0.5538, val_acc=0.5048\n",
      "iter= 4200, cost=1.0595, trn_acc=0.5587, val_acc=0.5124\n",
      "iter= 4300, cost=1.1266, trn_acc=0.5497, val_acc=0.5146\n",
      "iter= 4400, cost=1.3607, trn_acc=0.5591, val_acc=0.5110\n",
      "iter= 4500, cost=1.2633, trn_acc=0.5617, val_acc=0.5124\n",
      "iter= 4600, cost=1.1787, trn_acc=0.5665, val_acc=0.5112\n",
      "iter= 4700, cost=0.9562, trn_acc=0.5667, val_acc=0.5112\n",
      "iter= 4800, cost=1.1114, trn_acc=0.5736, val_acc=0.5158\n",
      "iter= 4900, cost=1.3358, trn_acc=0.5749, val_acc=0.5112\n",
      "iter= 5000, cost=1.1389, trn_acc=0.5690, val_acc=0.5080\n",
      "iter= 5100, cost=1.2379, trn_acc=0.5848, val_acc=0.5200\n",
      "iter= 5200, cost=1.1255, trn_acc=0.5776, val_acc=0.5162\n",
      "iter= 5300, cost=1.2273, trn_acc=0.5670, val_acc=0.5192\n",
      "iter= 5400, cost=1.1000, trn_acc=0.5709, val_acc=0.5136\n",
      "iter= 5500, cost=1.1302, trn_acc=0.5754, val_acc=0.5084\n",
      "iter= 5600, cost=1.3097, trn_acc=0.5844, val_acc=0.5166\n",
      "iter= 5700, cost=1.0174, trn_acc=0.5789, val_acc=0.5160\n",
      "iter= 5800, cost=1.0631, trn_acc=0.5826, val_acc=0.5164\n",
      "iter= 5900, cost=1.1821, trn_acc=0.5934, val_acc=0.5240\n",
      "iter= 6000, cost=1.1492, trn_acc=0.5902, val_acc=0.5118\n",
      "iter= 6100, cost=1.1142, trn_acc=0.5865, val_acc=0.5196\n",
      "iter= 6200, cost=1.0857, trn_acc=0.6045, val_acc=0.5244\n",
      "iter= 6300, cost=1.0785, trn_acc=0.5975, val_acc=0.5236\n",
      "iter= 6400, cost=1.0855, trn_acc=0.6116, val_acc=0.5228\n",
      "iter= 6500, cost=1.1401, trn_acc=0.6013, val_acc=0.5184\n",
      "iter= 6600, cost=1.0778, trn_acc=0.6047, val_acc=0.5272\n",
      "iter= 6700, cost=1.0290, trn_acc=0.6057, val_acc=0.5250\n",
      "iter= 6800, cost=1.3081, trn_acc=0.5893, val_acc=0.5156\n",
      "iter= 6900, cost=1.1880, trn_acc=0.6106, val_acc=0.5232\n",
      "iter= 7000, cost=1.0243, trn_acc=0.6033, val_acc=0.5220\n",
      "iter= 7100, cost=1.2031, trn_acc=0.6048, val_acc=0.5234\n",
      "iter= 7200, cost=1.0625, trn_acc=0.6173, val_acc=0.5240\n",
      "iter= 7300, cost=1.1700, trn_acc=0.6096, val_acc=0.5246\n",
      "iter= 7400, cost=1.1872, trn_acc=0.6163, val_acc=0.5216\n",
      "iter= 7500, cost=1.2446, trn_acc=0.6125, val_acc=0.5244\n",
      "iter= 7600, cost=1.0489, trn_acc=0.6032, val_acc=0.5188\n",
      "iter= 7700, cost=1.1384, trn_acc=0.5918, val_acc=0.5210\n",
      "iter= 7800, cost=1.0279, trn_acc=0.6024, val_acc=0.5228\n",
      "iter= 7900, cost=0.9422, trn_acc=0.5978, val_acc=0.5130\n",
      "iter= 8000, cost=1.1461, trn_acc=0.5892, val_acc=0.5154\n",
      "iter= 8100, cost=0.9225, trn_acc=0.6307, val_acc=0.5348\n",
      "iter= 8200, cost=0.9354, trn_acc=0.6265, val_acc=0.5316\n",
      "iter= 8300, cost=1.1557, trn_acc=0.6265, val_acc=0.5260\n",
      "iter= 8400, cost=1.0368, trn_acc=0.6287, val_acc=0.5164\n",
      "iter= 8500, cost=1.0244, trn_acc=0.6157, val_acc=0.5286\n",
      "iter= 8600, cost=0.9767, trn_acc=0.6287, val_acc=0.5210\n",
      "iter= 8700, cost=0.8728, trn_acc=0.6284, val_acc=0.5370\n",
      "iter= 8800, cost=1.0521, trn_acc=0.6368, val_acc=0.5378\n",
      "iter= 8900, cost=0.9317, trn_acc=0.6296, val_acc=0.5330\n",
      "iter= 9000, cost=1.0210, trn_acc=0.6102, val_acc=0.5194\n",
      "iter= 9100, cost=0.9164, trn_acc=0.6295, val_acc=0.5338\n",
      "iter= 9200, cost=0.9304, trn_acc=0.6277, val_acc=0.5346\n",
      "iter= 9300, cost=0.8835, trn_acc=0.6484, val_acc=0.5406\n",
      "iter= 9400, cost=0.9504, trn_acc=0.6450, val_acc=0.5344\n",
      "iter= 9500, cost=1.0147, trn_acc=0.6348, val_acc=0.5306\n",
      "iter= 9600, cost=0.9197, trn_acc=0.6397, val_acc=0.5292\n",
      "iter= 9700, cost=1.2144, trn_acc=0.6144, val_acc=0.5272\n",
      "iter= 9800, cost=1.0614, trn_acc=0.6372, val_acc=0.5294\n",
      "iter= 9900, cost=0.8987, trn_acc=0.6533, val_acc=0.5322\n",
      "iter=10000, cost=1.1111, trn_acc=0.6508, val_acc=0.5286\n"
     ]
    }
   ],
   "source": [
    "layer_dimensions = [X_train.shape[0], 1000, 200, 50, 10]\n",
    "NN = NeuralNetwork(layer_dimensions, drop_prob=0.01, reg_lambda=10**(-4))\n",
    "NN.load_validation_set(X_val, y_val)\n",
    "NN.train(X_trn, y_trn, iters=10001, alpha=1, batch_size=100, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=    0, cost=2.3516, trn_acc=0.1136, val_acc=0.1064\n",
      "iter=  100, cost=2.1572, trn_acc=0.2329, val_acc=0.2498\n",
      "iter=  200, cost=2.0785, trn_acc=0.2755, val_acc=0.2880\n",
      "iter=  300, cost=1.8734, trn_acc=0.3105, val_acc=0.3190\n",
      "iter=  400, cost=2.0387, trn_acc=0.3337, val_acc=0.3472\n",
      "iter=  500, cost=1.7582, trn_acc=0.3442, val_acc=0.3654\n",
      "iter=  600, cost=1.7384, trn_acc=0.3571, val_acc=0.3770\n",
      "iter=  700, cost=1.7969, trn_acc=0.3752, val_acc=0.3926\n",
      "iter=  800, cost=1.7700, trn_acc=0.3856, val_acc=0.3996\n",
      "iter=  900, cost=1.7318, trn_acc=0.4017, val_acc=0.4130\n",
      "iter= 1000, cost=1.6339, trn_acc=0.3991, val_acc=0.4130\n"
     ]
    }
   ],
   "source": [
    "layer_dimensions = [X_train.shape[0], 1000, 200, 50, 10]\n",
    "NN = NeuralNetwork(layer_dimensions, drop_prob=0.01, reg_lambda=10**(-4))\n",
    "NN.load_validation_set(X_val, y_val)\n",
    "NN.train(X_trn, y_trn, iters=10001, alpha=0.5, batch_size=100, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2: Regularizing the neural network\n",
    "#### Add dropout and L2 regularization\n",
    "#### Final output for the given test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=    0, cost=2.4009, trn_acc=0.1158, val_acc=nan\n",
      "iter=  100, cost=2.0661, trn_acc=0.2640, val_acc=nan\n",
      "iter=  200, cost=1.8798, trn_acc=0.3188, val_acc=nan\n",
      "iter=  300, cost=1.8643, trn_acc=0.3533, val_acc=nan\n",
      "iter=  400, cost=1.8440, trn_acc=0.3803, val_acc=nan\n",
      "iter=  500, cost=1.9582, trn_acc=0.3981, val_acc=nan\n",
      "iter=  600, cost=1.7163, trn_acc=0.4054, val_acc=nan\n",
      "iter=  700, cost=1.5828, trn_acc=0.4195, val_acc=nan\n",
      "iter=  800, cost=1.7700, trn_acc=0.4223, val_acc=nan\n",
      "iter=  900, cost=1.6074, trn_acc=0.4390, val_acc=nan\n",
      "iter= 1000, cost=1.6887, trn_acc=0.4416, val_acc=nan\n",
      "iter= 1100, cost=1.5612, trn_acc=0.4467, val_acc=nan\n",
      "iter= 1200, cost=1.6045, trn_acc=0.4521, val_acc=nan\n",
      "iter= 1300, cost=1.4667, trn_acc=0.4591, val_acc=nan\n",
      "iter= 1400, cost=1.4810, trn_acc=0.4623, val_acc=nan\n",
      "iter= 1500, cost=1.5924, trn_acc=0.4712, val_acc=nan\n",
      "iter= 1600, cost=1.5556, trn_acc=0.4766, val_acc=nan\n",
      "iter= 1700, cost=1.6885, trn_acc=0.4776, val_acc=nan\n",
      "iter= 1800, cost=1.5614, trn_acc=0.4863, val_acc=nan\n",
      "iter= 1900, cost=1.4208, trn_acc=0.4856, val_acc=nan\n",
      "iter= 2000, cost=1.4679, trn_acc=0.4954, val_acc=nan\n",
      "iter= 2100, cost=1.5874, trn_acc=0.4785, val_acc=nan\n",
      "iter= 2200, cost=1.3630, trn_acc=0.5051, val_acc=nan\n",
      "iter= 2300, cost=1.3398, trn_acc=0.4984, val_acc=nan\n",
      "iter= 2400, cost=1.4532, trn_acc=0.5055, val_acc=nan\n",
      "iter= 2500, cost=1.4631, trn_acc=0.5154, val_acc=nan\n",
      "iter= 2600, cost=1.3885, trn_acc=0.5067, val_acc=nan\n",
      "iter= 2700, cost=1.2828, trn_acc=0.5229, val_acc=nan\n",
      "iter= 2800, cost=1.4735, trn_acc=0.5243, val_acc=nan\n",
      "iter= 2900, cost=1.2479, trn_acc=0.5181, val_acc=nan\n",
      "iter= 3000, cost=1.1750, trn_acc=0.5159, val_acc=nan\n",
      "iter= 3100, cost=1.3638, trn_acc=0.5417, val_acc=nan\n",
      "iter= 3200, cost=1.2505, trn_acc=0.5388, val_acc=nan\n",
      "iter= 3300, cost=1.3432, trn_acc=0.5390, val_acc=nan\n",
      "iter= 3400, cost=1.4685, trn_acc=0.5345, val_acc=nan\n",
      "iter= 3500, cost=1.2794, trn_acc=0.5450, val_acc=nan\n",
      "iter= 3600, cost=1.3563, trn_acc=0.5450, val_acc=nan\n",
      "iter= 3700, cost=1.6264, trn_acc=0.5434, val_acc=nan\n",
      "iter= 3800, cost=1.2648, trn_acc=0.5486, val_acc=nan\n",
      "iter= 3900, cost=1.3437, trn_acc=0.5574, val_acc=nan\n",
      "iter= 4000, cost=1.2501, trn_acc=0.5521, val_acc=nan\n",
      "iter= 4100, cost=1.3028, trn_acc=0.5612, val_acc=nan\n",
      "iter= 4200, cost=1.1836, trn_acc=0.5481, val_acc=nan\n",
      "iter= 4300, cost=1.4426, trn_acc=0.5646, val_acc=nan\n",
      "iter= 4400, cost=1.2883, trn_acc=0.5697, val_acc=nan\n",
      "iter= 4500, cost=1.2967, trn_acc=0.5710, val_acc=nan\n",
      "iter= 4600, cost=1.1519, trn_acc=0.5708, val_acc=nan\n",
      "iter= 4700, cost=1.1281, trn_acc=0.5722, val_acc=nan\n",
      "iter= 4800, cost=1.4029, trn_acc=0.5797, val_acc=nan\n",
      "iter= 4900, cost=1.2957, trn_acc=0.5812, val_acc=nan\n",
      "iter= 5000, cost=1.2640, trn_acc=0.5874, val_acc=nan\n",
      "iter= 5100, cost=1.2952, trn_acc=0.5768, val_acc=nan\n",
      "iter= 5200, cost=1.1979, trn_acc=0.5826, val_acc=nan\n",
      "iter= 5300, cost=1.3146, trn_acc=0.5882, val_acc=nan\n",
      "iter= 5400, cost=1.0961, trn_acc=0.5928, val_acc=nan\n",
      "iter= 5500, cost=1.2943, trn_acc=0.5870, val_acc=nan\n",
      "iter= 5600, cost=1.1813, trn_acc=0.5780, val_acc=nan\n",
      "iter= 5700, cost=1.0498, trn_acc=0.6002, val_acc=nan\n",
      "iter= 5800, cost=1.2711, trn_acc=0.5812, val_acc=nan\n",
      "iter= 5900, cost=1.1233, trn_acc=0.6058, val_acc=nan\n",
      "iter= 6000, cost=1.1642, trn_acc=0.6120, val_acc=nan\n",
      "iter= 6100, cost=1.2020, trn_acc=0.6107, val_acc=nan\n",
      "iter= 6200, cost=1.2841, trn_acc=0.6121, val_acc=nan\n",
      "iter= 6300, cost=1.1288, trn_acc=0.6226, val_acc=nan\n",
      "iter= 6400, cost=1.0367, trn_acc=0.6159, val_acc=nan\n",
      "iter= 6500, cost=1.2231, trn_acc=0.6157, val_acc=nan\n",
      "iter= 6600, cost=1.2029, trn_acc=0.6243, val_acc=nan\n",
      "iter= 6700, cost=1.3063, trn_acc=0.6281, val_acc=nan\n",
      "iter= 6800, cost=1.1522, trn_acc=0.6224, val_acc=nan\n",
      "iter= 6900, cost=1.1360, trn_acc=0.6287, val_acc=nan\n",
      "iter= 7000, cost=1.2506, trn_acc=0.6266, val_acc=nan\n",
      "iter= 7100, cost=1.1663, trn_acc=0.6204, val_acc=nan\n",
      "iter= 7200, cost=1.1712, trn_acc=0.6440, val_acc=nan\n",
      "iter= 7300, cost=1.1640, trn_acc=0.6327, val_acc=nan\n",
      "iter= 7400, cost=1.0796, trn_acc=0.6345, val_acc=nan\n",
      "iter= 7500, cost=1.1560, trn_acc=0.6363, val_acc=nan\n",
      "iter= 7600, cost=1.0772, trn_acc=0.6470, val_acc=nan\n",
      "iter= 7700, cost=0.9736, trn_acc=0.6306, val_acc=nan\n",
      "iter= 7800, cost=1.1269, trn_acc=0.6436, val_acc=nan\n",
      "iter= 7900, cost=0.9719, trn_acc=0.6556, val_acc=nan\n",
      "iter= 8000, cost=1.1507, trn_acc=0.6483, val_acc=nan\n",
      "iter= 8100, cost=0.9034, trn_acc=0.6595, val_acc=nan\n",
      "iter= 8200, cost=0.9848, trn_acc=0.6448, val_acc=nan\n",
      "iter= 8300, cost=1.1416, trn_acc=0.6540, val_acc=nan\n",
      "iter= 8400, cost=0.8942, trn_acc=0.6597, val_acc=nan\n",
      "iter= 8500, cost=0.9932, trn_acc=0.6564, val_acc=nan\n",
      "iter= 8600, cost=1.0011, trn_acc=0.6586, val_acc=nan\n",
      "iter= 8700, cost=0.8591, trn_acc=0.6520, val_acc=nan\n",
      "iter= 8800, cost=1.0416, trn_acc=0.6576, val_acc=nan\n",
      "iter= 8900, cost=1.2586, trn_acc=0.6731, val_acc=nan\n",
      "iter= 9000, cost=0.9730, trn_acc=0.6652, val_acc=nan\n",
      "iter= 9100, cost=1.0776, trn_acc=0.6691, val_acc=nan\n",
      "iter= 9200, cost=0.9727, trn_acc=0.6526, val_acc=nan\n",
      "iter= 9300, cost=0.9466, trn_acc=0.6624, val_acc=nan\n",
      "iter= 9400, cost=0.9859, trn_acc=0.6741, val_acc=nan\n",
      "iter= 9500, cost=0.9613, trn_acc=0.6784, val_acc=nan\n",
      "iter= 9600, cost=0.8479, trn_acc=0.6862, val_acc=nan\n",
      "iter= 9700, cost=0.8761, trn_acc=0.6796, val_acc=nan\n",
      "iter= 9800, cost=0.8617, trn_acc=0.6896, val_acc=nan\n",
      "iter= 9900, cost=0.9095, trn_acc=0.6897, val_acc=nan\n",
      "iter=10000, cost=1.1608, trn_acc=0.7019, val_acc=nan\n"
     ]
    }
   ],
   "source": [
    "layer_dimensions = [X_train.shape[0], 1000, 200, 50, 10]\n",
    "NN2 = NeuralNetwork(layer_dimensions, drop_prob=0.1, reg_lambda=0.0)\n",
    "NN2.train(X_train, y_train, iters=10001, alpha=1, batch_size=100, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted2 = NN2.predict(X_test)\n",
    "save_predictions('ans2-uni', y_predicted2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# expression testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 496 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for _ in range(10**7):\n",
    "    _2 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, '1')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (1, '1')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, '1', 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a + (3,)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, '1', 3, None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b + (None, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.51287398,  0.43687488, -0.57080603])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = np.random.rand(size = A.shape)\n",
    "M = (M < prob) / (1 - prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "b = np.array([[-1, 2, 3], [4,5,6]])\n",
    "c = np.array([1,2,3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c.ravel().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.T.dot(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is 1, 2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob = np.exp(a - np.max(a, axis=0, keepdims=True)) \n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.arange(300).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.arange(3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob[[0, 1, 1], np.arange(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sum(prob, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = a.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.arange(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob[[1, 1, 0], np.arange(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row_dim = x.shape[0]\n",
    "row_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_dim = np.prod(x.shape[1:])\n",
    "col_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row_dim = x.shape[0]\n",
    "row_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.prod(x.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_dim = np.prod(x.shape[1:])\n",
    "col_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_reshape = x.reshape(row_dim, col_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sum(a, axis = 1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
