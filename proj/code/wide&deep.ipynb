{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load library and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-21 20:15:08\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/members.csv\n",
      "../input/sample_submission.csv\n",
      "../input/song_extra_info.csv\n",
      "../input/songs.csv\n",
      "../input/test.csv\n",
      "../input/train.csv\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "for x in glob.glob(\"../input/*\"):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/train.csv\")\n",
    "df_test  = pd.read_csv('../input/test.csv')\n",
    "df_songs = pd.read_csv('../input/songs.csv')\n",
    "df_song_extra = pd.read_csv(\"../input/song_extra_info.csv\")\n",
    "df_members = pd.read_csv(\"../input/members.csv\",parse_dates=[\"registration_init_time\",\"expiration_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>name</th>\n",
       "      <th>isrc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP7pLJoJFBvyuUwvu+oLzjT+bI+UeBPURCecJsX1jjs=</td>\n",
       "      <td>我們</td>\n",
       "      <td>TWUM71200043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ClazTFnk6r0Bnuie44bocdNMM3rdlrq0bCGAsGUWcHE=</td>\n",
       "      <td>Let Me Love You</td>\n",
       "      <td>QMZSY1600015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u2ja/bZE3zhCGxvbbOB3zOoUjx27u40cf5g09UXMoKQ=</td>\n",
       "      <td>原諒我</td>\n",
       "      <td>TWA530887303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92Fqsy0+p6+RHe2EoLKjHahORHR1Kq1TBJoClW9v+Ts=</td>\n",
       "      <td>Classic</td>\n",
       "      <td>USSM11301446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0QFmz/+rJy1Q56C1DuYqT9hKKqi5TUqx0sN0IwvoHrw=</td>\n",
       "      <td>愛投羅網</td>\n",
       "      <td>TWA471306001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        song_id             name          isrc\n",
       "0  LP7pLJoJFBvyuUwvu+oLzjT+bI+UeBPURCecJsX1jjs=               我們  TWUM71200043\n",
       "1  ClazTFnk6r0Bnuie44bocdNMM3rdlrq0bCGAsGUWcHE=  Let Me Love You  QMZSY1600015\n",
       "2  u2ja/bZE3zhCGxvbbOB3zOoUjx27u40cf5g09UXMoKQ=              原諒我  TWA530887303\n",
       "3  92Fqsy0+p6+RHe2EoLKjHahORHR1Kq1TBJoClW9v+Ts=          Classic  USSM11301446\n",
       "4  0QFmz/+rJy1Q56C1DuYqT9hKKqi5TUqx0sN0IwvoHrw=             愛投羅網  TWA471306001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_song_extra.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Members Dataset\n",
    "- rename column 'bd' -> 'age' \n",
    "- registration_init_time -> registration_year + registration_month + registration_day (2011-08-20 -> 2011 08 20)\n",
    "- expiration_date -> expiration_year + expiration_month + expiration_day (2011-08-20 -> 2011 08 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Members\n",
    "df_members.rename(columns={'bd': 'age'}, inplace=True)\n",
    "df_members['validate_days'] = (df_members['expiration_date'] - df_members['registration_init_time']).dt.days\n",
    "\n",
    "df_members['registration_year'] = df_members['registration_init_time'].apply(lambda x: str(x)[0:4])\n",
    "df_members['registration_month'] = df_members['registration_init_time'].apply(lambda x: str(x)[5:7])\n",
    "df_members['registration_day'] = df_members['registration_init_time'].apply(lambda x: str(x)[8:10])\n",
    "\n",
    "df_members['expiration_year'] = df_members['expiration_date'].apply(lambda x: str(x)[0:4])\n",
    "df_members['expiration_month'] = df_members['expiration_date'].apply(lambda x: str(x)[5:7])\n",
    "df_members['expiration_day'] = df_members['expiration_date'].apply(lambda x: str(x)[8:10])\n",
    "\n",
    "df_members.drop(['registration_init_time', 'expiration_date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>city</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>validate_days</th>\n",
       "      <th>registration_year</th>\n",
       "      <th>registration_month</th>\n",
       "      <th>registration_day</th>\n",
       "      <th>expiration_year</th>\n",
       "      <th>expiration_month</th>\n",
       "      <th>expiration_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XQxgAYj3klVKjR3oxPPXYYFp4soD4TuBghkhMTD4oTw=</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>2223</td>\n",
       "      <td>2011</td>\n",
       "      <td>08</td>\n",
       "      <td>20</td>\n",
       "      <td>2017</td>\n",
       "      <td>09</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UizsfmJb9mV54qE9hCYyU07Va97c0lCRLEQX3ae+ztM=</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>725</td>\n",
       "      <td>2015</td>\n",
       "      <td>06</td>\n",
       "      <td>28</td>\n",
       "      <td>2017</td>\n",
       "      <td>06</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D8nEhsIOBSoE6VthTaqDX8U6lqjJ7dLdr72mOyLya2A=</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>457</td>\n",
       "      <td>2016</td>\n",
       "      <td>04</td>\n",
       "      <td>11</td>\n",
       "      <td>2017</td>\n",
       "      <td>07</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mCuD+tZ1hERA/o5GPqk38e041J8ZsBaLcu7nGoIIvhI=</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>09</td>\n",
       "      <td>06</td>\n",
       "      <td>2015</td>\n",
       "      <td>09</td>\n",
       "      <td>07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q4HRBfVSssAFS9iRfxWrohxuk9kCYMKjHOEagUMV6rQ=</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>138</td>\n",
       "      <td>2017</td>\n",
       "      <td>01</td>\n",
       "      <td>26</td>\n",
       "      <td>2017</td>\n",
       "      <td>06</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           msno  city  age gender  \\\n",
       "0  XQxgAYj3klVKjR3oxPPXYYFp4soD4TuBghkhMTD4oTw=     1    0    NaN   \n",
       "1  UizsfmJb9mV54qE9hCYyU07Va97c0lCRLEQX3ae+ztM=     1    0    NaN   \n",
       "2  D8nEhsIOBSoE6VthTaqDX8U6lqjJ7dLdr72mOyLya2A=     1    0    NaN   \n",
       "3  mCuD+tZ1hERA/o5GPqk38e041J8ZsBaLcu7nGoIIvhI=     1    0    NaN   \n",
       "4  q4HRBfVSssAFS9iRfxWrohxuk9kCYMKjHOEagUMV6rQ=     1    0    NaN   \n",
       "\n",
       "   registered_via  validate_days registration_year registration_month  \\\n",
       "0               7           2223              2011                 08   \n",
       "1               7            725              2015                 06   \n",
       "2               4            457              2016                 04   \n",
       "3               9              1              2015                 09   \n",
       "4               4            138              2017                 01   \n",
       "\n",
       "  registration_day expiration_year expiration_month expiration_day  \n",
       "0               20            2017               09             20  \n",
       "1               28            2017               06             22  \n",
       "2               11            2017               07             12  \n",
       "3               06            2015               09             07  \n",
       "4               26            2017               06             13  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_members.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Song_extra Dataset\n",
    "- isrc -> song_year (TWUM7 12 00043 -> 2012, QMZSY 16 00015 -> 2016, TWA53 08 87303 -> 2008)\n",
    "- drop column song_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isrc_to_year(isrc):\n",
    "    if type(isrc) == str:\n",
    "        if int(isrc[5:7]) > 17:\n",
    "            return 1900 + int(isrc[5:7])\n",
    "        else:\n",
    "            return 2000 + int(isrc[5:7])\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "df_song_extra['song_year'] = df_song_extra['isrc'].apply(isrc_to_year)\n",
    "df_song_extra.drop(['isrc', 'name'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Merge all dataset based on song_id and msno(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# left join train and song based on song_id\n",
    "# left join train and members based on msno(user id)\n",
    "# left join train and song_extra based on song_id\n",
    "df_train = df_train.merge(df_songs, how=\"left\", on=\"song_id\")\n",
    "df_train = df_train.merge(df_members, how=\"left\", on=\"msno\")\n",
    "df_train = df_train.merge(df_song_extra, how='left', on='song_id')\n",
    "\n",
    "df_test  = df_test.merge(df_songs, how=\"left\", on=\"song_id\")\n",
    "df_test  = df_test.merge(df_members, how=\"left\", on=\"msno\")\n",
    "df_test = df_test.merge(df_song_extra, how='left', on='song_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msno\n",
      "song_id\n",
      "source_system_tab\n",
      "source_screen_name\n",
      "source_type\n",
      "target\n",
      "song_length\n",
      "genre_ids\n",
      "artist_name\n",
      "composer\n",
      "lyricist\n",
      "language\n",
      "city\n",
      "age\n",
      "gender\n",
      "registered_via\n",
      "validate_days\n",
      "registration_year\n",
      "registration_month\n",
      "registration_day\n",
      "expiration_year\n",
      "expiration_month\n",
      "expiration_day\n",
      "song_year\n"
     ]
    }
   ],
   "source": [
    "for col in df_train.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Fill in missing value\n",
    "- categorical feature -> 'Unknown'\n",
    "- continuous feaure -> mean or mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object     source_system_tab\n",
      "object     source_screen_name\n",
      "object     source_type\n",
      "float64    song_length\n",
      "object     genre_ids\n",
      "object     artist_name\n",
      "object     composer\n",
      "object     lyricist\n",
      "float64    language\n",
      "object     gender\n",
      "float64    song_year\n"
     ]
    }
   ],
   "source": [
    "# find columns with missing value\n",
    "for col in df_train.columns:\n",
    "    if df_train[col].isnull().any() or df_train[col].isnull().any():\n",
    "        print('{:10} {}'.format(str(df_train[col].dtype), col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fill categorical columns with tag: 'Unknown'\n",
    "UNKNOWN = 'Unknown'\n",
    "col_fill_with_unknown = [\n",
    "    'source_system_tab', 'source_screen_name', 'source_type', 'gender',\n",
    "    'genre_ids', 'artist_name', 'composer', 'lyricist',\n",
    "    'song_year'\n",
    "]\n",
    "for col in col_fill_with_unknown:\n",
    "    df_train[col].fillna(value=UNKNOWN, inplace=True)\n",
    "    df_test[col].fillna(value=UNKNOWN, inplace=True)\n",
    "df_train['song_year'] = df_train['song_year'].astype(str)\n",
    "    \n",
    "# fill in song length with mean\n",
    "df_train['song_length'].fillna(value=df_train['song_length'].mean(),inplace=True)\n",
    "df_test['song_length'].fillna(value=df_test['song_length'].mean(),inplace=True)\n",
    "\n",
    "# fill in language with mode\n",
    "df_train['language'].fillna(value=df_train['language'].mode()[0],inplace=True)\n",
    "df_test['language'].fillna(value=df_test['language'].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Deal with multiple labels\n",
    "- split into lists (242|726 -> [242, 726]) preparing for later one-hot or multi-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genre\n",
    "# TODO: genre_ids seldomly have values seperated by comma\n",
    "df_train['genre_ids'] = df_train['genre_ids'].str.split(\"|\")\n",
    "df_test['genre_ids'] = df_test['genre_ids'].str.split(\"|\")\n",
    "df_train['genre_count'] = df_train['genre_ids'].apply(lambda x : len(x) if UNKNOWN not in x else 0)\n",
    "df_test['genre_count'] = df_test['genre_ids'].apply(lambda x : len(x) if UNKNOWN not in x else 0)\n",
    "\n",
    "df_train['lyricist'] = df_train['lyricist'].str.split(\"|\")\n",
    "df_test['lyricist'] = df_test['lyricist'].str.split(\"|\")\n",
    "df_train['lyricist_count'] = df_train['lyricist'].apply(lambda x : len(x) if \"Unknown\" not in x else 0 )\n",
    "df_test['lyricist_count'] = df_test['lyricist'].apply(lambda x : len(x) if \"Unknown\" not in x else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7.377418e+06\n",
       "mean     1.037353e+00\n",
       "std      2.948952e-01\n",
       "min      0.000000e+00\n",
       "25%      1.000000e+00\n",
       "50%      1.000000e+00\n",
       "75%      1.000000e+00\n",
       "max      8.000000e+00\n",
       "Name: genre_count, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['genre_count'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7.377418e+06\n",
       "mean     7.751240e-01\n",
       "std      1.079586e+00\n",
       "min      0.000000e+00\n",
       "25%      0.000000e+00\n",
       "50%      1.000000e+00\n",
       "75%      1.000000e+00\n",
       "max      2.300000e+01\n",
       "Name: lyricist_count, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['lyricist_count'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Historical Data (artist's historical repeat rate, etc.)\n",
    "maybe sometimes later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_artists = df_train.loc[:,[\"artist_name\",\"target\"]]\n",
    "# artists_repeat = df_artists.groupby([\"artist_name\"], as_index=False).sum().rename(columns={\"target\":\"repeat_count\"})\n",
    "# artists_play = df_artists.groupby([\"artist_name\"], as_index=False).count().rename(columns = {\"target\":\"play_count\"})\n",
    "# df_artists_repeat = artists_repeat.merge(artists_play, how='inner', on='artist_name')\n",
    "\n",
    "# df_artists_repeat['artist_repeat_percentage'] = round(\n",
    "#     (df_artists_repeat['repeat_count']*100) / df_artists_repeat['play_count'], \n",
    "#     1\n",
    "# )\n",
    "\n",
    "# df_train = df_train.merge(df_artists_repeat, on=\"artist_name\",how=\"left\")\n",
    "# df_test = df_test.merge(df_artists_repeat,on=\"artist_name\",how=\"left\")\n",
    "\n",
    "# for col in ['repeat_count', 'play_count', 'artist_repeat_percentage']:\n",
    "#     df_test[col].fillna(np.nanmedian(df_test[col]), inplace=True, downcast='infer')\n",
    "\n",
    "# del df_artists\n",
    "# del df_artists_repeat\n",
    "# del artists_play\n",
    "# del artists_repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>song_id</th>\n",
       "      <th>source_system_tab</th>\n",
       "      <th>source_screen_name</th>\n",
       "      <th>source_type</th>\n",
       "      <th>target</th>\n",
       "      <th>song_length</th>\n",
       "      <th>genre_ids</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>composer</th>\n",
       "      <th>...</th>\n",
       "      <th>validate_days</th>\n",
       "      <th>registration_year</th>\n",
       "      <th>registration_month</th>\n",
       "      <th>registration_day</th>\n",
       "      <th>expiration_year</th>\n",
       "      <th>expiration_month</th>\n",
       "      <th>expiration_day</th>\n",
       "      <th>song_year</th>\n",
       "      <th>genre_count</th>\n",
       "      <th>lyricist_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg=</td>\n",
       "      <td>BBzumQNXUHKdEBOB7mAJuzok+IJA1c2Ryg/yzTF6tik=</td>\n",
       "      <td>explore</td>\n",
       "      <td>Explore</td>\n",
       "      <td>online-playlist</td>\n",
       "      <td>1</td>\n",
       "      <td>206471.0</td>\n",
       "      <td>[359]</td>\n",
       "      <td>Bastille</td>\n",
       "      <td>Dan Smith| Mark Crew</td>\n",
       "      <td>...</td>\n",
       "      <td>2103</td>\n",
       "      <td>2012</td>\n",
       "      <td>01</td>\n",
       "      <td>02</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>05</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8=</td>\n",
       "      <td>bhp/MpSNoqoxOIB+/l8WPqu6jldth4DIpCm3ayXnJqM=</td>\n",
       "      <td>my library</td>\n",
       "      <td>Local playlist more</td>\n",
       "      <td>local-playlist</td>\n",
       "      <td>1</td>\n",
       "      <td>284584.0</td>\n",
       "      <td>[1259]</td>\n",
       "      <td>Various Artists</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>2301</td>\n",
       "      <td>2011</td>\n",
       "      <td>05</td>\n",
       "      <td>25</td>\n",
       "      <td>2017</td>\n",
       "      <td>09</td>\n",
       "      <td>11</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           msno  \\\n",
       "0  FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg=   \n",
       "1  Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8=   \n",
       "\n",
       "                                        song_id source_system_tab  \\\n",
       "0  BBzumQNXUHKdEBOB7mAJuzok+IJA1c2Ryg/yzTF6tik=           explore   \n",
       "1  bhp/MpSNoqoxOIB+/l8WPqu6jldth4DIpCm3ayXnJqM=        my library   \n",
       "\n",
       "    source_screen_name      source_type  target  song_length genre_ids  \\\n",
       "0              Explore  online-playlist       1     206471.0     [359]   \n",
       "1  Local playlist more   local-playlist       1     284584.0    [1259]   \n",
       "\n",
       "       artist_name              composer      ...       validate_days  \\\n",
       "0         Bastille  Dan Smith| Mark Crew      ...                2103   \n",
       "1  Various Artists               Unknown      ...                2301   \n",
       "\n",
       "   registration_year  registration_month  registration_day expiration_year  \\\n",
       "0               2012                  01                02            2017   \n",
       "1               2011                  05                25            2017   \n",
       "\n",
       "   expiration_month  expiration_day song_year genre_count lyricist_count  \n",
       "0                10              05    2016.0           1              0  \n",
       "1                09              11    1999.0           1              0  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparing Data For TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO later\n",
    "- [ ] adjust the embedding dimension for crossed\n",
    "- [ ] add more crossed\n",
    "- [ ] (other parameters adjustment, add it here)\n",
    "- [ ] preprocessing the numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define categorical columns, continuous columns and labels\n",
    "CATEGORICAL_COLUMNS = [\n",
    "    'msno',\n",
    "    'song_id',\n",
    "    'artist_name',\n",
    "    'song_year',\n",
    "    'source_screen_name',\n",
    "    'source_type',\n",
    "    'source_system_tab',\n",
    "    'language',\n",
    "    'city',\n",
    "    'gender',\n",
    "    'registered_via',\n",
    "    'registration_year',\n",
    "    'registration_month',\n",
    "    'registration_day',\n",
    "    'expiration_year',\n",
    "    'expiration_month',\n",
    "    'expiration_day',\n",
    "]\n",
    "\n",
    "\n",
    "CONTINUOUS_COLUMNS = [\n",
    "    'age', \n",
    "    'song_length',\n",
    "    'genre_count', \n",
    "    'lyricist_count',\n",
    "]\n",
    "\n",
    "LABEL = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-21 20:19:14\n",
      "2017-11-21 20:20:44\n"
     ]
    }
   ],
   "source": [
    "# convert categorical columns to string\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "for col in CATEGORICAL_COLUMNS:\n",
    "    df_train[col] = df_train[col].astype(str)\n",
    "    df_test[col] = df_test[col].astype(str)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Source Related Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source related categorical feature\n",
    "# Columns with a small set of categories transform them into sparse columns with keys\n",
    "_source_cols = ['source_screen_name', 'source_type', 'source_system_tab']\n",
    "for col in _source_cols:\n",
    "    col_set = set(df_train[col].unique()).union(set(df_test[col].unique()))\n",
    "#     print(col, len(col_set))\n",
    "    columns[col] = tf.contrib.layers.sparse_column_with_keys(column_name=col, keys=col_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Members Related Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# members related categorical feature\n",
    "# Columns with a small set of categories transform them into sparse columns with keys\n",
    "_members_cols = [\n",
    "    'city', 'gender', 'registered_via', \n",
    "    'registration_year', 'registration_month', 'registration_day',\n",
    "    'expiration_year', 'expiration_month', 'expiration_day'\n",
    "]\n",
    "for col in _members_cols:\n",
    "    col_set = set(df_train[col].unique()).union(set(df_test[col].unique()))\n",
    "#     print(col, len(col_set))\n",
    "    columns[col] = tf.contrib.layers.sparse_column_with_keys(column_name=col, keys=col_set)\n",
    "\n",
    "    \n",
    "# members related numerical feature   \n",
    "# bucket the ages. \n",
    "# Bucketization allows us to find the music recommendtion by certain age groups \n",
    "columns['age'] = tf.feature_column.numeric_column('age')\n",
    "columns['age_buckets'] = tf.feature_column.bucketized_column(\n",
    "    columns['age'], boundaries=[15, 18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "\n",
    "# validate_days as continuous column\n",
    "columns['validate_days'] = tf.feature_column.numeric_column('validate_days')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Song Related Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# songs related categorical feature\n",
    "# Columns with a small set of categories transform them into sparse columns with keys\n",
    "_songs_cols = ['language', 'song_year']\n",
    "for col in _songs_cols:\n",
    "    col_set = set(df_train[col].unique()).union(set(df_test[col].unique()))\n",
    "#     print(col, len(col_set))\n",
    "    columns[col] = tf.contrib.layers.sparse_column_with_keys(column_name=col, keys=col_set)\n",
    "\n",
    "# artist_name\n",
    "# Columns with a large set of categories hash into real value \n",
    "col = 'artist_name'\n",
    "columns[col] = tf.contrib.layers.sparse_column_with_hash_bucket(col, hash_bucket_size=10000)\n",
    "\n",
    "# song length (ms) \n",
    "# bucketization allows us to find the music recommendtion by certain song length groups \n",
    "# TODO 1: use it as numerical by cutting cutting >3600s to 3600s\n",
    "# TODO 2: improved bucket\n",
    "col = 'song_length'\n",
    "columns[col] = tf.feature_column.numeric_column(col)\n",
    "columns['song_length_buckets'] = tf.feature_column.bucketized_column(\n",
    "    columns[col], boundaries=[x*1000 for x in [30, 60, 120, 180, 240, 300, 360, 420, 480, 540, 600]])\n",
    "\n",
    "# genre_count: number of genre \n",
    "# lyricist_count: number of lyricist\n",
    "for col in ['genre_count', 'lyricist_count']:\n",
    "    columns[col] = tf.feature_column.numeric_column(col)\n",
    "\n",
    "# TODO: genre_ids, composer, lyricist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.4 Hash msno(usr_id) and song_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34403\n",
      "419839\n"
     ]
    }
   ],
   "source": [
    "for col in ['msno', 'song_id']:\n",
    "    print(len(set(df_train[col]).union(set(df_test[col]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns['msno'] = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"msno\", hash_bucket_size=30000)\n",
    "columns['song_id'] = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"song_id\", hash_bucket_size=10**5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Final state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_screen_name\n",
      "source_type\n",
      "source_system_tab\n",
      "city\n",
      "gender\n",
      "registered_via\n",
      "registration_year\n",
      "registration_month\n",
      "registration_day\n",
      "expiration_year\n",
      "expiration_month\n",
      "expiration_day\n",
      "age\n",
      "age_buckets\n",
      "validate_days\n",
      "language\n",
      "song_year\n",
      "artist_name\n",
      "song_length\n",
      "song_length_buckets\n",
      "genre_count\n",
      "lyricist_count\n",
      "msno\n",
      "song_id\n"
     ]
    }
   ],
   "source": [
    "for key in columns.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['language'].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wide model input\n",
    "# categorical feature\n",
    "wide_columns_tag = ['song_id', \n",
    "                    'msno', \n",
    "                    'song_length_buckets', \n",
    "                    'language', \n",
    "                    'song_year',\n",
    "                    'source_screen_name',\n",
    "                    'source_type',\n",
    "                    'source_system_tab',\n",
    "                    'gender',\n",
    "                    'registered_via',\n",
    "                    'registration_year',\n",
    "                    'registration_month',\n",
    "                    'registration_day',\n",
    "                    'expiration_year',\n",
    "                    'expiration_month',\n",
    "                    'expiration_day',\n",
    "                    'artist_name',\n",
    "                    'age_buckets']\n",
    "wide_columns = []\n",
    "for tag in wide_columns_tag:\n",
    "    wide_columns.append(columns[tag])\n",
    "\n",
    "# cross column feature \n",
    "# these feature is related so hash them together to produce new feature\n",
    "wide_columns.append(tf.feature_column.crossed_column([columns['msno'], columns['song_id']], \n",
    "                                                     hash_bucket_size=int(1e6)))\n",
    "\n",
    "wide_columns.append(tf.feature_column.crossed_column([columns['registration_year'], columns['expiration_year']], \n",
    "                                                     hash_bucket_size=int(1e2)))\n",
    "\n",
    "wide_columns.append(tf.feature_column.crossed_column([columns['language'], columns['msno']], \n",
    "                                                     hash_bucket_size=int(1e6)))\n",
    "\n",
    "wide_columns.append(tf.feature_column.crossed_column([columns['age_buckets'], columns['artist_name']], \n",
    "                                                     hash_bucket_size=int(1e6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['source_screen_name',\n",
       " 'source_type',\n",
       " 'source_system_tab',\n",
       " _CrossedColumn(keys=(_SparseColumnHashed(column_name='msno', is_integerized=False, bucket_size=30000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='song_id', is_integerized=False, bucket_size=100000, lookup_config=None, combiner='sum', dtype=tf.string)), hash_bucket_size=1000000, hash_key=None),\n",
       " _CrossedColumn(keys=(_SparseColumnKeys(column_name='registration_year', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('2012', '2008', '2015', '2009', '2006', '2014', '2013', '2005', '2010', '2011', '2016', '2007', '2017', '2004'), num_oov_buckets=0, vocab_size=14, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumnKeys(column_name='expiration_year', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('2012', '2008', '2015', '2009', '2006', '2019', '2014', '2013', '2018', '1970', '2005', '2010', '2020', '2011', '2016', '2007', '2017', '2004'), num_oov_buckets=0, vocab_size=18, default_value=-1), combiner='sum', dtype=tf.string)), hash_bucket_size=100, hash_key=None),\n",
       " _CrossedColumn(keys=(_SparseColumnKeys(column_name='language', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('-1.0', '38.0', '31.0', '17.0', '24.0', '3.0', '52.0', '59.0', '45.0', '10.0'), num_oov_buckets=0, vocab_size=10, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumnHashed(column_name='msno', is_integerized=False, bucket_size=30000, lookup_config=None, combiner='sum', dtype=tf.string)), hash_bucket_size=1000000, hash_key=None),\n",
       " _CrossedColumn(keys=(_BucketizedColumn(source_column=_NumericColumn(key='age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(15, 18, 25, 30, 35, 40, 45, 50, 55, 60, 65)), _SparseColumnHashed(column_name='artist_name', is_integerized=False, bucket_size=10000, lookup_config=None, combiner='sum', dtype=tf.string)), hash_bucket_size=1000000, hash_key=None)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n"
     ]
    }
   ],
   "source": [
    "# deep model input\n",
    "# dense embedded feature\n",
    "deep_columns = []\n",
    "_categoricals = [\n",
    "    'source_screen_name',\n",
    "    'source_type',\n",
    "    'source_system_tab',\n",
    "    'language',\n",
    "    'city',\n",
    "    'gender',\n",
    "    'registered_via',\n",
    "    'registration_year',\n",
    "    'registration_month',\n",
    "    'registration_day',\n",
    "    'expiration_year',\n",
    "    'expiration_month',\n",
    "    'expiration_day',\n",
    "    'song_year',\n",
    "#     'age_buckets',\n",
    "#     'song_length_buckets',\n",
    "    'artist_name',\n",
    "    'msno',\n",
    "    'song_id'\n",
    "]\n",
    "for col in _categoricals:\n",
    "    deep_columns.append(tf.contrib.layers.embedding_column(columns[col], dimension=8))\n",
    "\n",
    "# 这一点有问题 我加到deep_column 里面了\n",
    "# Represents multi-hot representation of given categorical column???\n",
    "# _buckets = ['age_buckets', 'song_length_buckets', 'artist_name']\n",
    "# for col in _buckets:\n",
    "#     deep_columns.append(tf.feature_column.indicator_column(columns[col]))\n",
    "\n",
    "# numerical columns\n",
    "_numericals = [\n",
    "    'age', \n",
    "    'song_length',\n",
    "    'genre_count', \n",
    "    'lyricist_count'\n",
    "]\n",
    "for col in _numericals:\n",
    "    deep_columns.append(columns[col])\n",
    "\n",
    "# Todo: high dimension categorical\n",
    "# for col in ['msno', 'song_id', 'artist_name']:\n",
    "#     deep_columns.append(tf.contrib.layers.embedding_column(columns[col], dimension=64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 这也有问题 这个应该multihot! 不该drop 掉 gereids吧 我还没找到怎么加明天看你也可以先看看\n",
    "# here use multi-hot???\n",
    "# tf.feature_column.categorical_column_with_vocabulary_list??\n",
    "# tf.feature_column.indicator_column(columns[col])\n",
    "X_train = df_train.drop(['target', 'genre_ids', 'composer', 'lyricist'], axis=1)\n",
    "# cannot accept element with type list\n",
    "y_train = df_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = df_test.drop(['genre_ids', 'composer', 'lyricist'], axis=1)\n",
    "test_ids = df_test['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Modeling and evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 split into train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into train and validation set\n",
    "def split(X, y, val_size):\n",
    "    '''\n",
    "    split the data into training and validation set\n",
    "    '''\n",
    "    test_num = int(val_size * X.shape[0])\n",
    "    return X[test_num:], X[:test_num], y[test_num:], y[:test_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_trn, X_val, y_trn, y_val = split(X_train, y_train, val_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use DNNLinearCombinedClassifier estimator provided by tensorflow API\n",
    "def build_estimator(model_dir):\n",
    "      return tf.contrib.learn.DNNLinearCombinedClassifier(\n",
    "          model_dir=model_dir,\n",
    "          linear_feature_columns=wide_columns,\n",
    "          dnn_feature_columns=deep_columns,\n",
    "          # dnn_dropout=0.4,\n",
    "          dnn_optimizer=tf.train.AdagradOptimizer(learning_rate=0.3),\n",
    "          dnn_hidden_units=[100, 32],\n",
    "          fix_global_step_increment_bug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input data to model (train: small batch, test: whole testdata)\n",
    "def train_input_fn(shuffle, train = True):\n",
    "    if train:\n",
    "        return tf.estimator.inputs.pandas_input_fn(\n",
    "            x=X_trn,\n",
    "            y=y_trn,\n",
    "            batch_size=128,\n",
    "            num_epochs=1,\n",
    "            shuffle=shuffle,\n",
    "            num_threads=1,\n",
    "            target_column='target'\n",
    "        )\n",
    "    else:\n",
    "        return tf.estimator.inputs.pandas_input_fn(\n",
    "            x=X_val,\n",
    "            y=y_val,\n",
    "            batch_size=X_val.shape[0],\n",
    "            num_epochs=1,\n",
    "            shuffle=False,\n",
    "            num_threads=1,\n",
    "            target_column='target'\n",
    "        )\n",
    "def test_input_fn():\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x=X_test,\n",
    "        y=None,\n",
    "        batch_size=128,\n",
    "        num_epochs=1,\n",
    "        shuffle=False,\n",
    "        num_threads=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# produce submission\n",
    "def produce(m):\n",
    "    print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    predictions = m.predict(input_fn=test_input_fn())\n",
    "    result_df = pd.DataFrame()\n",
    "    result_df['id'] = test_ids\n",
    "    result_df['target'] = list(predictions)\n",
    "\n",
    "    timestamp = time.strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "    filename = '../result/' + timestamp + '.csv.gz'\n",
    "    result_df.to_csv(filename, compression = 'gzip', index=False, float_format = '%.5f')\n",
    "    print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/_n/8c5rjjsj4kl5v76kz7z1p9qm0000gn/T/tmpsspr74fg\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a9dd3b240>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/var/folders/_n/8c5rjjsj4kl5v76kz7z1p9qm0000gn/T/tmpsspr74fg'}\n",
      "2017-11-21 20:47:07\n",
      "WARNING:tensorflow:From /Users/yuwang/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:642: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:Casting <dtype: 'int64'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'int64'> labels to bool.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/_n/8c5rjjsj4kl5v76kz7z1p9qm0000gn/T/tmpsspr74fg/model.ckpt.\n",
      "INFO:tensorflow:loss = 11166.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 32.767\n",
      "INFO:tensorflow:loss = 0.559388, step = 101 (3.055 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.3084\n",
      "INFO:tensorflow:loss = 0.59425, step = 201 (2.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.6628\n",
      "INFO:tensorflow:loss = 0.619471, step = 301 (2.521 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.0953\n",
      "INFO:tensorflow:loss = 0.539616, step = 401 (2.772 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.6275\n",
      "INFO:tensorflow:loss = 0.636164, step = 501 (3.072 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.0565\n",
      "INFO:tensorflow:loss = 0.660863, step = 601 (2.691 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.398\n",
      "INFO:tensorflow:loss = 0.647481, step = 701 (2.999 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.0375\n",
      "INFO:tensorflow:loss = 0.633458, step = 801 (3.028 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.4917\n",
      "INFO:tensorflow:loss = 0.59343, step = 901 (2.592 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.94\n",
      "INFO:tensorflow:loss = 0.589621, step = 1001 (2.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.1383\n",
      "INFO:tensorflow:loss = 0.586569, step = 1101 (2.373 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.8918\n",
      "INFO:tensorflow:loss = 0.582182, step = 1201 (2.446 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.1791\n",
      "INFO:tensorflow:loss = 0.602876, step = 1301 (2.371 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.9522\n",
      "INFO:tensorflow:loss = 0.561164, step = 1401 (2.708 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.9308\n",
      "INFO:tensorflow:loss = 0.553536, step = 1501 (2.947 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.4935\n",
      "INFO:tensorflow:loss = 0.572517, step = 1601 (2.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.8688\n",
      "INFO:tensorflow:loss = 0.628143, step = 1701 (2.576 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.0453\n",
      "INFO:tensorflow:loss = 0.652916, step = 1801 (2.557 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.6451\n",
      "INFO:tensorflow:loss = 0.559456, step = 1901 (2.729 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.0928\n",
      "INFO:tensorflow:loss = 0.592787, step = 2001 (2.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.0446\n",
      "INFO:tensorflow:loss = 0.625793, step = 2101 (2.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.8445\n",
      "INFO:tensorflow:loss = 0.617391, step = 2201 (2.574 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.284\n",
      "INFO:tensorflow:loss = 0.611189, step = 2301 (2.310 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.6626\n",
      "INFO:tensorflow:loss = 0.59045, step = 2401 (2.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.1515\n",
      "INFO:tensorflow:loss = 0.620207, step = 2501 (2.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.587\n",
      "INFO:tensorflow:loss = 0.606972, step = 2601 (2.295 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.3297\n",
      "INFO:tensorflow:loss = 0.632514, step = 2701 (2.419 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.8814\n",
      "INFO:tensorflow:loss = 0.573211, step = 2801 (2.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.1017\n",
      "INFO:tensorflow:loss = 0.634982, step = 2901 (2.321 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.5936\n",
      "INFO:tensorflow:loss = 0.61662, step = 3001 (2.347 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.6685\n",
      "INFO:tensorflow:loss = 0.612169, step = 3101 (2.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.0675\n",
      "INFO:tensorflow:loss = 0.543335, step = 3201 (2.435 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.6129\n",
      "INFO:tensorflow:loss = 0.573766, step = 3301 (2.525 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.7141\n",
      "INFO:tensorflow:loss = 0.596042, step = 3401 (2.288 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.8572\n",
      "INFO:tensorflow:loss = 0.663526, step = 3501 (2.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.9405\n",
      "INFO:tensorflow:loss = 0.544903, step = 3601 (2.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.5268\n",
      "INFO:tensorflow:loss = 0.644376, step = 3701 (2.061 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.1731\n",
      "INFO:tensorflow:loss = 0.57102, step = 3801 (3.017 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.925\n",
      "INFO:tensorflow:loss = 0.661122, step = 3901 (2.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.9817\n",
      "INFO:tensorflow:loss = 0.613101, step = 4001 (2.864 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.2675\n",
      "INFO:tensorflow:loss = 0.658685, step = 4101 (3.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.9257\n",
      "INFO:tensorflow:loss = 0.568129, step = 4201 (3.038 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.0515\n",
      "INFO:tensorflow:loss = 0.630239, step = 4301 (3.025 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.8985\n",
      "INFO:tensorflow:loss = 0.695616, step = 4401 (2.643 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.3438\n",
      "INFO:tensorflow:loss = 0.585031, step = 4501 (2.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.3784\n",
      "INFO:tensorflow:loss = 0.668305, step = 4601 (2.537 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.9322\n",
      "INFO:tensorflow:loss = 0.628769, step = 4701 (2.637 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.3044\n",
      "INFO:tensorflow:loss = 0.61803, step = 4801 (2.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.2414\n",
      "INFO:tensorflow:loss = 0.589122, step = 4901 (3.009 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.9011\n",
      "INFO:tensorflow:loss = 0.591009, step = 5001 (2.949 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.3783\n",
      "INFO:tensorflow:loss = 0.587678, step = 5101 (2.749 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.1844\n",
      "INFO:tensorflow:loss = 0.688329, step = 5201 (2.371 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.8758\n",
      "INFO:tensorflow:loss = 0.624407, step = 5301 (2.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.389\n",
      "INFO:tensorflow:loss = 0.594216, step = 5401 (2.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.0146\n",
      "INFO:tensorflow:loss = 0.621611, step = 5501 (2.436 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.029\n",
      "INFO:tensorflow:loss = 0.594096, step = 5601 (2.271 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.4375\n",
      "INFO:tensorflow:loss = 0.652814, step = 5701 (2.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.7666\n",
      "INFO:tensorflow:loss = 0.561, step = 5801 (2.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.8298\n",
      "INFO:tensorflow:loss = 0.616728, step = 5901 (2.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.1258\n",
      "INFO:tensorflow:loss = 0.586911, step = 6001 (2.319 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.1464\n",
      "INFO:tensorflow:loss = 0.683483, step = 6101 (2.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.1051\n",
      "INFO:tensorflow:loss = 0.679052, step = 6201 (2.561 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.8368\n",
      "INFO:tensorflow:loss = 0.58321, step = 6301 (2.710 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.6198\n",
      "INFO:tensorflow:loss = 0.644955, step = 6401 (2.589 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.7367\n",
      "INFO:tensorflow:loss = 0.603954, step = 6501 (2.799 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.4173\n",
      "INFO:tensorflow:loss = 0.608995, step = 6601 (2.679 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 35.9466\n",
      "INFO:tensorflow:loss = 0.577265, step = 6701 (2.776 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.4652\n",
      "INFO:tensorflow:loss = 0.603229, step = 6801 (2.669 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.5803\n",
      "INFO:tensorflow:loss = 0.595863, step = 6901 (2.734 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.2659\n",
      "INFO:tensorflow:loss = 0.653866, step = 7001 (2.920 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.9406\n",
      "INFO:tensorflow:loss = 0.59747, step = 7101 (2.504 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.8653\n",
      "INFO:tensorflow:loss = 0.667987, step = 7201 (2.507 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.8815\n",
      "INFO:tensorflow:loss = 0.685732, step = 7301 (2.446 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.5913\n",
      "INFO:tensorflow:loss = 0.591227, step = 7401 (2.350 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.8584\n",
      "INFO:tensorflow:loss = 0.58356, step = 7501 (2.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.5045\n",
      "INFO:tensorflow:loss = 0.583701, step = 7601 (2.297 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.1602\n",
      "INFO:tensorflow:loss = 0.645436, step = 7701 (2.317 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.3483\n",
      "INFO:tensorflow:loss = 0.654734, step = 7801 (2.609 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.9347\n",
      "INFO:tensorflow:loss = 0.642037, step = 7901 (2.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.3492\n",
      "INFO:tensorflow:loss = 0.673883, step = 8001 (2.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.2614\n",
      "INFO:tensorflow:loss = 0.576838, step = 8101 (2.366 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.1251\n",
      "INFO:tensorflow:loss = 0.62117, step = 8201 (2.432 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.198\n",
      "INFO:tensorflow:loss = 0.658135, step = 8301 (2.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.7917\n",
      "INFO:tensorflow:loss = 0.670133, step = 8401 (2.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.234\n",
      "INFO:tensorflow:loss = 0.642773, step = 8501 (2.313 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.6546\n",
      "INFO:tensorflow:loss = 0.658758, step = 8601 (2.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.8701\n",
      "INFO:tensorflow:loss = 0.591137, step = 8701 (2.868 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.7074\n",
      "INFO:tensorflow:loss = 0.575382, step = 8801 (2.726 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.93\n",
      "INFO:tensorflow:loss = 0.615558, step = 8901 (2.782 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.0681\n",
      "INFO:tensorflow:loss = 0.595588, step = 9001 (2.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.4575\n",
      "INFO:tensorflow:loss = 0.539496, step = 9101 (2.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.7796\n",
      "INFO:tensorflow:loss = 0.614341, step = 9201 (2.794 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.0958\n",
      "INFO:tensorflow:loss = 0.613736, step = 9301 (2.320 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.4944\n",
      "INFO:tensorflow:loss = 0.614326, step = 9401 (2.818 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.7405\n",
      "INFO:tensorflow:loss = 0.633945, step = 9501 (2.650 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.4825\n",
      "INFO:tensorflow:loss = 0.597294, step = 9601 (2.532 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.6301\n",
      "INFO:tensorflow:loss = 0.623172, step = 9701 (2.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.2246\n",
      "INFO:tensorflow:loss = 0.627129, step = 9801 (2.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.2819\n",
      "INFO:tensorflow:loss = 0.616387, step = 9901 (2.611 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.2506\n",
      "INFO:tensorflow:loss = 0.631939, step = 10001 (2.615 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.511\n",
      "INFO:tensorflow:loss = 0.594293, step = 10101 (2.597 sec)\n"
     ]
    }
   ],
   "source": [
    "# add evaludation!!!\n",
    "# def train_and_eval():\n",
    "# model_dir = \"../models/model2\"\n",
    "model_dir = None\n",
    "# print(\"model directory = %s\" % model_dir)\n",
    "\n",
    "m = build_estimator(model_dir)\n",
    "\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "for iter in range(1):\n",
    "    m.fit(input_fn=train_input_fn(shuffle=True))\n",
    "    if iter % 2 == 0:\n",
    "        print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        results = m.evaluate(input_fn=train_input_fn(shuffle=False, train = False))\n",
    "        print('results:' + '=' * 30)\n",
    "        for key in sorted(results):\n",
    "            print(\"%s: %s\" % (key, results[key]))\n",
    "        print('results end:' + '=' * 30)\n",
    "        print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "produce(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "for iter in range(5):\n",
    "    # m.fit(input_fn=lambda: input_fn(df=df_train[:100], train=True), steps=10)\n",
    "    # results = m.evaluate(input_fn=lambda: input_fn(df_train[:10], train=True), steps=1)\n",
    "    m.fit(input_fn=train_input_fn(shuffle=True))\n",
    "\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "results = m.evaluate(input_fn=train_input_fn(shuffle=False))\n",
    "# results = m.evaluate(input_fn=lambda: input_fn(df_train[:10], train=True), steps=1)\n",
    "print('results:' + '=' * 30)\n",
    "for key in sorted(results):\n",
    "    print(\"%s: %s\" % (key, results[key]))\n",
    "print('results end:' + '=' * 30)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "produce(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 001\n",
    "\n",
    "- larger for msno 10000->30000, embedding dimension 32 -> 64\n",
    "- song_id, 10000->1e4, embedding dimension 32 -> 64\n",
    "- adagrad lr=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def train_and_eval():\n",
    "# model_dir = \"../models/model2\"\n",
    "model_dir = None\n",
    "print(\"model directory = %s\" % model_dir)\n",
    "\n",
    "m = build_estimator(model_dir)\n",
    "\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "for iter in range(5):\n",
    "    # m.fit(input_fn=lambda: input_fn(df=df_train[:100], train=True), steps=10)\n",
    "    # results = m.evaluate(input_fn=lambda: input_fn(df_train[:10], train=True), steps=1)\n",
    "    m.fit(input_fn=train_input_fn(shuffle=True))\n",
    "\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "results = m.evaluate(input_fn=train_input_fn(shuffle=False))\n",
    "# results = m.evaluate(input_fn=lambda: input_fn(df_train[:10], train=True), steps=1)\n",
    "print('results:' + '=' * 30)\n",
    "for key in sorted(results):\n",
    "    print(\"%s: %s\" % (key, results[key]))\n",
    "print('results end:' + '=' * 30)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "produce(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "for iter in range(1):\n",
    "    # m.fit(input_fn=lambda: input_fn(df=df_train[:100], train=True), steps=10)\n",
    "    # results = m.evaluate(input_fn=lambda: input_fn(df_train[:10], train=True), steps=1)\n",
    "    m.fit(input_fn=train_input_fn(shuffle=True))\n",
    "\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "results = m.evaluate(input_fn=train_input_fn(shuffle=False))\n",
    "# results = m.evaluate(input_fn=lambda: input_fn(df_train[:10], train=True), steps=1)\n",
    "print('results:' + '=' * 30)\n",
    "for key in sorted(results):\n",
    "    print(\"%s: %s\" % (key, results[key]))\n",
    "print('results end:' + '=' * 30)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "produce(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
