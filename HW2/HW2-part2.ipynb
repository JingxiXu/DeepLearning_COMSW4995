{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions, DO NOT modify this\n",
    "\n",
    "def get_img_array(path):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imread(path)\n",
    "\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)\n",
    "        \n",
    "# Functions to load data, DO NOT change these\n",
    "\n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    y_one_hot[y] = 1\n",
    "    return y_one_hot.T\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = get_img_array(f).astype(np.float)\n",
    "#         img_arr = img_arr.flatten() / 255.0\n",
    "        images.append(img_arr)\n",
    "    # X = np.column_stack(images)\n",
    "    X = np.stack(images, axis=0)\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n",
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "Loaded 10000/10000\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_root_path = 'cifar10-hw/'\n",
    "X_train, y_train = get_train_data(data_root_path) # this may take a few minutes\n",
    "X_test = get_images(data_root_path + 'test')\n",
    "print('Data loading done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group's helper function\n",
    "def split(X, y, val_size):\n",
    "    '''\n",
    "    split the data into training and validation set\n",
    "    '''\n",
    "    indices = np.random.permutation(X.shape[0])\n",
    "    test_num = int(val_size * X.shape[0])\n",
    "    return X[indices[test_num:]], X[indices[:test_num]], y[indices[test_num:]], y[indices[:test_num]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3)\n",
      "(5000, 32, 32, 3)\n",
      "(45000,)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "X_trn, X_val, y_trn, y_val = split(X_train, y_train, val_size=0.1)\n",
    "print(X_trn.shape)\n",
    "print(X_val.shape)\n",
    "print(y_trn.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(raw_images):\n",
    "    images = np.empty_like(raw_images)\n",
    "    np.copyto(images, raw_images)\n",
    "    for i in range(images.shape[0]):\n",
    "        old = images[i]\n",
    "        new = (old - np.mean(old)) / np.std(old)\n",
    "        images[i, :, :, :] = new\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_trn = normalize(X_trn)\n",
    "X_val = normalize(X_val)\n",
    "X_tst = normalize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GLobal value\n",
    "H, W, T = 32, 32, 10 # height/width of images, number of classes of images\n",
    "cnns = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN():\n",
    "    def __init__(self, model_fn, trainer, global_step=None):\n",
    "        \n",
    "        if global_step is None:\n",
    "            tf.reset_default_graph()\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "        self.iter_cnt = 0\n",
    "        self.X = tf.placeholder(tf.float32, [None, H, W, 3])\n",
    "        self.Y = tf.placeholder(tf.int64, [None])\n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "        with tf.name_scope('loss'):\n",
    "            logit, loss = model_fn(self.X, self.Y, self.is_training)\n",
    "            tf.summary.scalar('loss', loss)\n",
    "        # https://stackoverflow.com/a/43285333\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            with tf.variable_scope('optimizer'):\n",
    "                gradients = trainer.compute_gradients(loss)\n",
    "                train_op = trainer.apply_gradients(gradients, global_step=global_step)\n",
    "        for tup in gradients:\n",
    "            the_string = str(tup[1])\n",
    "            if 'conv1/kernel:0' in the_string:\n",
    "                tf.summary.histogram('Gradient/conv1', tup[0])\n",
    "            elif 'conv2/kernel:0' in the_string:\n",
    "                tf.summary.histogram('Gradient/conv2', tup[0])\n",
    "            elif 'last/kernel:0' in the_string:\n",
    "                tf.summary.histogram('Gradient/last_fc', tup[0])\n",
    "        # train_op = trainer.minimize(loss)\n",
    "        # Accuracy\n",
    "        correct = tf.equal(tf.argmax(logit, 1), self.Y)\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "        self.sess = tf.Session()\n",
    "        merged = tf.summary.merge_all()\n",
    "        self.train_writer = tf.summary.FileWriter('/Users/jingxixu/Desktop/train', self.sess.graph)\n",
    "        self.test_writer = tf.summary.FileWriter('/Users/jingxixu/Desktop/test')\n",
    "        self.variables = {\n",
    "            'train': [merged, loss, correct, train_op],\n",
    "            'validate': [merged, loss, correct, accuracy],\n",
    "            'test': logit\n",
    "        }\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def batch_gen(self, Xd, Yd, batch_size, shuffle=True):\n",
    "        indicies = np.arange(Xd.shape[0])\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indicies)\n",
    "        for i in range(int(math.ceil(Xd.shape[0] / batch_size))):\n",
    "            start_idx = (i * batch_size) % Xd.shape[0]\n",
    "            idx = indicies[start_idx:start_idx + batch_size]\n",
    "            yield Xd[idx, :], Yd[idx]\n",
    "\n",
    "    def run(self, Xd, Yd, epochs, batch_size, print_every, plot_losses, status):\n",
    "        iter_cnt = 0\n",
    "        for e in range(epochs):\n",
    "            correct = 0\n",
    "            losses = []\n",
    "            for Xb, Yb in self.batch_gen(Xd, Yd, batch_size, shuffle = True):\n",
    "                self.iter_cnt += 1\n",
    "                feed_dict = {self.X: Xb, self.Y: Yb, self.is_training: status=='train'}\n",
    "                summary, loss, corr, _ = self.sess.run(self.variables[status], feed_dict = feed_dict)\n",
    "                if status == 'train':\n",
    "                    self.train_writer.add_summary(summary, self.iter_cnt)\n",
    "                elif status == 'validate':\n",
    "                    self.test_writer.add_summary(summary, self.iter_cnt)\n",
    "                losses.append(loss)\n",
    "                correct += np.sum(corr)\n",
    "                if status == 'train' and iter_cnt % print_every == 0:\n",
    "                    print(\"{} Iter {}: batch trn loss = {:.3f}, accuracy = {:.3f}\".format(\n",
    "                        datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        iter_cnt,\n",
    "                        loss,\n",
    "                        np.mean(corr),\n",
    "                    ))\n",
    "                iter_cnt += 1\n",
    "            epoch_loss = np.mean(losses)\n",
    "            epoch_accuracy = correct / Xd.shape[0]\n",
    "            print(\"Epoch {}: mean loss = {:.3f}, accuracy = {:.3f}\".format(\n",
    "                e, epoch_loss, epoch_accuracy))\n",
    "            if plot_losses:\n",
    "                plt.plot(losses)\n",
    "                plt.grid(True)\n",
    "                plt.title('Epoch {} Mean Loss'.format(e+1))\n",
    "                plt.xlabel('minibatch number')\n",
    "                plt.ylabel('minibatch mean loss')\n",
    "                plt.show()\n",
    "        \n",
    "        return epoch_loss, epoch_accuracy\n",
    "\n",
    "    def train(self, Xd, Yd, epochs=1, batch_size=50, print_every=100, plot_losses=False):\n",
    "        return self.run(Xd, Yd, epochs, batch_size, print_every, plot_losses, status='train')\n",
    "            \n",
    "    def validate(self, Xd, Yd, epochs=1, batch_size=50, print_every=100, plot_losses=False):\n",
    "        return self.run(Xd, Yd, epochs, batch_size, print_every, plot_losses, status='validate')\n",
    "    \n",
    "    def predict(self, Xd):\n",
    "        feed_dict = {self.X: Xd, self.is_training: False}\n",
    "        logit = self.sess.run(self.variables['test'], feed_dict = feed_dict)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [0103] normalize + dropout\n",
    "- base (0101): add normalize and dropout = 0.5\n",
    "- batch size still 50\n",
    "- trn: , val: (05 epochs)\n",
    "- trn: , val: (10 epochs)\n",
    "- trn: , val: (15 epochs)\n",
    "- trn: , val: (20 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3136\n"
     ]
    }
   ],
   "source": [
    "def model_fn(layer_input, labels, is_training):\n",
    "    reg_scale= 0.1\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(scale=reg_scale)\n",
    "    # initializer = tf.contrib.layers.xavier_initializer()\n",
    "    initializer = None\n",
    "    h, w = H, W\n",
    "    # conv - bn - max_pool\n",
    "    F1 = 64\n",
    "    P1, S1 = 3, 2 # pool_size, strides\n",
    "    layer = tf.layers.conv2d(\n",
    "        inputs=layer_input, filters=F1, kernel_size=[5, 5], padding='same', activation=tf.nn.relu,\n",
    "        kernel_regularizer=regularizer, kernel_initializer=initializer, name='conv1')\n",
    "    layer = tf.layers.batch_normalization(\n",
    "        inputs=layer, training=is_training,\n",
    "        beta_regularizer=regularizer, gamma_regularizer=regularizer)\n",
    "    layer = tf.layers.max_pooling2d(inputs=layer, pool_size=P1, strides=S1)\n",
    "    h = math.floor((h - P1 + S1) / S1)\n",
    "    w = math.floor((w - P1 + S1) / S1)\n",
    "    # conv - bn - max_pool\n",
    "    F2 = 64\n",
    "    P2, S2 = 3, 2 # pool_size, strides\n",
    "    layer = tf.layers.conv2d(\n",
    "        inputs=layer, filters=F2, kernel_size=[5, 5], padding='same', activation=tf.nn.relu,\n",
    "        kernel_regularizer=regularizer, kernel_initializer=initializer, name='conv2')\n",
    "    layer = tf.layers.batch_normalization(\n",
    "        inputs=layer, training=is_training,\n",
    "        beta_regularizer=regularizer, gamma_regularizer=regularizer)\n",
    "    layer = tf.layers.max_pooling2d(inputs=layer, pool_size=P2, strides=S2)\n",
    "    h = math.floor((h - P2 + S2) / S2)\n",
    "    w = math.floor((w - P2 + S2) / S2)\n",
    "    # dense1 - bn - dropout - fc - softmax\n",
    "    flat_size = F2 * h * w\n",
    "    print(flat_size)\n",
    "    layer = tf.reshape(layer, [-1, flat_size])\n",
    "    layer = tf.layers.dense(\n",
    "        inputs=layer, units=1024, activation=tf.nn.relu,\n",
    "        kernel_regularizer=regularizer, kernel_initializer=initializer)\n",
    "    layer = tf.layers.batch_normalization(\n",
    "        inputs=layer, training=is_training,\n",
    "        beta_regularizer=regularizer, gamma_regularizer=regularizer)\n",
    "    layer = tf.layers.dropout(inputs=layer, rate=0.5, training=is_training)\n",
    "    # no activation here for logit, as it will be calculated in loss\n",
    "    logit = tf.layers.dense(\n",
    "        inputs=layer, units=T, activation=None,\n",
    "        kernel_regularizer=regularizer, kernel_initializer=initializer, name='last')\n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int64), depth=10)\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logit)\n",
    "    \n",
    "    return logit, loss\n",
    "\n",
    "\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.03)\n",
    "cnn = CNN(model_fn, trainer)\n",
    "cnns['0103'] = cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: epoch 0\n",
      "2017-10-30 19:38:53 Iter 0: batch trn loss = 3.714, accuracy = 0.060\n",
      "2017-10-30 19:39:22 Iter 100: batch trn loss = 2.642, accuracy = 0.320\n",
      "2017-10-30 19:39:53 Iter 200: batch trn loss = 1.951, accuracy = 0.380\n",
      "2017-10-30 19:40:24 Iter 300: batch trn loss = 1.616, accuracy = 0.500\n",
      "2017-10-30 19:40:55 Iter 400: batch trn loss = 1.143, accuracy = 0.520\n",
      "2017-10-30 19:41:26 Iter 500: batch trn loss = 1.747, accuracy = 0.460\n",
      "2017-10-30 19:41:56 Iter 600: batch trn loss = 1.292, accuracy = 0.540\n",
      "2017-10-30 19:42:27 Iter 700: batch trn loss = 1.239, accuracy = 0.520\n",
      "2017-10-30 19:42:58 Iter 800: batch trn loss = 1.406, accuracy = 0.600\n",
      "Epoch 0: mean loss = 1.591, accuracy = 0.492\n",
      "validation\n",
      "Epoch 0: mean loss = 1.179, accuracy = 0.604\n",
      "train: epoch 1\n",
      "2017-10-30 19:44:07 Iter 0: batch trn loss = 1.550, accuracy = 0.500\n",
      "2017-10-30 19:44:39 Iter 100: batch trn loss = 0.958, accuracy = 0.600\n",
      "2017-10-30 19:45:10 Iter 200: batch trn loss = 1.020, accuracy = 0.620\n",
      "2017-10-30 19:45:42 Iter 300: batch trn loss = 1.139, accuracy = 0.640\n",
      "2017-10-30 19:46:15 Iter 400: batch trn loss = 1.310, accuracy = 0.640\n",
      "2017-10-30 19:46:47 Iter 500: batch trn loss = 0.858, accuracy = 0.660\n",
      "2017-10-30 19:47:18 Iter 600: batch trn loss = 1.044, accuracy = 0.620\n",
      "2017-10-30 19:47:49 Iter 700: batch trn loss = 1.232, accuracy = 0.480\n",
      "2017-10-30 19:48:20 Iter 800: batch trn loss = 1.365, accuracy = 0.500\n",
      "Epoch 0: mean loss = 1.086, accuracy = 0.629\n",
      "validation\n",
      "Epoch 0: mean loss = 1.018, accuracy = 0.668\n",
      "train: epoch 2\n",
      "2017-10-30 19:49:28 Iter 0: batch trn loss = 0.885, accuracy = 0.680\n",
      "2017-10-30 19:50:00 Iter 100: batch trn loss = 0.608, accuracy = 0.820\n",
      "2017-10-30 19:50:32 Iter 200: batch trn loss = 0.832, accuracy = 0.720\n",
      "2017-10-30 19:51:04 Iter 300: batch trn loss = 0.837, accuracy = 0.760\n",
      "2017-10-30 19:51:34 Iter 400: batch trn loss = 0.818, accuracy = 0.820\n",
      "2017-10-30 19:52:06 Iter 500: batch trn loss = 0.891, accuracy = 0.660\n",
      "2017-10-30 19:52:37 Iter 600: batch trn loss = 0.896, accuracy = 0.680\n",
      "2017-10-30 19:53:07 Iter 700: batch trn loss = 0.876, accuracy = 0.720\n",
      "2017-10-30 19:53:38 Iter 800: batch trn loss = 0.735, accuracy = 0.680\n",
      "Epoch 0: mean loss = 0.941, accuracy = 0.678\n",
      "validation\n",
      "Epoch 0: mean loss = 0.916, accuracy = 0.693\n",
      "train: epoch 3\n",
      "2017-10-30 19:54:46 Iter 0: batch trn loss = 0.742, accuracy = 0.680\n",
      "2017-10-30 19:55:19 Iter 100: batch trn loss = 0.889, accuracy = 0.660\n",
      "2017-10-30 19:55:51 Iter 200: batch trn loss = 0.974, accuracy = 0.720\n",
      "2017-10-30 19:56:22 Iter 300: batch trn loss = 0.851, accuracy = 0.740\n",
      "2017-10-30 19:56:53 Iter 400: batch trn loss = 1.147, accuracy = 0.580\n",
      "2017-10-30 19:57:24 Iter 500: batch trn loss = 0.936, accuracy = 0.680\n",
      "2017-10-30 19:57:55 Iter 600: batch trn loss = 1.084, accuracy = 0.660\n",
      "2017-10-30 19:58:26 Iter 700: batch trn loss = 1.082, accuracy = 0.680\n",
      "2017-10-30 19:58:57 Iter 800: batch trn loss = 0.873, accuracy = 0.680\n",
      "Epoch 0: mean loss = 0.846, accuracy = 0.710\n",
      "validation\n",
      "Epoch 0: mean loss = 0.785, accuracy = 0.729\n",
      "train: epoch 4\n",
      "2017-10-30 20:00:02 Iter 0: batch trn loss = 0.710, accuracy = 0.780\n",
      "2017-10-30 20:00:33 Iter 100: batch trn loss = 0.714, accuracy = 0.780\n",
      "2017-10-30 20:01:04 Iter 200: batch trn loss = 0.660, accuracy = 0.700\n",
      "2017-10-30 20:01:35 Iter 300: batch trn loss = 0.641, accuracy = 0.780\n",
      "2017-10-30 20:02:05 Iter 400: batch trn loss = 0.792, accuracy = 0.720\n",
      "2017-10-30 20:02:36 Iter 500: batch trn loss = 1.097, accuracy = 0.620\n",
      "2017-10-30 20:03:06 Iter 600: batch trn loss = 0.621, accuracy = 0.820\n",
      "2017-10-30 20:03:37 Iter 700: batch trn loss = 0.928, accuracy = 0.680\n",
      "2017-10-30 20:04:08 Iter 800: batch trn loss = 0.725, accuracy = 0.720\n",
      "Epoch 0: mean loss = 0.764, accuracy = 0.738\n",
      "validation\n",
      "Epoch 0: mean loss = 0.828, accuracy = 0.721\n",
      "train: epoch 5\n",
      "2017-10-30 20:05:14 Iter 0: batch trn loss = 0.567, accuracy = 0.800\n",
      "2017-10-30 20:05:45 Iter 100: batch trn loss = 0.662, accuracy = 0.800\n",
      "2017-10-30 20:06:16 Iter 200: batch trn loss = 0.512, accuracy = 0.760\n",
      "2017-10-30 20:06:47 Iter 300: batch trn loss = 0.518, accuracy = 0.840\n",
      "2017-10-30 20:07:18 Iter 400: batch trn loss = 0.448, accuracy = 0.840\n",
      "2017-10-30 20:07:49 Iter 500: batch trn loss = 0.781, accuracy = 0.740\n",
      "2017-10-30 20:08:21 Iter 600: batch trn loss = 0.804, accuracy = 0.640\n",
      "2017-10-30 20:08:52 Iter 700: batch trn loss = 0.684, accuracy = 0.740\n",
      "2017-10-30 20:09:23 Iter 800: batch trn loss = 0.621, accuracy = 0.800\n",
      "Epoch 0: mean loss = 0.689, accuracy = 0.761\n",
      "validation\n",
      "Epoch 0: mean loss = 0.765, accuracy = 0.747\n",
      "train: epoch 6\n",
      "2017-10-30 20:10:33 Iter 0: batch trn loss = 0.709, accuracy = 0.700\n",
      "2017-10-30 20:11:05 Iter 100: batch trn loss = 0.439, accuracy = 0.860\n",
      "2017-10-30 20:11:36 Iter 200: batch trn loss = 0.727, accuracy = 0.720\n",
      "2017-10-30 20:12:08 Iter 300: batch trn loss = 0.709, accuracy = 0.760\n",
      "2017-10-30 20:12:39 Iter 400: batch trn loss = 0.535, accuracy = 0.840\n",
      "2017-10-30 20:13:11 Iter 500: batch trn loss = 0.559, accuracy = 0.720\n",
      "2017-10-30 20:13:43 Iter 600: batch trn loss = 0.510, accuracy = 0.820\n",
      "2017-10-30 20:14:15 Iter 700: batch trn loss = 0.435, accuracy = 0.880\n",
      "2017-10-30 20:14:47 Iter 800: batch trn loss = 0.506, accuracy = 0.820\n",
      "Epoch 0: mean loss = 0.637, accuracy = 0.779\n",
      "validation\n",
      "Epoch 0: mean loss = 0.789, accuracy = 0.741\n",
      "train: epoch 7\n",
      "2017-10-30 20:15:53 Iter 0: batch trn loss = 0.580, accuracy = 0.760\n",
      "2017-10-30 20:16:24 Iter 100: batch trn loss = 0.860, accuracy = 0.640\n",
      "2017-10-30 20:16:56 Iter 200: batch trn loss = 0.755, accuracy = 0.700\n",
      "2017-10-30 20:17:29 Iter 300: batch trn loss = 0.448, accuracy = 0.820\n",
      "2017-10-30 20:18:01 Iter 400: batch trn loss = 0.749, accuracy = 0.800\n",
      "2017-10-30 20:18:32 Iter 500: batch trn loss = 0.742, accuracy = 0.780\n",
      "2017-10-30 20:19:04 Iter 600: batch trn loss = 0.634, accuracy = 0.800\n",
      "2017-10-30 20:19:36 Iter 700: batch trn loss = 0.741, accuracy = 0.720\n",
      "2017-10-30 20:20:08 Iter 800: batch trn loss = 0.490, accuracy = 0.840\n",
      "Epoch 0: mean loss = 0.585, accuracy = 0.797\n",
      "validation\n",
      "Epoch 0: mean loss = 0.736, accuracy = 0.762\n",
      "train: epoch 8\n",
      "2017-10-30 20:21:17 Iter 0: batch trn loss = 0.362, accuracy = 0.840\n",
      "2017-10-30 20:21:48 Iter 100: batch trn loss = 0.400, accuracy = 0.860\n",
      "2017-10-30 20:22:20 Iter 200: batch trn loss = 0.524, accuracy = 0.800\n",
      "2017-10-30 20:22:51 Iter 300: batch trn loss = 0.432, accuracy = 0.880\n",
      "2017-10-30 20:23:24 Iter 400: batch trn loss = 0.423, accuracy = 0.780\n",
      "2017-10-30 20:23:56 Iter 500: batch trn loss = 0.613, accuracy = 0.820\n",
      "2017-10-30 20:24:28 Iter 600: batch trn loss = 0.522, accuracy = 0.840\n",
      "2017-10-30 20:24:58 Iter 700: batch trn loss = 0.632, accuracy = 0.760\n",
      "2017-10-30 20:25:31 Iter 800: batch trn loss = 0.482, accuracy = 0.880\n",
      "Epoch 0: mean loss = 0.539, accuracy = 0.812\n",
      "validation\n",
      "Epoch 0: mean loss = 0.721, accuracy = 0.763\n",
      "train: epoch 9\n",
      "2017-10-30 20:26:38 Iter 0: batch trn loss = 0.473, accuracy = 0.820\n",
      "2017-10-30 20:27:11 Iter 100: batch trn loss = 0.263, accuracy = 0.880\n",
      "2017-10-30 20:27:42 Iter 200: batch trn loss = 0.582, accuracy = 0.760\n",
      "2017-10-30 20:28:15 Iter 300: batch trn loss = 0.368, accuracy = 0.860\n",
      "2017-10-30 20:28:46 Iter 400: batch trn loss = 0.366, accuracy = 0.820\n",
      "2017-10-30 20:29:18 Iter 500: batch trn loss = 0.680, accuracy = 0.780\n",
      "2017-10-30 20:29:49 Iter 600: batch trn loss = 0.418, accuracy = 0.880\n",
      "2017-10-30 20:30:21 Iter 700: batch trn loss = 0.442, accuracy = 0.820\n",
      "2017-10-30 20:30:52 Iter 800: batch trn loss = 0.497, accuracy = 0.800\n",
      "Epoch 0: mean loss = 0.500, accuracy = 0.825\n",
      "validation\n",
      "Epoch 0: mean loss = 0.723, accuracy = 0.766\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('train: epoch %d' % i)\n",
    "    cnn.train(X_trn, y_trn, epochs=1, batch_size=50, print_every=100, plot_losses=False)\n",
    "    print('validation')\n",
    "    cnn.validate(X_val, y_val, epochs=1, batch_size=y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: epoch 10\n",
      "2017-10-30 20:42:25 Iter 0: batch trn loss = 0.410, accuracy = 0.820\n",
      "2017-10-30 20:42:54 Iter 100: batch trn loss = 0.350, accuracy = 0.840\n",
      "2017-10-30 20:43:25 Iter 200: batch trn loss = 0.229, accuracy = 0.920\n",
      "2017-10-30 20:43:56 Iter 300: batch trn loss = 0.551, accuracy = 0.800\n",
      "2017-10-30 20:44:29 Iter 400: batch trn loss = 0.386, accuracy = 0.860\n",
      "2017-10-30 20:45:01 Iter 500: batch trn loss = 0.528, accuracy = 0.820\n",
      "2017-10-30 20:45:33 Iter 600: batch trn loss = 0.347, accuracy = 0.880\n",
      "2017-10-30 20:46:05 Iter 700: batch trn loss = 0.423, accuracy = 0.800\n",
      "2017-10-30 20:46:36 Iter 800: batch trn loss = 0.314, accuracy = 0.860\n",
      "Epoch 0: mean loss = 0.456, accuracy = 0.840\n",
      "validation\n",
      "Epoch 0: mean loss = 0.768, accuracy = 0.759\n",
      "train: epoch 11\n",
      "2017-10-30 20:47:45 Iter 0: batch trn loss = 0.434, accuracy = 0.840\n",
      "2017-10-30 20:48:16 Iter 100: batch trn loss = 0.428, accuracy = 0.820\n",
      "2017-10-30 20:48:47 Iter 200: batch trn loss = 0.582, accuracy = 0.780\n",
      "2017-10-30 20:49:18 Iter 300: batch trn loss = 0.294, accuracy = 0.900\n",
      "2017-10-30 20:49:49 Iter 400: batch trn loss = 0.481, accuracy = 0.880\n",
      "2017-10-30 20:50:19 Iter 500: batch trn loss = 0.432, accuracy = 0.780\n",
      "2017-10-30 20:50:49 Iter 600: batch trn loss = 0.440, accuracy = 0.860\n",
      "2017-10-30 20:51:19 Iter 700: batch trn loss = 0.188, accuracy = 0.920\n",
      "2017-10-30 20:51:48 Iter 800: batch trn loss = 0.500, accuracy = 0.840\n",
      "Epoch 0: mean loss = 0.424, accuracy = 0.851\n",
      "validation\n",
      "Epoch 0: mean loss = 0.702, accuracy = 0.781\n",
      "train: epoch 12\n",
      "2017-10-30 20:52:52 Iter 0: batch trn loss = 0.453, accuracy = 0.820\n",
      "2017-10-30 20:53:21 Iter 100: batch trn loss = 0.438, accuracy = 0.840\n",
      "2017-10-30 20:53:51 Iter 200: batch trn loss = 0.617, accuracy = 0.760\n",
      "2017-10-30 20:54:21 Iter 300: batch trn loss = 0.230, accuracy = 0.920\n",
      "2017-10-30 20:54:50 Iter 400: batch trn loss = 0.291, accuracy = 0.880\n",
      "2017-10-30 20:55:20 Iter 500: batch trn loss = 0.561, accuracy = 0.820\n",
      "2017-10-30 20:55:49 Iter 600: batch trn loss = 0.348, accuracy = 0.880\n",
      "2017-10-30 20:56:19 Iter 700: batch trn loss = 0.211, accuracy = 0.940\n",
      "2017-10-30 20:56:49 Iter 800: batch trn loss = 0.609, accuracy = 0.860\n",
      "Epoch 0: mean loss = 0.385, accuracy = 0.864\n",
      "validation\n",
      "Epoch 0: mean loss = 0.739, accuracy = 0.780\n",
      "train: epoch 13\n",
      "2017-10-30 20:57:51 Iter 0: batch trn loss = 0.229, accuracy = 0.920\n",
      "2017-10-30 20:58:21 Iter 100: batch trn loss = 0.405, accuracy = 0.820\n",
      "2017-10-30 20:58:51 Iter 200: batch trn loss = 0.333, accuracy = 0.880\n",
      "2017-10-30 20:59:20 Iter 300: batch trn loss = 0.392, accuracy = 0.860\n",
      "2017-10-30 20:59:50 Iter 400: batch trn loss = 0.549, accuracy = 0.820\n",
      "2017-10-30 21:00:20 Iter 500: batch trn loss = 0.240, accuracy = 0.940\n",
      "2017-10-30 21:00:50 Iter 600: batch trn loss = 0.276, accuracy = 0.860\n",
      "2017-10-30 21:01:21 Iter 700: batch trn loss = 0.491, accuracy = 0.840\n",
      "2017-10-30 21:01:51 Iter 800: batch trn loss = 0.201, accuracy = 0.980\n",
      "Epoch 0: mean loss = 0.359, accuracy = 0.873\n",
      "validation\n",
      "Epoch 0: mean loss = 0.756, accuracy = 0.772\n",
      "train: epoch 14\n",
      "2017-10-30 21:02:53 Iter 0: batch trn loss = 0.210, accuracy = 0.920\n",
      "2017-10-30 21:03:25 Iter 100: batch trn loss = 0.242, accuracy = 0.880\n",
      "2017-10-30 21:03:56 Iter 200: batch trn loss = 0.608, accuracy = 0.760\n",
      "2017-10-30 21:04:27 Iter 300: batch trn loss = 0.433, accuracy = 0.860\n",
      "2017-10-30 21:04:58 Iter 400: batch trn loss = 0.194, accuracy = 0.940\n",
      "2017-10-30 21:05:28 Iter 500: batch trn loss = 0.310, accuracy = 0.880\n"
     ]
    }
   ],
   "source": [
    "for i in range(10, 20):\n",
    "    print('train: epoch %d' % i)\n",
    "    cnn.train(X_trn, y_trn, epochs=1, batch_size=50, print_every=100, plot_losses=False)\n",
    "    print('validation')\n",
    "    cnn.validate(X_val, y_val, epochs=1, batch_size=y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10000)\n"
     ]
    }
   ],
   "source": [
    "Y_pred = cnn.predict(Xd=X_tst)\n",
    "Y_pred = Y_pred.T\n",
    "save_predictions('ans1-uni.npy', Y_pred)\n",
    "\n",
    "loaded_y = np.load('ans1-uni.npy')\n",
    "print(loaded_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "yp = cnn.predict(X_val)\n",
    "z = np.argmax(yp, axis=1)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76580000000000004"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(z == y_val) / 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "2017-10-30 19:37:03 Iter 0: batch trn loss = 4.129, accuracy = 0.100\n",
      "2017-10-30 19:37:32 Iter 100: batch trn loss = 1.976, accuracy = 0.380\n",
      "2017-10-30 19:38:03 Iter 200: batch trn loss = 1.884, accuracy = 0.360\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e84b3edf5d8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 5 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_losses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2d1c2586739f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, Xd, Yd, epochs, batch_size, print_every, plot_losses)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_losses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_losses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2d1c2586739f>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, Xd, Yd, epochs, batch_size, print_every, plot_losses, status)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_cnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mXb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mYb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_cnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_session/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_session/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_session/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_session/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_session/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 5 epochs\n",
    "for i in range(15)\n",
    "print('train')\n",
    "cnn.train(X_trn, y_trn, epochs=1, batch_size=50, print_every=100, plot_losses=False)\n",
    "print('validation')\n",
    "cnn.validate(X_val, y_val, epochs=1, batch_size=y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 10 epochs\n",
    "print('train')\n",
    "cnn.train(X_trn, y_trn, epochs=1, batch_size=50, print_every=100, plot_losses=False)\n",
    "print('validation')\n",
    "cnn.validate(X_val, y_val, epochs=1, batch_size=y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5 epochs\n",
    "print('train')\n",
    "cnn.train(X_trn, y_trn, epochs=1, batch_size=50, print_every=100, plot_losses=False)\n",
    "print('validation')\n",
    "cnn.validate(X_val, y_val, epochs=1, batch_size=y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "2017-10-29 14:54:04 Iter 0: batch trn loss = 0.442, accuracy = 0.840\n",
      "2017-10-29 14:54:36 Iter 100: batch trn loss = 0.553, accuracy = 0.860\n",
      "2017-10-29 14:55:08 Iter 200: batch trn loss = 0.305, accuracy = 0.920\n",
      "2017-10-29 14:55:40 Iter 300: batch trn loss = 0.427, accuracy = 0.880\n",
      "2017-10-29 14:56:12 Iter 400: batch trn loss = 0.415, accuracy = 0.880\n",
      "2017-10-29 14:56:44 Iter 500: batch trn loss = 0.399, accuracy = 0.820\n",
      "2017-10-29 14:57:16 Iter 600: batch trn loss = 0.434, accuracy = 0.880\n",
      "2017-10-29 14:57:48 Iter 700: batch trn loss = 0.358, accuracy = 0.860\n",
      "2017-10-29 14:58:20 Iter 800: batch trn loss = 0.412, accuracy = 0.860\n",
      "Epoch 0: mean loss = 0.441, accuracy = 0.845\n",
      "2017-10-29 14:58:52 Iter 900: batch trn loss = 0.365, accuracy = 0.880\n",
      "2017-10-29 14:59:24 Iter 1000: batch trn loss = 0.640, accuracy = 0.800\n",
      "2017-10-29 14:59:56 Iter 1100: batch trn loss = 0.383, accuracy = 0.900\n",
      "2017-10-29 15:00:28 Iter 1200: batch trn loss = 0.453, accuracy = 0.840\n",
      "2017-10-29 15:01:00 Iter 1300: batch trn loss = 0.578, accuracy = 0.820\n",
      "2017-10-29 15:01:32 Iter 1400: batch trn loss = 0.417, accuracy = 0.860\n",
      "2017-10-29 15:02:04 Iter 1500: batch trn loss = 0.381, accuracy = 0.880\n",
      "2017-10-29 15:02:36 Iter 1600: batch trn loss = 0.271, accuracy = 0.900\n",
      "2017-10-29 15:03:08 Iter 1700: batch trn loss = 0.327, accuracy = 0.880\n",
      "Epoch 1: mean loss = 0.410, accuracy = 0.856\n",
      "2017-10-29 15:03:39 Iter 1800: batch trn loss = 0.351, accuracy = 0.880\n",
      "2017-10-29 15:04:11 Iter 1900: batch trn loss = 0.176, accuracy = 0.920\n",
      "2017-10-29 15:04:43 Iter 2000: batch trn loss = 0.207, accuracy = 0.940\n",
      "2017-10-29 15:05:15 Iter 2100: batch trn loss = 0.158, accuracy = 0.940\n",
      "2017-10-29 15:05:47 Iter 2200: batch trn loss = 0.409, accuracy = 0.800\n",
      "2017-10-29 15:06:19 Iter 2300: batch trn loss = 0.423, accuracy = 0.860\n",
      "2017-10-29 15:06:51 Iter 2400: batch trn loss = 0.727, accuracy = 0.760\n",
      "2017-10-29 15:07:23 Iter 2500: batch trn loss = 0.308, accuracy = 0.920\n",
      "2017-10-29 15:07:55 Iter 2600: batch trn loss = 0.263, accuracy = 0.920\n",
      "Epoch 2: mean loss = 0.373, accuracy = 0.868\n",
      "2017-10-29 15:08:27 Iter 2700: batch trn loss = 0.286, accuracy = 0.880\n",
      "2017-10-29 15:08:59 Iter 2800: batch trn loss = 0.620, accuracy = 0.840\n",
      "2017-10-29 15:09:31 Iter 2900: batch trn loss = 0.270, accuracy = 0.920\n",
      "2017-10-29 15:10:03 Iter 3000: batch trn loss = 0.331, accuracy = 0.900\n",
      "2017-10-29 15:10:35 Iter 3100: batch trn loss = 0.519, accuracy = 0.840\n",
      "2017-10-29 15:11:07 Iter 3200: batch trn loss = 0.365, accuracy = 0.900\n",
      "2017-10-29 15:11:39 Iter 3300: batch trn loss = 0.264, accuracy = 0.880\n",
      "2017-10-29 15:12:11 Iter 3400: batch trn loss = 0.233, accuracy = 0.940\n",
      "2017-10-29 15:12:47 Iter 3500: batch trn loss = 0.385, accuracy = 0.860\n",
      "Epoch 3: mean loss = 0.347, accuracy = 0.877\n",
      "2017-10-29 15:13:19 Iter 3600: batch trn loss = 0.321, accuracy = 0.860\n",
      "2017-10-29 15:13:51 Iter 3700: batch trn loss = 0.235, accuracy = 0.920\n",
      "2017-10-29 15:14:22 Iter 3800: batch trn loss = 0.418, accuracy = 0.860\n",
      "2017-10-29 15:14:54 Iter 3900: batch trn loss = 0.267, accuracy = 0.900\n",
      "2017-10-29 15:15:26 Iter 4000: batch trn loss = 0.276, accuracy = 0.940\n",
      "2017-10-29 15:15:58 Iter 4100: batch trn loss = 0.321, accuracy = 0.920\n",
      "2017-10-29 15:16:30 Iter 4200: batch trn loss = 0.156, accuracy = 0.960\n",
      "2017-10-29 15:17:02 Iter 4300: batch trn loss = 0.345, accuracy = 0.880\n",
      "2017-10-29 15:17:34 Iter 4400: batch trn loss = 0.236, accuracy = 0.880\n",
      "Epoch 4: mean loss = 0.315, accuracy = 0.889\n",
      "validation\n",
      "Epoch 0: mean loss = 0.706, accuracy = 0.794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.70572072, 0.79359999999999997)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 15 epochs\n",
    "print('train')\n",
    "cnn.train(m_X_trn, y_trn, epochs=5, batch_size=50, print_every=100, plot_losses=False)\n",
    "print('validation')\n",
    "cnn.validate(m_X_val, y_val, epochs=1, batch_size=y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "2017-10-29 15:53:00 Iter 0: batch trn loss = 0.115, accuracy = 0.980\n",
      "2017-10-29 15:53:34 Iter 100: batch trn loss = 0.162, accuracy = 0.980\n",
      "2017-10-29 15:54:08 Iter 200: batch trn loss = 0.306, accuracy = 0.880\n",
      "2017-10-29 15:54:41 Iter 300: batch trn loss = 0.385, accuracy = 0.840\n",
      "2017-10-29 15:55:15 Iter 400: batch trn loss = 0.320, accuracy = 0.860\n",
      "2017-10-29 15:55:52 Iter 500: batch trn loss = 0.191, accuracy = 0.940\n",
      "2017-10-29 15:56:27 Iter 600: batch trn loss = 0.373, accuracy = 0.920\n",
      "2017-10-29 15:57:00 Iter 700: batch trn loss = 0.351, accuracy = 0.880\n",
      "2017-10-29 15:57:33 Iter 800: batch trn loss = 0.512, accuracy = 0.860\n",
      "Epoch 0: mean loss = 0.290, accuracy = 0.896\n",
      "2017-10-29 15:58:06 Iter 900: batch trn loss = 0.363, accuracy = 0.820\n",
      "2017-10-29 15:58:40 Iter 1000: batch trn loss = 0.248, accuracy = 0.840\n",
      "2017-10-29 15:59:13 Iter 1100: batch trn loss = 0.346, accuracy = 0.860\n",
      "2017-10-29 15:59:47 Iter 1200: batch trn loss = 0.129, accuracy = 0.940\n",
      "2017-10-29 16:00:21 Iter 1300: batch trn loss = 0.277, accuracy = 0.900\n",
      "2017-10-29 16:00:54 Iter 1400: batch trn loss = 0.193, accuracy = 0.960\n",
      "2017-10-29 16:01:29 Iter 1500: batch trn loss = 0.228, accuracy = 0.880\n",
      "2017-10-29 16:02:03 Iter 1600: batch trn loss = 0.327, accuracy = 0.860\n",
      "2017-10-29 16:02:38 Iter 1700: batch trn loss = 0.397, accuracy = 0.920\n",
      "Epoch 1: mean loss = 0.269, accuracy = 0.904\n",
      "2017-10-29 16:03:11 Iter 1800: batch trn loss = 0.218, accuracy = 0.920\n",
      "2017-10-29 16:03:45 Iter 1900: batch trn loss = 0.266, accuracy = 0.900\n",
      "2017-10-29 16:04:20 Iter 2000: batch trn loss = 0.149, accuracy = 0.980\n",
      "2017-10-29 16:04:54 Iter 2100: batch trn loss = 0.098, accuracy = 0.980\n",
      "2017-10-29 16:05:28 Iter 2200: batch trn loss = 0.083, accuracy = 0.980\n",
      "2017-10-29 16:06:01 Iter 2300: batch trn loss = 0.331, accuracy = 0.900\n",
      "2017-10-29 16:06:36 Iter 2400: batch trn loss = 0.197, accuracy = 0.940\n",
      "2017-10-29 16:07:12 Iter 2500: batch trn loss = 0.233, accuracy = 0.860\n",
      "2017-10-29 16:07:45 Iter 2600: batch trn loss = 0.373, accuracy = 0.840\n",
      "Epoch 2: mean loss = 0.249, accuracy = 0.910\n",
      "2017-10-29 16:08:18 Iter 2700: batch trn loss = 0.108, accuracy = 0.960\n",
      "2017-10-29 16:08:51 Iter 2800: batch trn loss = 0.136, accuracy = 0.940\n",
      "2017-10-29 16:09:25 Iter 2900: batch trn loss = 0.278, accuracy = 0.920\n",
      "2017-10-29 16:09:59 Iter 3000: batch trn loss = 0.171, accuracy = 0.940\n",
      "2017-10-29 16:10:31 Iter 3100: batch trn loss = 0.142, accuracy = 0.940\n",
      "2017-10-29 16:11:06 Iter 3200: batch trn loss = 0.203, accuracy = 0.920\n",
      "2017-10-29 16:11:39 Iter 3300: batch trn loss = 0.229, accuracy = 0.920\n",
      "2017-10-29 16:12:13 Iter 3400: batch trn loss = 0.291, accuracy = 0.900\n",
      "2017-10-29 16:12:49 Iter 3500: batch trn loss = 0.492, accuracy = 0.880\n",
      "Epoch 3: mean loss = 0.225, accuracy = 0.919\n",
      "2017-10-29 16:13:24 Iter 3600: batch trn loss = 0.078, accuracy = 0.980\n",
      "2017-10-29 16:13:58 Iter 3700: batch trn loss = 0.162, accuracy = 0.940\n",
      "2017-10-29 16:14:31 Iter 3800: batch trn loss = 0.182, accuracy = 0.940\n",
      "2017-10-29 16:15:04 Iter 3900: batch trn loss = 0.540, accuracy = 0.780\n",
      "2017-10-29 16:15:45 Iter 4000: batch trn loss = 0.273, accuracy = 0.880\n",
      "2017-10-29 16:16:19 Iter 4100: batch trn loss = 0.240, accuracy = 0.920\n",
      "2017-10-29 16:16:55 Iter 4200: batch trn loss = 0.308, accuracy = 0.900\n",
      "2017-10-29 16:17:28 Iter 4300: batch trn loss = 0.147, accuracy = 0.940\n",
      "2017-10-29 16:18:01 Iter 4400: batch trn loss = 0.232, accuracy = 0.900\n",
      "Epoch 4: mean loss = 0.213, accuracy = 0.924\n",
      "validation\n",
      "Epoch 0: mean loss = 0.755, accuracy = 0.802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7545625, 0.80220000000000002)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20 epochs\n",
    "print('train')\n",
    "cnn.train(m_X_trn, y_trn, epochs=5, batch_size=50, print_every=100, plot_losses=False)\n",
    "print('validation')\n",
    "cnn.validate(m_X_val, y_val, epochs=1, batch_size=y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "2017-10-29 16:18:47 Iter 0: batch trn loss = 0.268, accuracy = 0.920\n",
      "2017-10-29 16:19:21 Iter 100: batch trn loss = 0.071, accuracy = 0.960\n",
      "2017-10-29 16:19:54 Iter 200: batch trn loss = 0.121, accuracy = 0.940\n",
      "2017-10-29 16:20:27 Iter 300: batch trn loss = 0.147, accuracy = 0.960\n",
      "2017-10-29 16:21:00 Iter 400: batch trn loss = 0.162, accuracy = 0.940\n",
      "2017-10-29 16:21:34 Iter 500: batch trn loss = 0.143, accuracy = 0.960\n",
      "2017-10-29 16:22:08 Iter 600: batch trn loss = 0.077, accuracy = 0.980\n",
      "2017-10-29 16:22:44 Iter 700: batch trn loss = 0.201, accuracy = 0.960\n",
      "2017-10-29 16:23:19 Iter 800: batch trn loss = 0.207, accuracy = 0.900\n",
      "Epoch 0: mean loss = 0.203, accuracy = 0.927\n",
      "2017-10-29 16:23:52 Iter 900: batch trn loss = 0.252, accuracy = 0.940\n",
      "2017-10-29 16:24:26 Iter 1000: batch trn loss = 0.190, accuracy = 0.940\n",
      "2017-10-29 16:24:59 Iter 1100: batch trn loss = 0.207, accuracy = 0.940\n",
      "2017-10-29 16:25:33 Iter 1200: batch trn loss = 0.176, accuracy = 0.900\n",
      "2017-10-29 16:26:06 Iter 1300: batch trn loss = 0.109, accuracy = 0.960\n",
      "2017-10-29 16:26:40 Iter 1400: batch trn loss = 0.208, accuracy = 0.940\n",
      "2017-10-29 16:27:13 Iter 1500: batch trn loss = 0.095, accuracy = 1.000\n",
      "2017-10-29 16:27:47 Iter 1600: batch trn loss = 0.282, accuracy = 0.900\n",
      "2017-10-29 16:28:20 Iter 1700: batch trn loss = 0.217, accuracy = 0.940\n",
      "Epoch 1: mean loss = 0.186, accuracy = 0.933\n",
      "2017-10-29 16:28:55 Iter 1800: batch trn loss = 0.186, accuracy = 0.920\n",
      "2017-10-29 16:29:30 Iter 1900: batch trn loss = 0.119, accuracy = 0.960\n",
      "2017-10-29 16:30:05 Iter 2000: batch trn loss = 0.075, accuracy = 1.000\n",
      "2017-10-29 16:30:39 Iter 2100: batch trn loss = 0.141, accuracy = 0.940\n",
      "2017-10-29 16:31:12 Iter 2200: batch trn loss = 0.069, accuracy = 0.960\n",
      "2017-10-29 16:31:46 Iter 2300: batch trn loss = 0.225, accuracy = 0.920\n",
      "2017-10-29 16:32:19 Iter 2400: batch trn loss = 0.187, accuracy = 0.940\n",
      "2017-10-29 16:32:53 Iter 2500: batch trn loss = 0.125, accuracy = 0.960\n",
      "2017-10-29 16:33:26 Iter 2600: batch trn loss = 0.143, accuracy = 0.960\n",
      "Epoch 2: mean loss = 0.174, accuracy = 0.939\n",
      "2017-10-29 16:33:59 Iter 2700: batch trn loss = 0.086, accuracy = 0.980\n",
      "2017-10-29 16:34:35 Iter 2800: batch trn loss = 0.200, accuracy = 0.960\n",
      "2017-10-29 16:35:08 Iter 2900: batch trn loss = 0.304, accuracy = 0.920\n",
      "2017-10-29 16:35:40 Iter 3000: batch trn loss = 0.074, accuracy = 0.960\n",
      "2017-10-29 16:36:13 Iter 3100: batch trn loss = 0.083, accuracy = 0.980\n",
      "2017-10-29 16:36:46 Iter 3200: batch trn loss = 0.296, accuracy = 0.920\n",
      "2017-10-29 16:37:20 Iter 3300: batch trn loss = 0.300, accuracy = 0.900\n",
      "2017-10-29 16:37:54 Iter 3400: batch trn loss = 0.180, accuracy = 0.920\n",
      "2017-10-29 16:38:28 Iter 3500: batch trn loss = 0.168, accuracy = 0.920\n",
      "Epoch 3: mean loss = 0.171, accuracy = 0.939\n",
      "2017-10-29 16:39:02 Iter 3600: batch trn loss = 0.128, accuracy = 0.940\n",
      "2017-10-29 16:39:36 Iter 3700: batch trn loss = 0.217, accuracy = 0.940\n",
      "2017-10-29 16:40:09 Iter 3800: batch trn loss = 0.250, accuracy = 0.900\n",
      "2017-10-29 16:40:44 Iter 3900: batch trn loss = 0.143, accuracy = 0.920\n",
      "2017-10-29 16:41:17 Iter 4000: batch trn loss = 0.202, accuracy = 0.920\n",
      "2017-10-29 16:41:50 Iter 4100: batch trn loss = 0.286, accuracy = 0.860\n",
      "2017-10-29 16:42:23 Iter 4200: batch trn loss = 0.240, accuracy = 0.920\n",
      "2017-10-29 16:42:57 Iter 4300: batch trn loss = 0.246, accuracy = 0.900\n",
      "2017-10-29 16:43:29 Iter 4400: batch trn loss = 0.080, accuracy = 1.000\n",
      "Epoch 4: mean loss = 0.162, accuracy = 0.942\n",
      "validation\n",
      "Epoch 0: mean loss = 0.793, accuracy = 0.797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.79273933, 0.79679999999999995)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 25 epochs\n",
    "print('train')\n",
    "cnn.train(m_X_trn, y_trn, epochs=5, batch_size=50, print_every=100, plot_losses=False)\n",
    "print('validation')\n",
    "cnn.validate(m_X_val, y_val, epochs=1, batch_size=y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "2017-10-29 17:17:07 Iter 0: batch trn loss = 0.086, accuracy = 0.980\n",
      "2017-10-29 17:17:40 Iter 100: batch trn loss = 0.197, accuracy = 0.940\n",
      "2017-10-29 17:18:13 Iter 200: batch trn loss = 0.107, accuracy = 0.980\n",
      "2017-10-29 17:18:45 Iter 300: batch trn loss = 0.257, accuracy = 0.860\n",
      "2017-10-29 17:19:18 Iter 400: batch trn loss = 0.142, accuracy = 0.960\n",
      "2017-10-29 17:19:50 Iter 500: batch trn loss = 0.284, accuracy = 0.900\n",
      "2017-10-29 17:20:24 Iter 600: batch trn loss = 0.243, accuracy = 0.920\n",
      "2017-10-29 17:20:57 Iter 700: batch trn loss = 0.089, accuracy = 0.980\n",
      "2017-10-29 17:21:31 Iter 800: batch trn loss = 0.206, accuracy = 0.920\n",
      "Epoch 0: mean loss = 0.147, accuracy = 0.947\n",
      "2017-10-29 17:22:04 Iter 900: batch trn loss = 0.014, accuracy = 1.000\n",
      "2017-10-29 17:22:37 Iter 1000: batch trn loss = 0.042, accuracy = 1.000\n",
      "2017-10-29 17:23:10 Iter 1100: batch trn loss = 0.062, accuracy = 0.980\n",
      "2017-10-29 17:23:43 Iter 1200: batch trn loss = 0.112, accuracy = 0.960\n",
      "2017-10-29 17:24:16 Iter 1300: batch trn loss = 0.042, accuracy = 1.000\n",
      "2017-10-29 17:24:49 Iter 1400: batch trn loss = 0.182, accuracy = 0.940\n",
      "2017-10-29 17:25:22 Iter 1500: batch trn loss = 0.165, accuracy = 0.920\n",
      "2017-10-29 17:25:56 Iter 1600: batch trn loss = 0.104, accuracy = 0.980\n",
      "2017-10-29 17:26:29 Iter 1700: batch trn loss = 0.125, accuracy = 0.960\n",
      "Epoch 1: mean loss = 0.137, accuracy = 0.950\n",
      "2017-10-29 17:27:02 Iter 1800: batch trn loss = 0.056, accuracy = 1.000\n",
      "2017-10-29 17:27:34 Iter 1900: batch trn loss = 0.079, accuracy = 0.940\n",
      "2017-10-29 17:28:07 Iter 2000: batch trn loss = 0.154, accuracy = 0.920\n",
      "2017-10-29 17:28:41 Iter 2100: batch trn loss = 0.048, accuracy = 0.980\n",
      "2017-10-29 17:29:14 Iter 2200: batch trn loss = 0.232, accuracy = 0.940\n",
      "2017-10-29 17:29:48 Iter 2300: batch trn loss = 0.036, accuracy = 1.000\n",
      "2017-10-29 17:30:22 Iter 2400: batch trn loss = 0.179, accuracy = 0.920\n",
      "2017-10-29 17:30:56 Iter 2500: batch trn loss = 0.199, accuracy = 0.920\n",
      "2017-10-29 17:31:30 Iter 2600: batch trn loss = 0.378, accuracy = 0.860\n",
      "Epoch 2: mean loss = 0.120, accuracy = 0.958\n",
      "2017-10-29 17:32:06 Iter 2700: batch trn loss = 0.099, accuracy = 0.980\n",
      "2017-10-29 17:32:40 Iter 2800: batch trn loss = 0.144, accuracy = 0.960\n",
      "2017-10-29 17:33:13 Iter 2900: batch trn loss = 0.380, accuracy = 0.900\n",
      "2017-10-29 17:33:46 Iter 3000: batch trn loss = 0.046, accuracy = 1.000\n",
      "2017-10-29 17:34:20 Iter 3100: batch trn loss = 0.103, accuracy = 0.960\n",
      "2017-10-29 17:34:54 Iter 3200: batch trn loss = 0.337, accuracy = 0.900\n",
      "2017-10-29 17:35:28 Iter 3300: batch trn loss = 0.079, accuracy = 0.960\n",
      "2017-10-29 17:36:02 Iter 3400: batch trn loss = 0.144, accuracy = 0.980\n",
      "2017-10-29 17:36:35 Iter 3500: batch trn loss = 0.028, accuracy = 1.000\n",
      "Epoch 3: mean loss = 0.113, accuracy = 0.960\n",
      "2017-10-29 17:37:10 Iter 3600: batch trn loss = 0.210, accuracy = 0.920\n",
      "2017-10-29 17:37:43 Iter 3700: batch trn loss = 0.081, accuracy = 0.960\n",
      "2017-10-29 17:38:16 Iter 3800: batch trn loss = 0.109, accuracy = 0.960\n",
      "2017-10-29 17:38:49 Iter 3900: batch trn loss = 0.033, accuracy = 1.000\n",
      "2017-10-29 17:39:21 Iter 4000: batch trn loss = 0.030, accuracy = 0.980\n",
      "2017-10-29 17:39:55 Iter 4100: batch trn loss = 0.320, accuracy = 0.900\n",
      "2017-10-29 17:40:29 Iter 4200: batch trn loss = 0.098, accuracy = 0.940\n",
      "2017-10-29 17:41:01 Iter 4300: batch trn loss = 0.211, accuracy = 0.940\n",
      "2017-10-29 17:41:34 Iter 4400: batch trn loss = 0.078, accuracy = 1.000\n",
      "Epoch 4: mean loss = 0.113, accuracy = 0.960\n",
      "validation\n",
      "Epoch 0: mean loss = 0.870, accuracy = 0.802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.86955673, 0.80159999999999998)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 30 epochs\n",
    "print('train')\n",
    "cnn.train(m_X_trn, y_trn, epochs=5, batch_size=50, print_every=100, plot_losses=False)\n",
    "print('validation')\n",
    "cnn.validate(m_X_val, y_val, epochs=1, batch_size=y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "2017-10-29 18:43:59 Iter 0: batch trn loss = 0.073, accuracy = 0.980\n",
      "2017-10-29 18:44:31 Iter 100: batch trn loss = 0.135, accuracy = 0.960\n",
      "2017-10-29 18:45:03 Iter 200: batch trn loss = 0.019, accuracy = 1.000\n",
      "2017-10-29 18:45:36 Iter 300: batch trn loss = 0.039, accuracy = 0.980\n",
      "2017-10-29 18:46:08 Iter 400: batch trn loss = 0.072, accuracy = 0.980\n",
      "2017-10-29 18:46:40 Iter 500: batch trn loss = 0.268, accuracy = 0.900\n",
      "2017-10-29 18:47:12 Iter 600: batch trn loss = 0.011, accuracy = 1.000\n",
      "2017-10-29 18:47:44 Iter 700: batch trn loss = 0.275, accuracy = 0.820\n",
      "2017-10-29 18:48:17 Iter 800: batch trn loss = 0.061, accuracy = 0.980\n",
      "Epoch 0: mean loss = 0.105, accuracy = 0.962\n",
      "2017-10-29 18:48:49 Iter 900: batch trn loss = 0.059, accuracy = 0.980\n",
      "2017-10-29 18:49:21 Iter 1000: batch trn loss = 0.180, accuracy = 0.900\n",
      "2017-10-29 18:49:53 Iter 1100: batch trn loss = 0.089, accuracy = 0.960\n",
      "2017-10-29 18:50:25 Iter 1200: batch trn loss = 0.121, accuracy = 0.940\n",
      "2017-10-29 18:50:57 Iter 1300: batch trn loss = 0.042, accuracy = 0.980\n",
      "2017-10-29 18:51:29 Iter 1400: batch trn loss = 0.108, accuracy = 0.960\n",
      "2017-10-29 18:52:01 Iter 1500: batch trn loss = 0.130, accuracy = 0.960\n",
      "2017-10-29 18:52:32 Iter 1600: batch trn loss = 0.224, accuracy = 0.920\n",
      "2017-10-29 18:53:04 Iter 1700: batch trn loss = 0.167, accuracy = 0.920\n",
      "Epoch 1: mean loss = 0.101, accuracy = 0.963\n",
      "2017-10-29 18:53:36 Iter 1800: batch trn loss = 0.076, accuracy = 0.980\n",
      "2017-10-29 18:54:08 Iter 1900: batch trn loss = 0.044, accuracy = 0.980\n",
      "2017-10-29 18:54:40 Iter 2000: batch trn loss = 0.108, accuracy = 0.980\n",
      "2017-10-29 18:55:12 Iter 2100: batch trn loss = 0.016, accuracy = 1.000\n",
      "2017-10-29 18:55:44 Iter 2200: batch trn loss = 0.094, accuracy = 0.960\n",
      "2017-10-29 18:56:16 Iter 2300: batch trn loss = 0.130, accuracy = 0.960\n",
      "2017-10-29 18:56:48 Iter 2400: batch trn loss = 0.034, accuracy = 1.000\n",
      "2017-10-29 18:57:20 Iter 2500: batch trn loss = 0.229, accuracy = 0.900\n",
      "2017-10-29 18:57:52 Iter 2600: batch trn loss = 0.243, accuracy = 0.920\n",
      "Epoch 2: mean loss = 0.095, accuracy = 0.967\n",
      "2017-10-29 18:58:24 Iter 2700: batch trn loss = 0.167, accuracy = 0.960\n",
      "2017-10-29 18:58:55 Iter 2800: batch trn loss = 0.119, accuracy = 0.940\n",
      "2017-10-29 18:59:27 Iter 2900: batch trn loss = 0.032, accuracy = 1.000\n",
      "2017-10-29 18:59:59 Iter 3000: batch trn loss = 0.056, accuracy = 0.980\n",
      "2017-10-29 19:00:31 Iter 3100: batch trn loss = 0.096, accuracy = 0.980\n",
      "2017-10-29 19:01:03 Iter 3200: batch trn loss = 0.059, accuracy = 0.960\n",
      "2017-10-29 19:01:35 Iter 3300: batch trn loss = 0.062, accuracy = 1.000\n",
      "2017-10-29 19:02:07 Iter 3400: batch trn loss = 0.237, accuracy = 0.920\n",
      "2017-10-29 19:02:38 Iter 3500: batch trn loss = 0.114, accuracy = 0.980\n",
      "Epoch 3: mean loss = 0.092, accuracy = 0.967\n",
      "2017-10-29 19:03:10 Iter 3600: batch trn loss = 0.162, accuracy = 0.920\n",
      "2017-10-29 19:03:42 Iter 3700: batch trn loss = 0.106, accuracy = 0.960\n",
      "2017-10-29 19:04:14 Iter 3800: batch trn loss = 0.075, accuracy = 0.960\n",
      "2017-10-29 19:04:46 Iter 3900: batch trn loss = 0.159, accuracy = 0.960\n",
      "2017-10-29 19:05:18 Iter 4000: batch trn loss = 0.040, accuracy = 1.000\n",
      "2017-10-29 19:05:50 Iter 4100: batch trn loss = 0.164, accuracy = 0.940\n",
      "2017-10-29 19:06:21 Iter 4200: batch trn loss = 0.050, accuracy = 1.000\n",
      "2017-10-29 19:06:53 Iter 4300: batch trn loss = 0.010, accuracy = 1.000\n",
      "2017-10-29 19:07:25 Iter 4400: batch trn loss = 0.145, accuracy = 0.920\n",
      "Epoch 4: mean loss = 0.086, accuracy = 0.970\n",
      "validation\n",
      "Epoch 0: mean loss = 0.881, accuracy = 0.799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.88099295, 0.7994)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 35 epochs\n",
    "print('train')\n",
    "cnn.train(m_X_trn, y_trn, epochs=5, batch_size=50, print_every=100, plot_losses=False)\n",
    "print('validation')\n",
    "cnn.validate(m_X_val, y_val, epochs=1, batch_size=y_val.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
