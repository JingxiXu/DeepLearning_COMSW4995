{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMS 4995_002 Deep Learning Assignment 1\n",
    "### This assignment can be done in groups of at most 3 students. Everyone must submit on Courseworks individually.\n",
    "### Write down the UNIs of your group (if applicable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Member 1: Kaho Chan, kc3137\n",
    "## Member 2: Yu Wang, yw3025\n",
    "## Member 3: Jingxi Xu, jx2324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions, DO NOT modify this\n",
    "\n",
    "def get_img_array(path):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imread(path)\n",
    "\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to load data, DO NOT change these\n",
    "\n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    y_one_hot[y] = 1\n",
    "    return y_one_hot.T\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = get_img_array(f).astype(np.float)\n",
    "#         img_arr = img_arr.flatten() / 255.0\n",
    "        images.append(img_arr)\n",
    "    # X = np.column_stack(images)\n",
    "    X = np.stack(images, axis=0)\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n",
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "Loaded 10000/10000\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_root_path = 'cifar10-hw/'\n",
    "X_train, y_train = get_train_data(data_root_path) # this may take a few minutes\n",
    "X_test = get_images(data_root_path + 'test')\n",
    "print('Data loading done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group's helper function\n",
    "def split(X, y, val_size):\n",
    "    '''\n",
    "    split the data into training and validation set\n",
    "    '''\n",
    "    indices = np.random.permutation(X.shape[0])\n",
    "    test_num = int(val_size * X.shape[0])\n",
    "    return X[indices[test_num:]], X[indices[:test_num]], y[indices[test_num:]], y[indices[:test_num]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3)\n",
      "(5000, 32, 32, 3)\n",
      "(45000,)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "X_trn, X_val, y_trn, y_val = split(X_train, y_train, val_size=0.1)\n",
    "print(X_trn.shape)\n",
    "print(X_val.shape)\n",
    "print(y_trn.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalizae the input\n",
    "def input_normalization(images):\n",
    "    for i in range(images.shape[0]):\n",
    "        old_image = images[i,:,:,:]\n",
    "        new_image = (old_image - np.mean(old_image)) / np.std(old_image)\n",
    "        images[i,:,:,:] = new_image\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image whitening\n",
    "# imput normalization part\n",
    "X_trn = input_normalization(X_trn)\n",
    "X_val = input_normalization(X_val)\n",
    "X_test = input_normalization(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GLobal value\n",
    "H, W, T = 32, 32, 10 # height/width of images, number of classes of images\n",
    "cnns = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "Using TensorFlow, implement a Convolutional Neural Network to classify CIFAR10\n",
    "- At least two Convolutional Layers followed by normalization and pooling layers. \n",
    "- Activation function ReLU.\n",
    "- Optimizer: Gradient Descent\n",
    "- At least one fully connected layer followed by softmax transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN():\n",
    "    def __init__(self, model_fn, trainer, global_step=None):\n",
    "        if global_step is None:\n",
    "            tf.reset_default_graph()\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "        self.X = tf.placeholder(tf.float32, [None, H, W, 3])\n",
    "        self.Y = tf.placeholder(tf.int64, [None])\n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "        logit, loss = model_fn(self.X, self.Y, self.is_training)\n",
    "                    \n",
    "        # https://stackoverflow.com/a/43285333\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            train_op = trainer.minimize(loss, global_step=global_step)\n",
    "        # train_op = trainer.minimize(loss)\n",
    "        # Accuracy\n",
    "        correct = tf.equal(tf.argmax(logit, 1), self.Y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        self.variables = {\n",
    "            'train': [loss, correct, train_op],\n",
    "            'validate': [loss, correct, accuracy],\n",
    "            'test':logit}\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "    def batch_gen(self, Xd, Yd, batch_size, shuffle=True):\n",
    "        indicies = np.arange(Xd.shape[0])\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indicies)\n",
    "        for i in range(int(math.ceil(Xd.shape[0] / batch_size))):\n",
    "            start_idx = (i * batch_size) % Xd.shape[0]\n",
    "            idx = indicies[start_idx:start_idx + batch_size]\n",
    "            yield Xd[idx, :], Yd[idx]\n",
    "\n",
    "    def run(self, Xd, Yd, epochs, batch_size, print_every, plot_losses, status):\n",
    "        iter_cnt = 0\n",
    "        for e in range(epochs):\n",
    "            correct = 0\n",
    "            losses = []\n",
    "            for Xb, Yb in self.batch_gen(Xd, Yd, batch_size, shuffle = True):\n",
    "                feed_dict = {self.X: Xb, self.Y: Yb, self.is_training: status=='train'}\n",
    "                loss, corr, _ = self.sess.run(self.variables[status], feed_dict = feed_dict)\n",
    "                losses.append(loss)\n",
    "                correct += np.sum(corr)\n",
    "                if status == 'train' and iter_cnt % print_every == 0:\n",
    "                    print(\"{} Iter {}: batch trn loss = {:.3f}, accuracy = {:.3f}\".format(\n",
    "                        datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        iter_cnt,\n",
    "                        loss,\n",
    "                        np.mean(corr),\n",
    "                    ))\n",
    "                iter_cnt += 1\n",
    "            epoch_loss = np.mean(losses)\n",
    "            \n",
    "            epoch_accuracy = correct / Xd.shape[0]\n",
    "            print(\"Epoch: mean loss = {:.3f}, accuracy = {:.3f}\".format(epoch_loss, epoch_accuracy))\n",
    "            if plot_losses:\n",
    "                plt.plot(losses)\n",
    "                plt.grid(True)\n",
    "                plt.title('Epoch {} Mean Loss'.format(e))\n",
    "                plt.xlabel('minibatch number')\n",
    "                plt.ylabel('minibatch mean loss')\n",
    "                plt.show()\n",
    "        return epoch_loss, epoch_accuracy\n",
    "\n",
    "    def train(self, Xd, Yd, epochs=1, batch_size=50, print_every=100, plot_losses=False):\n",
    "        return self.run(Xd, Yd, epochs, batch_size, print_every, plot_losses, status='train')\n",
    "            \n",
    "    def validate(self, Xd, Yd, epochs=1, batch_size=50, print_every=100, plot_losses=False):\n",
    "        return self.run(Xd, Yd, epochs, batch_size, print_every, plot_losses, status='validate')\n",
    "    \n",
    "    def predict(self, Xd):\n",
    "        feed_dict = {self.X: Xd, self.is_training: False}\n",
    "        logit = self.sess.run(self.variables['test'], feed_dict = feed_dict)\n",
    "#         variables = [self.Y]\n",
    "#         Yp = self.sess.run(Xd, None, None, None, None, None, status='test')\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I \n",
    "- trn_loss: 1.598, train_acc: 0.512, test_loss: 1.173 test_acc: 0.595(0 epoch)\n",
    "- trn_loss: 1.011, train_acc: 0.650, test_loss: 0.985 test_acc: 0.670(1 epoch) \n",
    "- trn_loss: 0.853, train_acc: 0.701, test_loss: 0.947 test_acc: 0.676(2 epoch)\n",
    "- trn_loss: 0.749, train_acc: 0.740, test_loss: 0.888 test_acc: 0.705(3 epoch) \n",
    "- trn_loss: 0.671, train_acc: 0.766, test_loss: 0.919 test_acc: 0.695(4 epoch)\n",
    "- trn_loss: 0.602, train_acc: 0.789, test_loss: 0.889 test_acc: 0.718(5 epoch) \n",
    "- trn_loss: 0.550, train_acc: 0.806, test_loss: 0.902 test_acc: 0.716(6 epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trn: 0.753, val: 0.651\n",
    "def model_fn(layer_input, labels, is_training):\n",
    "    F1 = 32\n",
    "    layer_conv1 = tf.layers.conv2d(inputs=layer_input, filters=F1, kernel_size=[5, 5], padding='same', activation=tf.nn.relu)\n",
    "    layer_bn1 = tf.layers.batch_normalization(inputs=layer_conv1, training=is_training)\n",
    "    layer_pool1 = tf.layers.max_pooling2d(inputs=layer_bn1, pool_size=[2, 2], strides=2)\n",
    "    F2 = 64\n",
    "    layer_conv2 = tf.layers.conv2d(inputs=layer_pool1, filters=F2, kernel_size=[5, 5], padding='same', activation=tf.nn.relu)\n",
    "    layer_bn2 = tf.layers.batch_normalization(inputs=layer_conv2, training=is_training)\n",
    "    layer_pool2 = tf.layers.max_pooling2d(inputs=layer_bn2, pool_size=[2, 2], strides=2)\n",
    "    flat_size = int(H / 4 * W / 4 * F2)\n",
    "#     print(flat_size)\n",
    "    layer_pool2_flat = tf.reshape(layer_pool2, [-1, flat_size])\n",
    "    layer_dense = tf.layers.dense(inputs=layer_pool2_flat, units=1024)\n",
    "    # layer_dropout = tf.layers.dropout(inputs=layer_dense, rate=0.4)\n",
    "    layer_logit = tf.layers.dense(inputs=layer_dense, units=T)\n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int64), depth=10)\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "        onehot_labels=onehot_labels, logits=layer_logit)\n",
    "    return layer_logit, loss\n",
    "\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.03)\n",
    "cnn = CNN(model_fn, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: epoch 0\n",
      "2017-10-30 20:02:34 Iter 0: batch trn loss = 4.834, accuracy = 0.080\n",
      "2017-10-30 20:02:53 Iter 100: batch trn loss = 1.959, accuracy = 0.400\n",
      "2017-10-30 20:03:12 Iter 200: batch trn loss = 1.406, accuracy = 0.520\n",
      "2017-10-30 20:03:32 Iter 300: batch trn loss = 1.713, accuracy = 0.460\n",
      "2017-10-30 20:03:51 Iter 400: batch trn loss = 1.532, accuracy = 0.480\n",
      "2017-10-30 20:04:11 Iter 500: batch trn loss = 1.313, accuracy = 0.500\n",
      "2017-10-30 20:04:31 Iter 600: batch trn loss = 1.465, accuracy = 0.560\n",
      "2017-10-30 20:04:50 Iter 700: batch trn loss = 1.375, accuracy = 0.560\n",
      "2017-10-30 20:05:12 Iter 800: batch trn loss = 1.058, accuracy = 0.580\n",
      "Epoch: mean loss = 1.598, accuracy = 0.512\n",
      "validation\n",
      "Epoch: mean loss = 1.173, accuracy = 0.595\n",
      "train: epoch 1\n",
      "2017-10-30 20:05:40 Iter 0: batch trn loss = 1.058, accuracy = 0.600\n",
      "2017-10-30 20:06:01 Iter 100: batch trn loss = 1.363, accuracy = 0.540\n",
      "2017-10-30 20:06:21 Iter 200: batch trn loss = 1.020, accuracy = 0.620\n",
      "2017-10-30 20:06:41 Iter 300: batch trn loss = 1.222, accuracy = 0.600\n",
      "2017-10-30 20:07:02 Iter 400: batch trn loss = 1.362, accuracy = 0.540\n",
      "2017-10-30 20:07:21 Iter 500: batch trn loss = 0.851, accuracy = 0.660\n",
      "2017-10-30 20:07:41 Iter 600: batch trn loss = 0.791, accuracy = 0.740\n",
      "2017-10-30 20:08:01 Iter 700: batch trn loss = 1.268, accuracy = 0.560\n",
      "2017-10-30 20:08:21 Iter 800: batch trn loss = 1.007, accuracy = 0.700\n",
      "Epoch: mean loss = 1.011, accuracy = 0.650\n",
      "validation\n",
      "Epoch: mean loss = 0.985, accuracy = 0.670\n",
      "train: epoch 2\n",
      "2017-10-30 20:08:49 Iter 0: batch trn loss = 0.786, accuracy = 0.680\n",
      "2017-10-30 20:09:10 Iter 100: batch trn loss = 1.003, accuracy = 0.700\n",
      "2017-10-30 20:09:29 Iter 200: batch trn loss = 0.970, accuracy = 0.700\n",
      "2017-10-30 20:09:49 Iter 300: batch trn loss = 0.628, accuracy = 0.720\n",
      "2017-10-30 20:10:09 Iter 400: batch trn loss = 0.893, accuracy = 0.660\n",
      "2017-10-30 20:10:29 Iter 500: batch trn loss = 0.941, accuracy = 0.700\n",
      "2017-10-30 20:10:49 Iter 600: batch trn loss = 0.748, accuracy = 0.720\n",
      "2017-10-30 20:11:09 Iter 700: batch trn loss = 1.074, accuracy = 0.660\n",
      "2017-10-30 20:11:30 Iter 800: batch trn loss = 0.716, accuracy = 0.720\n",
      "Epoch: mean loss = 0.853, accuracy = 0.701\n",
      "validation\n",
      "Epoch: mean loss = 0.947, accuracy = 0.676\n",
      "train: epoch 3\n",
      "2017-10-30 20:11:57 Iter 0: batch trn loss = 0.693, accuracy = 0.720\n",
      "2017-10-30 20:12:18 Iter 100: batch trn loss = 0.548, accuracy = 0.860\n",
      "2017-10-30 20:12:38 Iter 200: batch trn loss = 0.917, accuracy = 0.740\n",
      "2017-10-30 20:12:58 Iter 300: batch trn loss = 0.917, accuracy = 0.660\n",
      "2017-10-30 20:13:21 Iter 400: batch trn loss = 0.748, accuracy = 0.680\n",
      "2017-10-30 20:13:42 Iter 500: batch trn loss = 0.769, accuracy = 0.740\n",
      "2017-10-30 20:14:02 Iter 600: batch trn loss = 0.788, accuracy = 0.760\n",
      "2017-10-30 20:14:22 Iter 700: batch trn loss = 0.707, accuracy = 0.720\n",
      "2017-10-30 20:14:41 Iter 800: batch trn loss = 0.695, accuracy = 0.740\n",
      "Epoch: mean loss = 0.749, accuracy = 0.740\n",
      "validation\n",
      "Epoch: mean loss = 0.888, accuracy = 0.705\n",
      "train: epoch 4\n",
      "2017-10-30 20:15:10 Iter 0: batch trn loss = 0.635, accuracy = 0.800\n",
      "2017-10-30 20:15:30 Iter 100: batch trn loss = 0.574, accuracy = 0.740\n",
      "2017-10-30 20:15:51 Iter 200: batch trn loss = 0.607, accuracy = 0.740\n",
      "2017-10-30 20:16:10 Iter 300: batch trn loss = 0.976, accuracy = 0.700\n",
      "2017-10-30 20:16:31 Iter 400: batch trn loss = 0.519, accuracy = 0.860\n",
      "2017-10-30 20:16:52 Iter 500: batch trn loss = 0.719, accuracy = 0.800\n",
      "2017-10-30 20:17:11 Iter 600: batch trn loss = 0.458, accuracy = 0.860\n",
      "2017-10-30 20:17:30 Iter 700: batch trn loss = 0.778, accuracy = 0.740\n",
      "2017-10-30 20:17:50 Iter 800: batch trn loss = 0.362, accuracy = 0.860\n",
      "Epoch: mean loss = 0.671, accuracy = 0.766\n",
      "validation\n",
      "Epoch: mean loss = 0.919, accuracy = 0.695\n",
      "train: epoch 5\n",
      "2017-10-30 20:18:17 Iter 0: batch trn loss = 0.563, accuracy = 0.800\n",
      "2017-10-30 20:18:36 Iter 100: batch trn loss = 0.510, accuracy = 0.800\n",
      "2017-10-30 20:18:55 Iter 200: batch trn loss = 0.613, accuracy = 0.800\n",
      "2017-10-30 20:19:15 Iter 300: batch trn loss = 0.408, accuracy = 0.840\n",
      "2017-10-30 20:19:34 Iter 400: batch trn loss = 0.592, accuracy = 0.780\n",
      "2017-10-30 20:19:53 Iter 500: batch trn loss = 0.647, accuracy = 0.760\n",
      "2017-10-30 20:20:12 Iter 600: batch trn loss = 0.399, accuracy = 0.840\n",
      "2017-10-30 20:20:31 Iter 700: batch trn loss = 0.688, accuracy = 0.820\n",
      "2017-10-30 20:20:51 Iter 800: batch trn loss = 0.503, accuracy = 0.860\n",
      "Epoch: mean loss = 0.602, accuracy = 0.789\n",
      "validation\n",
      "Epoch: mean loss = 0.889, accuracy = 0.718\n",
      "train: epoch 6\n",
      "2017-10-30 20:21:18 Iter 0: batch trn loss = 0.435, accuracy = 0.900\n",
      "2017-10-30 20:21:37 Iter 100: batch trn loss = 0.554, accuracy = 0.880\n",
      "2017-10-30 20:21:57 Iter 200: batch trn loss = 0.401, accuracy = 0.820\n",
      "2017-10-30 20:22:16 Iter 300: batch trn loss = 0.668, accuracy = 0.780\n",
      "2017-10-30 20:22:37 Iter 400: batch trn loss = 0.562, accuracy = 0.780\n",
      "2017-10-30 20:22:57 Iter 500: batch trn loss = 0.676, accuracy = 0.720\n",
      "2017-10-30 20:23:21 Iter 600: batch trn loss = 0.465, accuracy = 0.800\n",
      "2017-10-30 20:23:42 Iter 700: batch trn loss = 0.372, accuracy = 0.920\n",
      "2017-10-30 20:24:07 Iter 800: batch trn loss = 0.704, accuracy = 0.740\n",
      "Epoch: mean loss = 0.550, accuracy = 0.806\n",
      "validation\n",
      "Epoch: mean loss = 0.902, accuracy = 0.716\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    print('train: epoch %d' % i)\n",
    "    cnn.train(X_trn, y_trn, epochs=1, batch_size=50, print_every=100, plot_losses=False)\n",
    "    print('validation')\n",
    "    cnn.validate(X_val, y_val, epochs=1, batch_size=y_val.shape[0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict the result, save in ans1-uni.npy\n",
    "y_predict = cnn.predict(X_test)\n",
    "# transpose y\n",
    "save_predictions('ans1-uni.npy', y_predict.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10000)\n"
     ]
    }
   ],
   "source": [
    "# test if your numpy file has been saved correctly\n",
    "loaded_y = np.load('ans1-uni.npy')\n",
    "print(loaded_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test if your numpy file has been saved correctly\n",
    "y_predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
