{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions, DO NOT modify this\n",
    "\n",
    "def get_img_array(path):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imread(path)\n",
    "\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)\n",
    "        \n",
    "# Functions to load data, DO NOT change these\n",
    "\n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    y_one_hot[y] = 1\n",
    "    return y_one_hot.T\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = get_img_array(f).astype(np.float)\n",
    "#         img_arr = img_arr.flatten() / 255.0\n",
    "        images.append(img_arr)\n",
    "    # X = np.column_stack(images)\n",
    "    X = np.stack(images, axis=0)\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n",
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "Loaded 10000/10000\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_root_path = 'cifar10-hw/'\n",
    "X_train, y_train = get_train_data(data_root_path) # this may take a few minutes\n",
    "X_test = get_images(data_root_path + 'test')\n",
    "print('Data loading done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group's helper function\n",
    "def split(X, y, val_size):\n",
    "    '''\n",
    "    split the data into training and validation set\n",
    "    '''\n",
    "    indices = np.random.permutation(X.shape[0])\n",
    "    test_num = int(val_size * X.shape[0])\n",
    "    return X[indices[test_num:]], X[indices[:test_num]], y[indices[test_num:]], y[indices[:test_num]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3)\n",
      "(5000, 32, 32, 3)\n",
      "(45000,)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "X_trn, X_val, y_trn, y_val = split(X_train, y_train, val_size=0.1)\n",
    "print(X_trn.shape)\n",
    "print(X_val.shape)\n",
    "print(y_trn.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(raw_images):\n",
    "    images = np.empty_like(raw_images)\n",
    "    np.copyto(images, raw_images)\n",
    "    for i in range(images.shape[0]):\n",
    "        old = images[i]\n",
    "        new = (old - np.mean(old)) / np.std(old)\n",
    "        images[i, :, :, :] = new\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_trn = normalize(X_trn)\n",
    "X_val = normalize(X_val)\n",
    "X_tst = normalize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GLobal value\n",
    "H, W, T = 32, 32, 10 # height/width of images, number of classes of images\n",
    "cnns = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN():\n",
    "    def __init__(self, model_fn, trainer, global_step=None):\n",
    "        \n",
    "        if global_step is None:\n",
    "            tf.reset_default_graph()\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "        self.iter_cnt = 0\n",
    "        self.X = tf.placeholder(tf.float32, [None, H, W, 3])\n",
    "        self.Y = tf.placeholder(tf.int64, [None])\n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "        with tf.name_scope('loss'):\n",
    "            logit, loss = model_fn(self.X, self.Y, self.is_training)\n",
    "            tf.summary.scalar('loss', loss)\n",
    "        # https://stackoverflow.com/a/43285333\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            with tf.variable_scope('optimizer'):\n",
    "                gradients = trainer.compute_gradients(loss)\n",
    "                train_op = trainer.apply_gradients(gradients, global_step=global_step)\n",
    "        for tup in gradients:\n",
    "            the_string = str(tup[1])\n",
    "            if 'conv1/kernel:0' in the_string:\n",
    "                tf.summary.histogram('Gradient/conv1', tup[0])\n",
    "            elif 'conv2/kernel:0' in the_string:\n",
    "                tf.summary.histogram('Gradient/conv2', tup[0])\n",
    "            elif 'last/kernel:0' in the_string:\n",
    "                tf.summary.histogram('Gradient/last_fc', tup[0])\n",
    "        # train_op = trainer.minimize(loss)\n",
    "        # Accuracy\n",
    "        correct = tf.equal(tf.argmax(logit, 1), self.Y)\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "        self.sess = tf.Session()\n",
    "        merged = tf.summary.merge_all()\n",
    "        self.train_writer = tf.summary.FileWriter('/Users/jingxixu/Desktop/train', self.sess.graph)\n",
    "        self.test_writer = tf.summary.FileWriter('/Users/jingxixu/Desktop/test')\n",
    "        self.variables = {\n",
    "            'train': [merged, loss, correct, train_op],\n",
    "            'validate': [merged, loss, correct, accuracy],\n",
    "            'test': logit\n",
    "        }\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def batch_gen(self, Xd, Yd, batch_size, shuffle=True):\n",
    "        indicies = np.arange(Xd.shape[0])\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indicies)\n",
    "        for i in range(int(math.ceil(Xd.shape[0] / batch_size))):\n",
    "            start_idx = (i * batch_size) % Xd.shape[0]\n",
    "            idx = indicies[start_idx:start_idx + batch_size]\n",
    "            yield Xd[idx, :], Yd[idx]\n",
    "\n",
    "    def run(self, Xd, Yd, epochs, batch_size, print_every, plot_losses, status):\n",
    "        iter_cnt = 0\n",
    "        for e in range(epochs):\n",
    "            correct = 0\n",
    "            losses = []\n",
    "            for Xb, Yb in self.batch_gen(Xd, Yd, batch_size, shuffle = True):\n",
    "                self.iter_cnt += 1\n",
    "                feed_dict = {self.X: Xb, self.Y: Yb, self.is_training: status=='train'}\n",
    "                summary, loss, corr, _ = self.sess.run(self.variables[status], feed_dict = feed_dict)\n",
    "                if status == 'train':\n",
    "                    self.train_writer.add_summary(summary, self.iter_cnt)\n",
    "                elif status == 'validate':\n",
    "                    self.test_writer.add_summary(summary, self.iter_cnt)\n",
    "                losses.append(loss)\n",
    "                correct += np.sum(corr)\n",
    "                if status == 'train' and iter_cnt % print_every == 0:\n",
    "                    print(\"{} Iter {}: batch trn loss = {:.3f}, accuracy = {:.3f}\".format(\n",
    "                        datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        iter_cnt,\n",
    "                        loss,\n",
    "                        np.mean(corr),\n",
    "                    ))\n",
    "                iter_cnt += 1\n",
    "            epoch_loss = np.mean(losses)\n",
    "            epoch_accuracy = correct / Xd.shape[0]\n",
    "            print(\"Epoch {}: mean loss = {:.3f}, accuracy = {:.3f}\".format(\n",
    "                e, epoch_loss, epoch_accuracy))\n",
    "            if plot_losses:\n",
    "                plt.plot(losses)\n",
    "                plt.grid(True)\n",
    "                plt.title('Epoch {} Mean Loss'.format(e+1))\n",
    "                plt.xlabel('minibatch number')\n",
    "                plt.ylabel('minibatch mean loss')\n",
    "                plt.show()\n",
    "        \n",
    "        return epoch_loss, epoch_accuracy\n",
    "\n",
    "    def train(self, Xd, Yd, epochs=1, batch_size=50, print_every=100, plot_losses=False):\n",
    "        return self.run(Xd, Yd, epochs, batch_size, print_every, plot_losses, status='train')\n",
    "            \n",
    "    def validate(self, Xd, Yd, epochs=1, batch_size=50, print_every=100, plot_losses=False):\n",
    "        return self.run(Xd, Yd, epochs, batch_size, print_every, plot_losses, status='validate')\n",
    "    \n",
    "    def predict(self, Xd):\n",
    "        feed_dict = {self.X: Xd, self.is_training: False}\n",
    "        logit = self.sess.run(self.variables['test'], feed_dict = feed_dict)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [0103] normalize + dropout\n",
    "- base (0101): add normalize and dropout = 0.5\n",
    "- batch size still 50\n",
    "- trn: , val: (05 epochs)\n",
    "- trn: , val: (10 epochs)\n",
    "- trn: , val: (15 epochs)\n",
    "- trn: , val: (20 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3136\n"
     ]
    }
   ],
   "source": [
    "def model_fn(layer_input, labels, is_training):\n",
    "    reg_scale= 0.1\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(scale=reg_scale)\n",
    "    # initializer = tf.contrib.layers.xavier_initializer()\n",
    "    initializer = None\n",
    "    h, w = H, W\n",
    "    # conv - bn - max_pool\n",
    "    F1 = 64\n",
    "    P1, S1 = 3, 2 # pool_size, strides\n",
    "    layer = tf.layers.conv2d(\n",
    "        inputs=layer_input, filters=F1, kernel_size=[5, 5], padding='same', activation=tf.nn.relu,\n",
    "        kernel_regularizer=regularizer, kernel_initializer=initializer, name='conv1')\n",
    "    layer = tf.layers.batch_normalization(\n",
    "        inputs=layer, training=is_training,\n",
    "        beta_regularizer=regularizer, gamma_regularizer=regularizer)\n",
    "    layer = tf.layers.max_pooling2d(inputs=layer, pool_size=P1, strides=S1)\n",
    "    h = math.floor((h - P1 + S1) / S1)\n",
    "    w = math.floor((w - P1 + S1) / S1)\n",
    "    # conv - bn - max_pool\n",
    "    F2 = 64\n",
    "    P2, S2 = 3, 2 # pool_size, strides\n",
    "    layer = tf.layers.conv2d(\n",
    "        inputs=layer, filters=F2, kernel_size=[5, 5], padding='same', activation=tf.nn.relu,\n",
    "        kernel_regularizer=regularizer, kernel_initializer=initializer, name='conv2')\n",
    "    layer = tf.layers.batch_normalization(\n",
    "        inputs=layer, training=is_training,\n",
    "        beta_regularizer=regularizer, gamma_regularizer=regularizer)\n",
    "    layer = tf.layers.max_pooling2d(inputs=layer, pool_size=P2, strides=S2)\n",
    "    h = math.floor((h - P2 + S2) / S2)\n",
    "    w = math.floor((w - P2 + S2) / S2)\n",
    "    # dense1 - bn - dropout - fc - softmax\n",
    "    flat_size = F2 * h * w\n",
    "    print(flat_size)\n",
    "    layer = tf.reshape(layer, [-1, flat_size])\n",
    "    layer = tf.layers.dense(\n",
    "        inputs=layer, units=1024, activation=tf.nn.relu,\n",
    "        kernel_regularizer=regularizer, kernel_initializer=initializer)\n",
    "    layer = tf.layers.batch_normalization(\n",
    "        inputs=layer, training=is_training,\n",
    "        beta_regularizer=regularizer, gamma_regularizer=regularizer)\n",
    "    layer = tf.layers.dropout(inputs=layer, rate=0.5, training=is_training)\n",
    "    # no activation here for logit, as it will be calculated in loss\n",
    "    logit = tf.layers.dense(\n",
    "        inputs=layer, units=T, activation=None,\n",
    "        kernel_regularizer=regularizer, kernel_initializer=initializer, name='last')\n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int64), depth=10)\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logit)\n",
    "    \n",
    "    return logit, loss\n",
    "\n",
    "\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.03)\n",
    "cnn = CNN(model_fn, trainer)\n",
    "cnns['0103'] = cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: epoch 0\n",
      "2017-10-30 19:38:53 Iter 0: batch trn loss = 3.714, accuracy = 0.060\n",
      "2017-10-30 19:39:22 Iter 100: batch trn loss = 2.642, accuracy = 0.320\n",
      "2017-10-30 19:39:53 Iter 200: batch trn loss = 1.951, accuracy = 0.380\n",
      "2017-10-30 19:40:24 Iter 300: batch trn loss = 1.616, accuracy = 0.500\n",
      "2017-10-30 19:40:55 Iter 400: batch trn loss = 1.143, accuracy = 0.520\n",
      "2017-10-30 19:41:26 Iter 500: batch trn loss = 1.747, accuracy = 0.460\n",
      "2017-10-30 19:41:56 Iter 600: batch trn loss = 1.292, accuracy = 0.540\n",
      "2017-10-30 19:42:27 Iter 700: batch trn loss = 1.239, accuracy = 0.520\n",
      "2017-10-30 19:42:58 Iter 800: batch trn loss = 1.406, accuracy = 0.600\n",
      "Epoch 0: mean loss = 1.591, accuracy = 0.492\n",
      "validation\n",
      "Epoch 0: mean loss = 1.179, accuracy = 0.604\n",
      "train: epoch 1\n",
      "2017-10-30 19:44:07 Iter 0: batch trn loss = 1.550, accuracy = 0.500\n",
      "2017-10-30 19:44:39 Iter 100: batch trn loss = 0.958, accuracy = 0.600\n",
      "2017-10-30 19:45:10 Iter 200: batch trn loss = 1.020, accuracy = 0.620\n",
      "2017-10-30 19:45:42 Iter 300: batch trn loss = 1.139, accuracy = 0.640\n",
      "2017-10-30 19:46:15 Iter 400: batch trn loss = 1.310, accuracy = 0.640\n",
      "2017-10-30 19:46:47 Iter 500: batch trn loss = 0.858, accuracy = 0.660\n",
      "2017-10-30 19:47:18 Iter 600: batch trn loss = 1.044, accuracy = 0.620\n",
      "2017-10-30 19:47:49 Iter 700: batch trn loss = 1.232, accuracy = 0.480\n",
      "2017-10-30 19:48:20 Iter 800: batch trn loss = 1.365, accuracy = 0.500\n",
      "Epoch 0: mean loss = 1.086, accuracy = 0.629\n",
      "validation\n",
      "Epoch 0: mean loss = 1.018, accuracy = 0.668\n",
      "train: epoch 2\n",
      "2017-10-30 19:49:28 Iter 0: batch trn loss = 0.885, accuracy = 0.680\n",
      "2017-10-30 19:50:00 Iter 100: batch trn loss = 0.608, accuracy = 0.820\n",
      "2017-10-30 19:50:32 Iter 200: batch trn loss = 0.832, accuracy = 0.720\n",
      "2017-10-30 19:51:04 Iter 300: batch trn loss = 0.837, accuracy = 0.760\n",
      "2017-10-30 19:51:34 Iter 400: batch trn loss = 0.818, accuracy = 0.820\n",
      "2017-10-30 19:52:06 Iter 500: batch trn loss = 0.891, accuracy = 0.660\n",
      "2017-10-30 19:52:37 Iter 600: batch trn loss = 0.896, accuracy = 0.680\n",
      "2017-10-30 19:53:07 Iter 700: batch trn loss = 0.876, accuracy = 0.720\n",
      "2017-10-30 19:53:38 Iter 800: batch trn loss = 0.735, accuracy = 0.680\n",
      "Epoch 0: mean loss = 0.941, accuracy = 0.678\n",
      "validation\n",
      "Epoch 0: mean loss = 0.916, accuracy = 0.693\n",
      "train: epoch 3\n",
      "2017-10-30 19:54:46 Iter 0: batch trn loss = 0.742, accuracy = 0.680\n",
      "2017-10-30 19:55:19 Iter 100: batch trn loss = 0.889, accuracy = 0.660\n",
      "2017-10-30 19:55:51 Iter 200: batch trn loss = 0.974, accuracy = 0.720\n",
      "2017-10-30 19:56:22 Iter 300: batch trn loss = 0.851, accuracy = 0.740\n",
      "2017-10-30 19:56:53 Iter 400: batch trn loss = 1.147, accuracy = 0.580\n",
      "2017-10-30 19:57:24 Iter 500: batch trn loss = 0.936, accuracy = 0.680\n",
      "2017-10-30 19:57:55 Iter 600: batch trn loss = 1.084, accuracy = 0.660\n",
      "2017-10-30 19:58:26 Iter 700: batch trn loss = 1.082, accuracy = 0.680\n",
      "2017-10-30 19:58:57 Iter 800: batch trn loss = 0.873, accuracy = 0.680\n",
      "Epoch 0: mean loss = 0.846, accuracy = 0.710\n",
      "validation\n",
      "Epoch 0: mean loss = 0.785, accuracy = 0.729\n",
      "train: epoch 4\n",
      "2017-10-30 20:00:02 Iter 0: batch trn loss = 0.710, accuracy = 0.780\n",
      "2017-10-30 20:00:33 Iter 100: batch trn loss = 0.714, accuracy = 0.780\n",
      "2017-10-30 20:01:04 Iter 200: batch trn loss = 0.660, accuracy = 0.700\n",
      "2017-10-30 20:01:35 Iter 300: batch trn loss = 0.641, accuracy = 0.780\n",
      "2017-10-30 20:02:05 Iter 400: batch trn loss = 0.792, accuracy = 0.720\n",
      "2017-10-30 20:02:36 Iter 500: batch trn loss = 1.097, accuracy = 0.620\n",
      "2017-10-30 20:03:06 Iter 600: batch trn loss = 0.621, accuracy = 0.820\n",
      "2017-10-30 20:03:37 Iter 700: batch trn loss = 0.928, accuracy = 0.680\n",
      "2017-10-30 20:04:08 Iter 800: batch trn loss = 0.725, accuracy = 0.720\n",
      "Epoch 0: mean loss = 0.764, accuracy = 0.738\n",
      "validation\n",
      "Epoch 0: mean loss = 0.828, accuracy = 0.721\n",
      "train: epoch 5\n",
      "2017-10-30 20:05:14 Iter 0: batch trn loss = 0.567, accuracy = 0.800\n",
      "2017-10-30 20:05:45 Iter 100: batch trn loss = 0.662, accuracy = 0.800\n",
      "2017-10-30 20:06:16 Iter 200: batch trn loss = 0.512, accuracy = 0.760\n",
      "2017-10-30 20:06:47 Iter 300: batch trn loss = 0.518, accuracy = 0.840\n",
      "2017-10-30 20:07:18 Iter 400: batch trn loss = 0.448, accuracy = 0.840\n",
      "2017-10-30 20:07:49 Iter 500: batch trn loss = 0.781, accuracy = 0.740\n",
      "2017-10-30 20:08:21 Iter 600: batch trn loss = 0.804, accuracy = 0.640\n",
      "2017-10-30 20:08:52 Iter 700: batch trn loss = 0.684, accuracy = 0.740\n",
      "2017-10-30 20:09:23 Iter 800: batch trn loss = 0.621, accuracy = 0.800\n",
      "Epoch 0: mean loss = 0.689, accuracy = 0.761\n",
      "validation\n",
      "Epoch 0: mean loss = 0.765, accuracy = 0.747\n",
      "train: epoch 6\n",
      "2017-10-30 20:10:33 Iter 0: batch trn loss = 0.709, accuracy = 0.700\n",
      "2017-10-30 20:11:05 Iter 100: batch trn loss = 0.439, accuracy = 0.860\n",
      "2017-10-30 20:11:36 Iter 200: batch trn loss = 0.727, accuracy = 0.720\n",
      "2017-10-30 20:12:08 Iter 300: batch trn loss = 0.709, accuracy = 0.760\n",
      "2017-10-30 20:12:39 Iter 400: batch trn loss = 0.535, accuracy = 0.840\n",
      "2017-10-30 20:13:11 Iter 500: batch trn loss = 0.559, accuracy = 0.720\n",
      "2017-10-30 20:13:43 Iter 600: batch trn loss = 0.510, accuracy = 0.820\n",
      "2017-10-30 20:14:15 Iter 700: batch trn loss = 0.435, accuracy = 0.880\n",
      "2017-10-30 20:14:47 Iter 800: batch trn loss = 0.506, accuracy = 0.820\n",
      "Epoch 0: mean loss = 0.637, accuracy = 0.779\n",
      "validation\n",
      "Epoch 0: mean loss = 0.789, accuracy = 0.741\n",
      "train: epoch 7\n",
      "2017-10-30 20:15:53 Iter 0: batch trn loss = 0.580, accuracy = 0.760\n",
      "2017-10-30 20:16:24 Iter 100: batch trn loss = 0.860, accuracy = 0.640\n",
      "2017-10-30 20:16:56 Iter 200: batch trn loss = 0.755, accuracy = 0.700\n",
      "2017-10-30 20:17:29 Iter 300: batch trn loss = 0.448, accuracy = 0.820\n",
      "2017-10-30 20:18:01 Iter 400: batch trn loss = 0.749, accuracy = 0.800\n",
      "2017-10-30 20:18:32 Iter 500: batch trn loss = 0.742, accuracy = 0.780\n",
      "2017-10-30 20:19:04 Iter 600: batch trn loss = 0.634, accuracy = 0.800\n",
      "2017-10-30 20:19:36 Iter 700: batch trn loss = 0.741, accuracy = 0.720\n",
      "2017-10-30 20:20:08 Iter 800: batch trn loss = 0.490, accuracy = 0.840\n",
      "Epoch 0: mean loss = 0.585, accuracy = 0.797\n",
      "validation\n",
      "Epoch 0: mean loss = 0.736, accuracy = 0.762\n",
      "train: epoch 8\n",
      "2017-10-30 20:21:17 Iter 0: batch trn loss = 0.362, accuracy = 0.840\n",
      "2017-10-30 20:21:48 Iter 100: batch trn loss = 0.400, accuracy = 0.860\n",
      "2017-10-30 20:22:20 Iter 200: batch trn loss = 0.524, accuracy = 0.800\n",
      "2017-10-30 20:22:51 Iter 300: batch trn loss = 0.432, accuracy = 0.880\n",
      "2017-10-30 20:23:24 Iter 400: batch trn loss = 0.423, accuracy = 0.780\n",
      "2017-10-30 20:23:56 Iter 500: batch trn loss = 0.613, accuracy = 0.820\n",
      "2017-10-30 20:24:28 Iter 600: batch trn loss = 0.522, accuracy = 0.840\n",
      "2017-10-30 20:24:58 Iter 700: batch trn loss = 0.632, accuracy = 0.760\n",
      "2017-10-30 20:25:31 Iter 800: batch trn loss = 0.482, accuracy = 0.880\n",
      "Epoch 0: mean loss = 0.539, accuracy = 0.812\n",
      "validation\n",
      "Epoch 0: mean loss = 0.721, accuracy = 0.763\n",
      "train: epoch 9\n",
      "2017-10-30 20:26:38 Iter 0: batch trn loss = 0.473, accuracy = 0.820\n",
      "2017-10-30 20:27:11 Iter 100: batch trn loss = 0.263, accuracy = 0.880\n",
      "2017-10-30 20:27:42 Iter 200: batch trn loss = 0.582, accuracy = 0.760\n",
      "2017-10-30 20:28:15 Iter 300: batch trn loss = 0.368, accuracy = 0.860\n",
      "2017-10-30 20:28:46 Iter 400: batch trn loss = 0.366, accuracy = 0.820\n",
      "2017-10-30 20:29:18 Iter 500: batch trn loss = 0.680, accuracy = 0.780\n",
      "2017-10-30 20:29:49 Iter 600: batch trn loss = 0.418, accuracy = 0.880\n",
      "2017-10-30 20:30:21 Iter 700: batch trn loss = 0.442, accuracy = 0.820\n",
      "2017-10-30 20:30:52 Iter 800: batch trn loss = 0.497, accuracy = 0.800\n",
      "Epoch 0: mean loss = 0.500, accuracy = 0.825\n",
      "validation\n",
      "Epoch 0: mean loss = 0.723, accuracy = 0.766\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('train: epoch %d' % i)\n",
    "    cnn.train(X_trn, y_trn, epochs=1, batch_size=50, print_every=100, plot_losses=False)\n",
    "    print('validation')\n",
    "    cnn.validate(X_val, y_val, epochs=1, batch_size=y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: epoch 10\n",
      "2017-10-30 20:42:25 Iter 0: batch trn loss = 0.410, accuracy = 0.820\n",
      "2017-10-30 20:42:54 Iter 100: batch trn loss = 0.350, accuracy = 0.840\n",
      "2017-10-30 20:43:25 Iter 200: batch trn loss = 0.229, accuracy = 0.920\n",
      "2017-10-30 20:43:56 Iter 300: batch trn loss = 0.551, accuracy = 0.800\n",
      "2017-10-30 20:44:29 Iter 400: batch trn loss = 0.386, accuracy = 0.860\n",
      "2017-10-30 20:45:01 Iter 500: batch trn loss = 0.528, accuracy = 0.820\n",
      "2017-10-30 20:45:33 Iter 600: batch trn loss = 0.347, accuracy = 0.880\n",
      "2017-10-30 20:46:05 Iter 700: batch trn loss = 0.423, accuracy = 0.800\n",
      "2017-10-30 20:46:36 Iter 800: batch trn loss = 0.314, accuracy = 0.860\n",
      "Epoch 0: mean loss = 0.456, accuracy = 0.840\n",
      "validation\n",
      "Epoch 0: mean loss = 0.768, accuracy = 0.759\n",
      "train: epoch 11\n",
      "2017-10-30 20:47:45 Iter 0: batch trn loss = 0.434, accuracy = 0.840\n",
      "2017-10-30 20:48:16 Iter 100: batch trn loss = 0.428, accuracy = 0.820\n",
      "2017-10-30 20:48:47 Iter 200: batch trn loss = 0.582, accuracy = 0.780\n",
      "2017-10-30 20:49:18 Iter 300: batch trn loss = 0.294, accuracy = 0.900\n",
      "2017-10-30 20:49:49 Iter 400: batch trn loss = 0.481, accuracy = 0.880\n",
      "2017-10-30 20:50:19 Iter 500: batch trn loss = 0.432, accuracy = 0.780\n",
      "2017-10-30 20:50:49 Iter 600: batch trn loss = 0.440, accuracy = 0.860\n",
      "2017-10-30 20:51:19 Iter 700: batch trn loss = 0.188, accuracy = 0.920\n",
      "2017-10-30 20:51:48 Iter 800: batch trn loss = 0.500, accuracy = 0.840\n",
      "Epoch 0: mean loss = 0.424, accuracy = 0.851\n",
      "validation\n",
      "Epoch 0: mean loss = 0.702, accuracy = 0.781\n",
      "train: epoch 12\n",
      "2017-10-30 20:52:52 Iter 0: batch trn loss = 0.453, accuracy = 0.820\n",
      "2017-10-30 20:53:21 Iter 100: batch trn loss = 0.438, accuracy = 0.840\n",
      "2017-10-30 20:53:51 Iter 200: batch trn loss = 0.617, accuracy = 0.760\n",
      "2017-10-30 20:54:21 Iter 300: batch trn loss = 0.230, accuracy = 0.920\n",
      "2017-10-30 20:54:50 Iter 400: batch trn loss = 0.291, accuracy = 0.880\n",
      "2017-10-30 20:55:20 Iter 500: batch trn loss = 0.561, accuracy = 0.820\n",
      "2017-10-30 20:55:49 Iter 600: batch trn loss = 0.348, accuracy = 0.880\n",
      "2017-10-30 20:56:19 Iter 700: batch trn loss = 0.211, accuracy = 0.940\n",
      "2017-10-30 20:56:49 Iter 800: batch trn loss = 0.609, accuracy = 0.860\n",
      "Epoch 0: mean loss = 0.385, accuracy = 0.864\n",
      "validation\n",
      "Epoch 0: mean loss = 0.739, accuracy = 0.780\n",
      "train: epoch 13\n",
      "2017-10-30 20:57:51 Iter 0: batch trn loss = 0.229, accuracy = 0.920\n",
      "2017-10-30 20:58:21 Iter 100: batch trn loss = 0.405, accuracy = 0.820\n",
      "2017-10-30 20:58:51 Iter 200: batch trn loss = 0.333, accuracy = 0.880\n",
      "2017-10-30 20:59:20 Iter 300: batch trn loss = 0.392, accuracy = 0.860\n",
      "2017-10-30 20:59:50 Iter 400: batch trn loss = 0.549, accuracy = 0.820\n",
      "2017-10-30 21:00:20 Iter 500: batch trn loss = 0.240, accuracy = 0.940\n",
      "2017-10-30 21:00:50 Iter 600: batch trn loss = 0.276, accuracy = 0.860\n",
      "2017-10-30 21:01:21 Iter 700: batch trn loss = 0.491, accuracy = 0.840\n",
      "2017-10-30 21:01:51 Iter 800: batch trn loss = 0.201, accuracy = 0.980\n",
      "Epoch 0: mean loss = 0.359, accuracy = 0.873\n",
      "validation\n",
      "Epoch 0: mean loss = 0.756, accuracy = 0.772\n",
      "train: epoch 14\n",
      "2017-10-30 21:02:53 Iter 0: batch trn loss = 0.210, accuracy = 0.920\n",
      "2017-10-30 21:03:25 Iter 100: batch trn loss = 0.242, accuracy = 0.880\n",
      "2017-10-30 21:03:56 Iter 200: batch trn loss = 0.608, accuracy = 0.760\n",
      "2017-10-30 21:04:27 Iter 300: batch trn loss = 0.433, accuracy = 0.860\n",
      "2017-10-30 21:04:58 Iter 400: batch trn loss = 0.194, accuracy = 0.940\n",
      "2017-10-30 21:05:28 Iter 500: batch trn loss = 0.310, accuracy = 0.880\n",
      "2017-10-30 21:05:58 Iter 600: batch trn loss = 0.430, accuracy = 0.820\n",
      "2017-10-30 21:06:28 Iter 700: batch trn loss = 0.234, accuracy = 0.880\n",
      "2017-10-30 21:06:58 Iter 800: batch trn loss = 0.507, accuracy = 0.820\n",
      "Epoch 0: mean loss = 0.332, accuracy = 0.882\n",
      "validation\n",
      "Epoch 0: mean loss = 0.752, accuracy = 0.779\n",
      "train: epoch 15\n",
      "2017-10-30 21:08:07 Iter 0: batch trn loss = 0.200, accuracy = 0.920\n",
      "2017-10-30 21:08:37 Iter 100: batch trn loss = 0.341, accuracy = 0.900\n",
      "2017-10-30 21:09:08 Iter 200: batch trn loss = 0.351, accuracy = 0.880\n",
      "2017-10-30 21:09:39 Iter 300: batch trn loss = 0.271, accuracy = 0.880\n",
      "2017-10-30 21:10:12 Iter 400: batch trn loss = 0.235, accuracy = 0.900\n",
      "2017-10-30 21:10:44 Iter 500: batch trn loss = 0.218, accuracy = 0.920\n",
      "2017-10-30 21:11:16 Iter 600: batch trn loss = 0.493, accuracy = 0.840\n",
      "2017-10-30 21:11:47 Iter 700: batch trn loss = 0.255, accuracy = 0.940\n",
      "2017-10-30 21:12:19 Iter 800: batch trn loss = 0.272, accuracy = 0.880\n",
      "Epoch 0: mean loss = 0.306, accuracy = 0.890\n",
      "validation\n",
      "Epoch 0: mean loss = 0.767, accuracy = 0.777\n",
      "train: epoch 16\n",
      "2017-10-30 21:13:27 Iter 0: batch trn loss = 0.217, accuracy = 0.920\n",
      "2017-10-30 21:13:58 Iter 100: batch trn loss = 0.247, accuracy = 0.880\n",
      "2017-10-30 21:14:29 Iter 200: batch trn loss = 0.166, accuracy = 0.940\n",
      "2017-10-30 21:15:00 Iter 300: batch trn loss = 0.256, accuracy = 0.880\n",
      "2017-10-30 21:15:30 Iter 400: batch trn loss = 0.175, accuracy = 0.960\n",
      "2017-10-30 21:16:01 Iter 500: batch trn loss = 0.282, accuracy = 0.920\n",
      "2017-10-30 21:16:31 Iter 600: batch trn loss = 0.243, accuracy = 0.880\n",
      "2017-10-30 21:17:03 Iter 700: batch trn loss = 0.337, accuracy = 0.880\n",
      "2017-10-30 21:17:36 Iter 800: batch trn loss = 0.324, accuracy = 0.860\n",
      "Epoch 0: mean loss = 0.283, accuracy = 0.900\n",
      "validation\n",
      "Epoch 0: mean loss = 0.738, accuracy = 0.789\n",
      "train: epoch 17\n",
      "2017-10-30 21:18:45 Iter 0: batch trn loss = 0.392, accuracy = 0.840\n",
      "2017-10-30 21:19:17 Iter 100: batch trn loss = 0.354, accuracy = 0.900\n",
      "2017-10-30 21:19:48 Iter 200: batch trn loss = 0.188, accuracy = 0.960\n",
      "2017-10-30 21:20:20 Iter 300: batch trn loss = 0.212, accuracy = 0.920\n",
      "2017-10-30 21:20:50 Iter 400: batch trn loss = 0.277, accuracy = 0.840\n",
      "2017-10-30 21:21:21 Iter 500: batch trn loss = 0.248, accuracy = 0.880\n",
      "2017-10-30 21:21:51 Iter 600: batch trn loss = 0.284, accuracy = 0.940\n",
      "2017-10-30 21:22:22 Iter 700: batch trn loss = 0.219, accuracy = 0.920\n",
      "2017-10-30 21:22:53 Iter 800: batch trn loss = 0.322, accuracy = 0.940\n",
      "Epoch 0: mean loss = 0.267, accuracy = 0.905\n",
      "validation\n",
      "Epoch 0: mean loss = 0.760, accuracy = 0.782\n",
      "train: epoch 18\n",
      "2017-10-30 21:24:00 Iter 0: batch trn loss = 0.247, accuracy = 0.960\n",
      "2017-10-30 21:24:31 Iter 100: batch trn loss = 0.152, accuracy = 0.960\n",
      "2017-10-30 21:25:01 Iter 200: batch trn loss = 0.336, accuracy = 0.900\n",
      "2017-10-30 21:25:32 Iter 300: batch trn loss = 0.111, accuracy = 0.960\n",
      "2017-10-30 21:26:03 Iter 400: batch trn loss = 0.247, accuracy = 0.900\n",
      "2017-10-30 21:26:33 Iter 500: batch trn loss = 0.147, accuracy = 0.940\n",
      "2017-10-30 21:27:05 Iter 600: batch trn loss = 0.229, accuracy = 0.920\n",
      "2017-10-30 21:27:37 Iter 700: batch trn loss = 0.192, accuracy = 0.940\n",
      "2017-10-30 21:28:09 Iter 800: batch trn loss = 0.602, accuracy = 0.800\n",
      "Epoch 0: mean loss = 0.240, accuracy = 0.914\n",
      "validation\n",
      "Epoch 0: mean loss = 0.799, accuracy = 0.781\n",
      "train: epoch 19\n",
      "2017-10-30 21:29:15 Iter 0: batch trn loss = 0.187, accuracy = 0.900\n",
      "2017-10-30 21:29:46 Iter 100: batch trn loss = 0.328, accuracy = 0.840\n",
      "2017-10-30 21:30:18 Iter 200: batch trn loss = 0.086, accuracy = 0.980\n",
      "2017-10-30 21:30:50 Iter 300: batch trn loss = 0.065, accuracy = 0.980\n",
      "2017-10-30 21:31:21 Iter 400: batch trn loss = 0.380, accuracy = 0.840\n",
      "2017-10-30 21:31:51 Iter 500: batch trn loss = 0.222, accuracy = 0.900\n",
      "2017-10-30 21:32:22 Iter 600: batch trn loss = 0.150, accuracy = 0.940\n",
      "2017-10-30 21:32:54 Iter 700: batch trn loss = 0.243, accuracy = 0.920\n",
      "2017-10-30 21:33:27 Iter 800: batch trn loss = 0.262, accuracy = 0.900\n",
      "Epoch 0: mean loss = 0.220, accuracy = 0.922\n",
      "validation\n",
      "Epoch 0: mean loss = 0.776, accuracy = 0.785\n"
     ]
    }
   ],
   "source": [
    "for i in range(10, 20):\n",
    "    print('train: epoch %d' % i)\n",
    "    cnn.train(X_trn, y_trn, epochs=1, batch_size=50, print_every=100, plot_losses=False)\n",
    "    print('validation')\n",
    "    cnn.validate(X_val, y_val, epochs=1, batch_size=y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: epoch 20\n",
      "2017-10-30 21:42:27 Iter 0: batch trn loss = 0.163, accuracy = 0.940\n",
      "2017-10-30 21:42:57 Iter 100: batch trn loss = 0.148, accuracy = 0.900\n",
      "2017-10-30 21:43:29 Iter 200: batch trn loss = 0.219, accuracy = 0.920\n",
      "2017-10-30 21:44:00 Iter 300: batch trn loss = 0.231, accuracy = 0.920\n",
      "2017-10-30 21:44:32 Iter 400: batch trn loss = 0.167, accuracy = 0.940\n",
      "2017-10-30 21:45:03 Iter 500: batch trn loss = 0.246, accuracy = 0.900\n",
      "2017-10-30 21:45:34 Iter 600: batch trn loss = 0.732, accuracy = 0.840\n",
      "2017-10-30 21:46:04 Iter 700: batch trn loss = 0.175, accuracy = 0.940\n",
      "2017-10-30 21:46:34 Iter 800: batch trn loss = 0.147, accuracy = 0.940\n",
      "Epoch 0: mean loss = 0.199, accuracy = 0.928\n",
      "validation\n",
      "Epoch 0: mean loss = 0.790, accuracy = 0.789\n",
      "train: epoch 21\n",
      "2017-10-30 21:47:43 Iter 0: batch trn loss = 0.052, accuracy = 0.980\n",
      "2017-10-30 21:48:14 Iter 100: batch trn loss = 0.405, accuracy = 0.840\n",
      "2017-10-30 21:48:48 Iter 200: batch trn loss = 0.115, accuracy = 0.940\n",
      "2017-10-30 21:49:20 Iter 300: batch trn loss = 0.245, accuracy = 0.920\n",
      "2017-10-30 21:49:51 Iter 400: batch trn loss = 0.140, accuracy = 0.940\n",
      "2017-10-30 21:50:22 Iter 500: batch trn loss = 0.104, accuracy = 0.980\n",
      "2017-10-30 21:50:54 Iter 600: batch trn loss = 0.045, accuracy = 0.980\n",
      "2017-10-30 21:51:25 Iter 700: batch trn loss = 0.178, accuracy = 0.920\n",
      "2017-10-30 21:51:57 Iter 800: batch trn loss = 0.312, accuracy = 0.880\n",
      "Epoch 0: mean loss = 0.189, accuracy = 0.933\n",
      "validation\n",
      "Epoch 0: mean loss = 0.854, accuracy = 0.777\n",
      "train: epoch 22\n",
      "2017-10-30 21:53:04 Iter 0: batch trn loss = 0.344, accuracy = 0.900\n",
      "2017-10-30 21:53:36 Iter 100: batch trn loss = 0.388, accuracy = 0.880\n",
      "2017-10-30 21:54:08 Iter 200: batch trn loss = 0.217, accuracy = 0.920\n",
      "2017-10-30 21:54:39 Iter 300: batch trn loss = 0.053, accuracy = 1.000\n",
      "2017-10-30 21:55:12 Iter 400: batch trn loss = 0.052, accuracy = 0.980\n",
      "2017-10-30 21:55:42 Iter 500: batch trn loss = 0.166, accuracy = 0.940\n",
      "2017-10-30 21:56:13 Iter 600: batch trn loss = 0.099, accuracy = 0.980\n",
      "2017-10-30 21:56:43 Iter 700: batch trn loss = 0.414, accuracy = 0.840\n",
      "2017-10-30 21:57:13 Iter 800: batch trn loss = 0.519, accuracy = 0.840\n",
      "Epoch 0: mean loss = 0.183, accuracy = 0.935\n",
      "validation\n",
      "Epoch 0: mean loss = 0.809, accuracy = 0.788\n",
      "train: epoch 23\n",
      "2017-10-30 21:58:17 Iter 0: batch trn loss = 0.179, accuracy = 0.940\n",
      "2017-10-30 21:58:46 Iter 100: batch trn loss = 0.124, accuracy = 0.960\n",
      "2017-10-30 21:59:16 Iter 200: batch trn loss = 0.197, accuracy = 0.920\n",
      "2017-10-30 21:59:46 Iter 300: batch trn loss = 0.126, accuracy = 0.960\n",
      "2017-10-30 22:00:17 Iter 400: batch trn loss = 0.213, accuracy = 0.880\n",
      "2017-10-30 22:00:47 Iter 500: batch trn loss = 0.159, accuracy = 0.940\n",
      "2017-10-30 22:01:16 Iter 600: batch trn loss = 0.186, accuracy = 0.960\n",
      "2017-10-30 22:01:47 Iter 700: batch trn loss = 0.340, accuracy = 0.860\n",
      "2017-10-30 22:02:16 Iter 800: batch trn loss = 0.127, accuracy = 0.960\n",
      "Epoch 0: mean loss = 0.169, accuracy = 0.941\n",
      "validation\n",
      "Epoch 0: mean loss = 0.836, accuracy = 0.787\n",
      "train: epoch 24\n",
      "2017-10-30 22:03:18 Iter 0: batch trn loss = 0.308, accuracy = 0.920\n",
      "2017-10-30 22:03:50 Iter 100: batch trn loss = 0.169, accuracy = 0.940\n",
      "2017-10-30 22:04:20 Iter 200: batch trn loss = 0.069, accuracy = 0.960\n",
      "2017-10-30 22:04:52 Iter 300: batch trn loss = 0.249, accuracy = 0.900\n",
      "2017-10-30 22:05:21 Iter 400: batch trn loss = 0.160, accuracy = 0.940\n",
      "2017-10-30 22:05:52 Iter 500: batch trn loss = 0.180, accuracy = 0.980\n",
      "2017-10-30 22:06:21 Iter 600: batch trn loss = 0.125, accuracy = 0.940\n",
      "2017-10-30 22:06:51 Iter 700: batch trn loss = 0.121, accuracy = 0.960\n",
      "2017-10-30 22:07:21 Iter 800: batch trn loss = 0.134, accuracy = 0.920\n",
      "Epoch 0: mean loss = 0.159, accuracy = 0.942\n",
      "validation\n",
      "Epoch 0: mean loss = 0.852, accuracy = 0.783\n"
     ]
    }
   ],
   "source": [
    "for i in range(20, 25):\n",
    "    print('train: epoch %d' % i)\n",
    "    cnn.train(X_trn, y_trn, epochs=1, batch_size=50, print_every=100, plot_losses=False)\n",
    "    print('validation')\n",
    "    cnn.validate(X_val, y_val, epochs=1, batch_size=y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: epoch 25\n",
      "2017-10-30 22:15:26 Iter 0: batch trn loss = 0.064, accuracy = 1.000\n",
      "2017-10-30 22:15:55 Iter 100: batch trn loss = 0.096, accuracy = 0.980\n",
      "2017-10-30 22:16:25 Iter 200: batch trn loss = 0.089, accuracy = 0.960\n",
      "2017-10-30 22:16:55 Iter 300: batch trn loss = 0.149, accuracy = 0.940\n",
      "2017-10-30 22:17:25 Iter 400: batch trn loss = 0.030, accuracy = 1.000\n",
      "2017-10-30 22:17:54 Iter 500: batch trn loss = 0.143, accuracy = 0.960\n",
      "2017-10-30 22:18:24 Iter 600: batch trn loss = 0.135, accuracy = 0.960\n",
      "2017-10-30 22:18:53 Iter 700: batch trn loss = 0.096, accuracy = 0.960\n",
      "2017-10-30 22:19:23 Iter 800: batch trn loss = 0.118, accuracy = 0.960\n",
      "Epoch 0: mean loss = 0.149, accuracy = 0.948\n",
      "validation\n",
      "Epoch 0: mean loss = 0.856, accuracy = 0.786\n",
      "train: epoch 26\n",
      "2017-10-30 22:20:25 Iter 0: batch trn loss = 0.031, accuracy = 1.000\n",
      "2017-10-30 22:20:54 Iter 100: batch trn loss = 0.179, accuracy = 0.940\n",
      "2017-10-30 22:21:24 Iter 200: batch trn loss = 0.180, accuracy = 0.940\n",
      "2017-10-30 22:21:54 Iter 300: batch trn loss = 0.121, accuracy = 0.960\n",
      "2017-10-30 22:22:23 Iter 400: batch trn loss = 0.077, accuracy = 0.960\n",
      "2017-10-30 22:22:53 Iter 500: batch trn loss = 0.148, accuracy = 0.940\n",
      "2017-10-30 22:23:23 Iter 600: batch trn loss = 0.204, accuracy = 0.960\n",
      "2017-10-30 22:23:53 Iter 700: batch trn loss = 0.090, accuracy = 0.960\n",
      "2017-10-30 22:24:23 Iter 800: batch trn loss = 0.129, accuracy = 0.940\n",
      "Epoch 0: mean loss = 0.137, accuracy = 0.952\n",
      "validation\n",
      "Epoch 0: mean loss = 0.844, accuracy = 0.791\n",
      "train: epoch 27\n",
      "2017-10-30 22:25:24 Iter 0: batch trn loss = 0.094, accuracy = 0.960\n",
      "2017-10-30 22:25:54 Iter 100: batch trn loss = 0.047, accuracy = 1.000\n",
      "2017-10-30 22:26:24 Iter 200: batch trn loss = 0.193, accuracy = 0.960\n",
      "2017-10-30 22:26:54 Iter 300: batch trn loss = 0.184, accuracy = 0.920\n",
      "2017-10-30 22:27:24 Iter 400: batch trn loss = 0.202, accuracy = 0.940\n",
      "2017-10-30 22:27:53 Iter 500: batch trn loss = 0.103, accuracy = 0.980\n",
      "2017-10-30 22:28:24 Iter 600: batch trn loss = 0.138, accuracy = 0.980\n",
      "2017-10-30 22:28:55 Iter 700: batch trn loss = 0.256, accuracy = 0.880\n",
      "2017-10-30 22:29:26 Iter 800: batch trn loss = 0.078, accuracy = 0.980\n",
      "Epoch 0: mean loss = 0.127, accuracy = 0.955\n",
      "validation\n",
      "Epoch 0: mean loss = 0.856, accuracy = 0.792\n",
      "train: epoch 28\n",
      "2017-10-30 22:30:34 Iter 0: batch trn loss = 0.143, accuracy = 0.940\n",
      "2017-10-30 22:31:05 Iter 100: batch trn loss = 0.142, accuracy = 0.940\n",
      "2017-10-30 22:31:36 Iter 200: batch trn loss = 0.062, accuracy = 0.980\n",
      "2017-10-30 22:32:07 Iter 300: batch trn loss = 0.060, accuracy = 0.980\n",
      "2017-10-30 22:32:39 Iter 400: batch trn loss = 0.061, accuracy = 1.000\n",
      "2017-10-30 22:33:09 Iter 500: batch trn loss = 0.097, accuracy = 0.960\n",
      "2017-10-30 22:33:40 Iter 600: batch trn loss = 0.075, accuracy = 0.960\n",
      "2017-10-30 22:34:10 Iter 700: batch trn loss = 0.173, accuracy = 0.960\n",
      "2017-10-30 22:34:40 Iter 800: batch trn loss = 0.045, accuracy = 0.980\n",
      "Epoch 0: mean loss = 0.121, accuracy = 0.957\n",
      "validation\n",
      "Epoch 0: mean loss = 0.858, accuracy = 0.793\n",
      "train: epoch 29\n",
      "2017-10-30 22:35:45 Iter 0: batch trn loss = 0.047, accuracy = 1.000\n",
      "2017-10-30 22:36:15 Iter 100: batch trn loss = 0.040, accuracy = 1.000\n",
      "2017-10-30 22:36:45 Iter 200: batch trn loss = 0.080, accuracy = 0.940\n",
      "2017-10-30 22:37:16 Iter 300: batch trn loss = 0.038, accuracy = 1.000\n",
      "2017-10-30 22:37:48 Iter 400: batch trn loss = 0.117, accuracy = 0.960\n",
      "2017-10-30 22:38:19 Iter 500: batch trn loss = 0.077, accuracy = 0.980\n",
      "2017-10-30 22:38:50 Iter 600: batch trn loss = 0.177, accuracy = 0.940\n",
      "2017-10-30 22:39:21 Iter 700: batch trn loss = 0.182, accuracy = 0.920\n",
      "2017-10-30 22:39:52 Iter 800: batch trn loss = 0.198, accuracy = 0.960\n",
      "Epoch 0: mean loss = 0.121, accuracy = 0.957\n",
      "validation\n",
      "Epoch 0: mean loss = 0.901, accuracy = 0.787\n"
     ]
    }
   ],
   "source": [
    "for i in range(25, 30):\n",
    "    print('train: epoch %d' % i)\n",
    "    cnn.train(X_trn, y_trn, epochs=1, batch_size=50, print_every=100, plot_losses=False)\n",
    "    print('validation')\n",
    "    cnn.validate(X_val, y_val, epochs=1, batch_size=y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Y_pred = cnn.predict(Xd=X_tst)\n",
    "# Y_pred = Y_pred.T\n",
    "# save_predictions('ans1-uni.npy', Y_pred)\n",
    "\n",
    "loaded_y = np.load('ans2-uni.npy')\n",
    "print(loaded_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "yp = cnn.predict(X_val)\n",
    "z = np.argmax(yp, axis=1)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78820000000000001"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(z == y_val) / 5000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
